644p
ViT의 물체 크기나 해상도를 계층적 학습하기 어려움 - 모든 패치를 동일한 크기
CvT는 CNN에서 쓰는 계층 구조를 ViT를 적용해서
크기별 지역 특징/전역특징을 모두 활용하여 이를 타개
-> 적은 매개변수로 높은 성능

(ImageNet Bench기준 그렇게 높지는 않던데,
후속모델에서 CvT를 계승한게 이름이 달라졌다던지 일반 CNN->ResNet 마냥
Swin등 다른 ViT기반모델에 밀린건지
ViT등에 데이터셋을 아주 많이 때려박으면 개선이되는건지
아니면 이 모든게 합쳐졌던지 확인해봐야함)

645p
저수준에서 선, 점, 중간에선 눈귀, 고수준에선 얼굴 구조의 특징을 잡는등 CvT의 계층구조 예시
CvT에서 어텐션 레이어에 넣기전에 Conv에 먼저넣음,
Output Feature Map을 패치로 잘라 '겹쳐' 어텐션시킨다

646p
ViT에서는 위치 임베딩을 사용하나,
CvT는 Conv과정에서 커널을 이전과 조금씩 겹쳐 이동하므로 위치정보가 따로 필요없는듯하다

ViT에서 순서 (패치 임베딩(겹치지 않게 stride) -> 선형 변환(=필터와 합성곱)으로 선형 임베딩) -> 위치임베딩을 더함
CvT의 순서는 합성곱 임베딩(겹치게 stride) -> 합성곱 임베딩

사실 ViT도 안겹치게 Stride해서 Conv layer쓰는걸로 자르기/선형변환 둘 다 하기때문에 별 차이는 없고 stride 차이인듯

괜히 헷갈리게 적어놨어 책은...
stride로 안겹치게 한다음 위치임베딩을 나중에 더하기
vs stride 겹치게 feature map 뽑고 위치임베딩 안쓰기

647p
CvT 임베딩 방식의 예시, kernel 크기 3, stride 1로 합성곱하는 모습
커널(혹은 그게 모인 필터) 각각과 합성곱한 결과를 Flatten한것을
그대로 선형 임베딩 이후의 QKV로 쓰는모양?

648p
어텐션에 대한 합성곱 임베딩이란? 2D로 재구성된 토큰맵에서
분리 가능한 합성곱 연산을 적용해 성능 저하 및 계산 복잡도를 낮추는 연산
??? ???
QKV에서 Q와 KV에서, stride를 다르게 하여 패치 길이를 각각 9와 4로 다르게 하는 예시를 보여준다
https://velog.io/@kiolke/CvT-Introducing-Convolutions-to-Vision-Transformers-%EC%A0%9C5%EB%B6%80
여긴 Q/KV가 16/4

2D 합성곱 기반의 축소된 어텐션 임베딩 Squeezed Convolutional Projection)
{
Q/KV의 길이가 다르더라도 (K와 V는 같아야함) 어텐션의 입출력 길이는 같게되는 이유는
Q*transpose(K)*V에서
임베딩을 거친이후 Q가 a*1/KV가 b*1일 때,
Q*transpose(K)는 a*b크기 벡터, 이를 다시 V와 곱하면 a*b * b*1이므로 a*1의크기 즉 원래 벡터길이다

QK 내적하면 attention distribution이 담긴 attention map, 즉 패치 임베딩에 대한 중요도 계산
attention map과 value의 곱으로 가중합을 구하고 중요한 패치를 확인하는듯
}

649p
CvT 모델은 계층이 깊어지면
패치 크기를 축소하는 대신에
임베딩 벡터의 차원을 증가시킨다(커널 개수를 늘린다는 소린가?)

이미지 패치가 줄어든 만큼
임베딩 차원을 늘려
네트워크 안정성을 향상시키고 모델설계를 단순화 < ???

650p
실전 코드에 관하여
CvT는 기존 ViT나 Swin과 달리 높이나 너비를 이미지 프로세서에서 가져오는거 말고
shortest_edge즉 이미지의 min(너비, 높이)를 의미
microsoft/cvt-21 모델의 특징이 짧은 쪽으로 정규화한다

651p
마소 cvt 21모델의 구조에서 분류기 이전에 풀링이 없는 이유를
전역 특징과 지역 특징을 conv 층에서 파악했기 때문이라고...한다
(CNN은 같이 쓰지않나???, 패러미터 줄이는게 목적이 아닌가?, 전역 풀링이라도 쓰나???)
