전체적 설명
https://heeya-stupidbutstudying.tistory.com/entry/DL-Swin-Transformer-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Hierarchical-Vision-Transformer-using-Shifted-Windows
https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/swin-transformer/
https://silhyeonha-git.tistory.com/13

상대적 위치 편향 설명
https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/swin-transformer-v2/
https://lcyking.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows
이 주소가 적어도 책보다는 설명을 잘해주는듯

https://pajamacoder.tistory.com/18
고뇌하여 깨우친자의 블로그

624p
서론
기존 트랜스포머 기반 모델의 공통적인 문제 ->
고정된 패치 크기로 인해
세밀한 예측을 필요로 하는
의미론적 분할과 같은 작업에 적합하지 않음,

입력 이미지 크기에대한 2차 계산복잡도 < ???를 가지므로
고해상도 이미지를 처리하기 힘듬
따라서, 계층적 특징맵을??? 가지고 1차 계산복잡도를 갖는 Swin Transformer로 해결

스윈은 분류/객체 감지/영상 분할같은 인식 작업에서 강력한 성능을 낸다고 한다

--

스윈 트랜스포머는 시프트 윈도라는 기술로 패치 분할을 하고
패치를 쌓아 윈도를 구성한다
구성된 윈도 영역만 셀프 어텐션을 계산한다.

시프트 윈도 방식은 패치를 겹친다고한다
계층마다 어텐션이 수행되는 패치의 크기와 개수를 계층적으로 적용해 처리하므로
높은 해상도와 다양한 크기를 가진 객체를 효율적으로 처리가능하다고함

객체 탐지나 객체 분할에서 백본모델로 사용된다고함

625p
스윈과 ViT의 차이점은, 계층별로 패치의 크기와 개수가 다르다는것이다
이는 계층마다 어텐션이 되는 패치의 크기와 개수를 다르도록 로컬 윈도를 조정하는것으로 이루어진다.

입력 패치의 로컬윈도로 계층적 접근
시프트 윈도로 인접 패치간 정보 계산
계층적 접근 방식은 로컬 윈도가 각 패치의 세부정보에 집중
시프트로 전역 특징 확인
고해상도 이미지를 효율적으로 처리
???

ViT와 스윈 트랜스포머 비교
                             ViT              Swin
분류 헤드                   [CLS]토큰    각 토큰의 평균값 사용
로컬 윈도                       X                 O
상대적 위치 편향 사용             X                  O
계층별 패치 크기                동일             다양함

윈도: M개의 인접 patch로 구성된 patch set
----

626p
스윈 트랜스포머 모델 구조
이미지를 패치 파티션으로 분할 -> 4회반복[선형 임베딩(1스테이지)/패치병합(2~4스테이지, 각기 다른 해상도와 크기로) -> 스윈 트랜스포머 블록x2]
을 반복 수행

{
    스윈 트랜스포머 블록은 Attention/Layer Norm/FC layer등을 포함
    패치 간 상호작용을 수행
    패치 내부의 관계/격자 간의 관계를 학습하는 두 가지 어텐션 모듈로 되어있다.
    각 블록은 변형된 멀티 헤드 어텐션인 W-MSA와 SW-MSA를 사용한다
    W-MSA: 로컬 윈도를 사용하는 멀티 헤드 어텐션, 윈도를 Input Feature Map이 겹치지 않게 두고 내부에서만 독립적으로 self-attention함, 계산 줄어듬
    SW-MSA: 윈도를 가로 또는 세로 방향으로 이동한 후 인접한 윈도 간의 셀프 어텐션 계산

    SW-MSA는 순환시프트와 어텐션 마스크를 사용한다
    순환시프트는 로컬 윈도가 이동할 때 이동한 위치의 정보를 이전 위치에서 가져오는것...이라하는데
    위로 움직이면 위를 잘라 아래에 붙이는 개념인듯하다???
    마스크를 쓰는이유는, 잘라붙여봐야 실제 윈도우가 아니기 때문에 윈도 내부 어텐션 연산에서 무시하도록 처리
    이 둘의 신묘?한 조합으로 window를 움직인것과 움직이지 않았을 때 분할 수가 같아진다(이를 책에서는 역순환 시프트라고 서술한듯)

    한 블록은 [{입력과 (계층정규화 -> W-MSA)의 잔차연결} -> {입력과 (계층정규화 -> MLP)의 잔차연결}] ->
    다른 블록은 [{입력과 (계층정규화 -> SW-MSA)의 잔차연결} -> {입력과 (계층정규화 -> MLP)의 잔차연결}] 으로 되어있다

    윈도우 도입이유-패치가 너무 많음->QKV어텐션 연산너무많음->윈도우 내부만 어텐션해서 연산줄이기/가변길이 사진 사이즈를 패치크기의 격자로 대응
    윈도우가 움직이는이유-윈도 간에 어텐션이 안되서 전역 정보를 집계할필요성있음
}

패치파티션: 이미지를 작은 사각형인 패치로 분할
패치병합: 인접 패치를 저차원으로 축소하는 과정
패치 병합때는
이미지 패치 텐서 [C, H, W]를 [2C, H/2, W/2]로 재정렬, 2C의 차원을 저차원으로 임베딩
ex) [3, 8, 8] -> (재정렬)[6, 4, 4] -> (임베딩)[3, 4, 4]

632p
Swin만의 특징은 상대적 위치 편향의 고려이다
상대적 위치 편향은 로컬 윈도 안에 있는 패치 간의 상대적 거리를 임베딩하는 목적을 가진다.
이는 셀프 어텐션에서 추가 편향 항에 쓰인다
축별로 떨어진 거리를 합쳐 연산하는 느낌이며
책에서 나온거 기준으로 각 축별로 윈도우<->윈도우간 축별 거리를 X, Y 2행렬에 나눠담고 ->
(윈도 크기 - 1)을 두 행렬에 더하고 ->
X 축에만 (윈도 크기 * 2 - 1) 곱한다.
??? 이게 무슨 수학적 의미를 가지는지는 나중에 찾아보자

634p
해당 쪽의 예제 코드에는 헤드별로 윈도 크기등등 다르니까 전부 다른 상대적 위치 편향 테이블을 지정하는 모습을 보여준다.
셀프 어텐션의 수식 기준으로 나타내면
Attention(Q, K, V) = SoftMax(Q*Transfose(K) / sqrt(d) + B) * V에서, B를 담당한다
QKV는 각각 쿼리키값 벡터, d는 벡터의 임베딩 차원

636p
저게 이름이 '합성곱'인데 실제로는 4x4픽셀 3채널을 4x4x3 = 48
즉, NLP의 토큰마냥 한줄로 쭉-늘려놓은것에 불과한듯??

637p의
코드는 [32, 3, 224, 224]텐서 즉
RGB를 가진 224x224 크기의 사진 32개를
이는 4x4크기의 커널 96개를 가진 필터를 가진 합성곱 신경망에 넣어
[32, 3136, 96]텐서로 바꾸었다.
Swin도 트랜스포머이므로 정황상 이것은 [N, S, d]즉 개수/시퀀스길이/차원을 가진 트랜스포머의 입력텐서이다
정황상 이 합성곱이 해당 모델의 패치 파티션의 구현이다.

그렇다면, 선형 임베딩은 무엇인가?
Self-Attnetion의 QKV는 정확히 같은 텐서는 아니지만, 같은 원천을 둔 텐서이다.
이를 만드는 작업이 각각의 임베딩을 만들고 학습시키는 일인데
그 과정으로 만들어진것은 선형 임베딩 층이다.

앞선 ViT와는 달리, Seq에 [CLS]에 해당하는걸 추가하지는 않는다.
객체 검출과 같은 작업에서 이미지에 여러 물체가 있을 경우 [CLS] 하나로는 잡기 어렵기 때문이다
따라서 평균 값을 채택한다
(??? 대체 무엇을 평균내는가???)

638p
스윈 트랜스포머의 스테이지는 blocks(블록)과 downsample(패치 병합)으로 이루어져있다.
블록은 다시 SwinLayer-0와 SwinLayer-1로 이루어져있다.
각각 W-MSA와 SM-MSA의 구현체인듯

639p
layernorm_before/after는 1,2번째 계층정규화, attention은 W/SM-MSA
intermediate와 output은 MLP를 의미한다고 한다

특이한것은 활성화함수로 ReLU와 유사한 GELU를 쓴다고 한다
GELU란?
https://hongl.tistory.com/236

간단히 말해, dying ReLU(bias 크기 등으로 음수로 꼬라박혀 역전파 안되는거)를 방지하는
함수중에 음수가 너무 크지 않게 조절하는 함수인듯
가우시안 분포에 대한 누적 분포 함수이며, 입력값 0 근처에서 부드럽게 변화한다고 한다

640p
임베딩/어텐션은 모두 이전과 같은 방법으로 테스트하며,
[32, 3136, 96]차원을 유지한다

패치 병합은 입력 크기의 절반으로 분할하는 reduction과 layer norm을 수행한다
[32, 56x56, 96]가 4개의 [32, 28, 28, 96]텐서가 되고 이를 병합하면 [32, 28, 28, 96x4]
병합한걸 Linear(in384 out192)에 넣어 채널수를 절반으로 조절한다
patch_merge의결과, [32, 784, 192]텐서가 된다.

2,3,4 스테이지에서 모두 이뤄지므로
[32, 7x7, 96x8]즉 [32, 49, 768]텐서가 된다
이 텐서가 풀링 계층을 통과해 클래스를 예측한다.