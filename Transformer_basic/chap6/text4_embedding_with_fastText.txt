299p
fastText 단어의 하위 단어를 고려하여 단어를 임베딩한다
-N-gram을 사용해 단어를 분해하고 벡터화 한다
-<, >와 같은 특수 기호를 사용해서 단어의 시작과 끝을 나타내고 하위 문자열을 고려하는데 중요한 역할을 한다
-기호가 추가된 단어는 N-gram으로 하위 단어로 분해된다
-분해된 하위 단어 집합에는 나눠지지 않은 자기 자신도 포함

-OOV도 하위 단어로 나누어 임베딩 계산가능
-비슷한 단어는 비슷한 임베딩

예시?
서울특별시

<서울특별시>

<서울
서울특
울특별
특별시
별시>

<서울
서울특
울특별
특별시
별시>
<서울특별시>


fastText는 다양한 N-gram을 적용해 입력 토큰을 분해하고 하위 단어 벡터를 구성하여
단어의 부분 문자열을 고려하는 유연하고 정확한 하위 단어 집합을 생성
->
같은 하위 단어를 공유하는 단어끼리는 정보를 공유해 학습가능
비슷한 단어는 비슷한 임베딩 벡터를 가짐

301p
fastText 모델도 CBoW와 Skip-gram으로 구성, 네거티브 샘플링 사용

Word2Vec과 달리 하위 단어로 구성하고, 해당 하위 단어 사전크기를 갖는 임베딩 계층이 필요하다
gensim에서 FastText 클래스로 제공
Word2Vec과 유사한 기술이므로 대부분의 매개변수를 공유한다

gensim.models.FastText 클래스에 매개변수는
sentences, corpus_file, vector_size, alpha, window, min_count, workers
sg, hs, chow_mean, negative, ns_exponent, max_final_vocab, epochs, batch_words
는 word2vec과 같다

min_n과 max_n은 각각 N-gram의 최대값으로, 유일하게 word2vec과 다른 매개변수이다
N-gram의 N단위로 나누어 하위 단어 집합을 생성한다고 한다???

302p
KorNLI 데이터셋은 한국어 자연어 추론을 위한 데이터셋
자연어추론: 두 개 이상의 문장에서 관계를 분류하는 작업
두 문장이 함의/중립/불일치 관계중 어디에 해당하나 분류 가능