355p
트랜스포머는 모델은 입력 시퀀스를 병렬로 처리하는 기능을 가진다
셀프 어텐션을 기반으로 하며, 이는 순차 처리나 반복 연결에 의존하지 않고
입력 토큰 간의 관계를 직접 처리하고 이해하도록한다.
모델이 재귀나 합성곱 연산 없이 입력 토큰 간의 관계를 직접 모델링한다.

대규모 데이터셋에 효율적이라고 한다
언어 모델링과 텍스트 분류에 효과적
기계 번역, 언어 모델링, 텍스트 요약과 같은
장기적인 종속성을 포함하는 작업에 주로 사용

356p
트랜스포머 기반 모델의 학습 방식은 2개가 있다.

오토인코딩방식: 랜덤하게 문장의 일부를 빈칸 토큰으로 만들고
해당 빈칸에 어떤 단어가 적절할지 예측하는 작업(Task) 수행
예측할 때 양옆의 토큰을 참조하므로 양방향구조, 이를 인코더라고 한다

자기 회귀 방식: 예측되는 토큰의 왼쪽에 있는 토큰들만 참조
단방향이고 이를 디코더라고한다

357p
※ 트랜스포머 해석
https://analytics4everything.tistory.com/150

트랜스포머는 위치 임베딩, 순방향 신경망, 멀티헤드 어텐션으로 이루어져있다.

{
트랜스포머 모델은 어텐션 메커니즘을 사용하며
이를 이용해서 시퀀스 임베딩을 표현한다.

인코더와 디코더 간의 상호작용으로
입력 시퀀스의 중요한 부분에 초점을 맞추어 문백을 이해하고
적절한 출력을 생성한다.
}

인코더는 입력 시퀀스를 임베딩하여 고차원 벡터로 변환
디코더는 인코더의 출력을 입력으로 받아 출력 시퀀스를 생성
이 때, 어텐션 메커니즘은 인코더와 디코더 단어 사이의 상관관계를 계산하여 중요한 정보에 집중
입력 시퀀스의 각 단어가 출력 시퀀스의 어떤 단어와 관련이 있는지를 파악,
번역이나 요약문 생성등의 작업 수행

358p
인코더와 디코더는 두 부분으로 되어있고
이는 각각 N개의 트랜스포머 블록으로 되어있다.
이 블록은 멀티 헤드 어텐션과, 순방향 신경망으로 이루어져있다

멀티헤드 어텐션은 여러 어텐션을 병렬 처리한다.
입력 시퀀스에서 쿼리, 키, 값 벡터를 정의하고
입력 시퀀스들의 관계를 셀프 어텐션 한다.
쿼리와 키의 유사도를 계산하고,
해당 유사도를 가중치로 사용하여 값 벡터를 합산한다.

이렇게 만든 어텐션 행렬은 입력 시퀀스의
각 단어의 임베딩을 대체한다.

---
순방향 신경망은 이과정에서 산출된 임베딩 벡터를
더욱 고도화 하기 위해 사용된다.
여러 선형 계층으로 되어 잘아는 신경망의 구조처럼
입력 벡터에 가중치를 곱하고 편향을 더한것을 활성화함수에 넣는다
학습된 가중치들은 입력 시퀀스 각 단어의
의미를 잘 파악할 수 있는 방식으로 갱신된다.

---
트랜스포머는 인코더와 디코더로 이루어져있으며
각각 N개의 트랜스포머 블록으로 이루어져있고
블록은 멀티헤드어텐션과 순방향 신경망으로 되어있다
멀티헤드 어텐션에서 단어의 임베딩을 대체할 어텐션행렬을 제작하고
순방향 신경망에 넣어 각 단어의 의미를 잘 파악할 수 있도록 가중치를 갱신한다.

---
359p
트랜스포머는 입력 시퀀스를 소스와 타켓으로 나누어 처리한다.
인코더는 소스 시퀀스를 위치 인코딩된 입력 임베딩으로 표현해
트랜스포머 블록의 출력 벡터를 생성한다
이 출력 벡터는 입력 시퀀스 데이터의 관계를 잘표현한다

디코더도 인코더와 유사하게 트랜스포머 블록으로 구성되어 있지만
마스크 멀티 헤드 어텐션을 사용해
타깃 시퀀스 데이터를 순차적으로 생성시킨다
이 때, 디코더 입력 시퀀스들의 관계를 고도화하기 위해
인코더의 출력 벡터 정보를 참조한다. <- ??? 이게 무슨 소리고

최종적으로 생성된 디코더의 출력 벡터는 선형 임베딩으로
재표현되어 이미지나 자연어 모델에 활용된다
---
입력 임베딩과 위치 인코딩

트랜스포머는 RNN과 달리 입력시퀀스가 병렬 처리되므로
단어의 순서 정보를 제공하지 않기 때문에
위치 정보를 임베딩 벡터에 추가할 필요가 있다.
이를 위해 위치 인코딩 방식을 트랜스포머에서 사용한다

위치 인코딩은 입력 시퀀스의 순서 정보를 모델에 전달하는 방법으로
각 단어의 위치 정보를 나타내는 벡터를 더하여 임베딩 벡터에 위치 정보를 반영
결합된 최종 입력 벡터를 학습한다.

위치인코딩에서는 각 토큰의 위치를 각도로 표현해
sin과 cos함수로 위치 인코딩 벡터를 계산한다
토큰의 위치마다 동일한 임베딩 벡터를 사용하지 않기 때문에
각 토큰의 위치 정보를 모델이 학습할 수 있다 ???
(위치 정보를 도출하는 과정은 https://hongl.tistory.com/231)
(https://codingopera.tistory.com/45 이것도 있음)

361p
해당 페이지의 PositionalEncoding class는
파이토치로 작성된 위치 인코딩 코드의 예시이다.

위치 인코딩의 수식 PE는 다음과 같다
PE(pos, 2i) = sin(pos/10000^(2i/d_model))
PE(pos, 2i + 1) = cos(pos/10000^(2i/d_model))

pos: 입력 시퀀스에서 단어의 위치
i: 임베딩 벡터의 차원에서의 인덱스(※ 정확히는 2i또는 2i+1의 꼴). 즉 하나의 위치 임베딩 벡터를 위해 i=0부터 i=(d-1)까지 모든 차원의 값을 계산
d_model: 트랜스포머 모든 층에서 출력이 같다고 하며 그 값을 나타내는 상순. 이 값은 임베딩 벡터의 차원이기도 함

※ d값은 책에서는 입력 임베딩의 차원 128을 사용, 근본 논문에서는 51를 사용

해당 코드의 출력 텐서 차원은 [최대 시퀀스 길이, 1, 입력 임베딩의 차원]이다.

---
362p
특수 토큰: 입력 시퀀스의 시작과 끝 혹은 마스킹 영역을 나타낸다
BOS/EOS/UNK/PAD

입력 임베딩 변환과정
특수 토큰을 포함하여, 문장을 토큰의 원-핫 인코딩으로 표현한다
이 때 문장의 개수가 N개, 토큰의 최대 길이가 S, 어휘 사전의 크기가 V, 입력 임베딩 차원을 d라하면
하나의 단어의 원-핫 인코딩[1, V]가 [V, d] 크기의 룩업테이블을 지나 [1, d]크기의 임베딩 벡터가된다.
한 문장에는 최대 길이 S의 크기가 있으니 [S, d]크기의 행렬이 문장 하나이며,
문장의 개수 N개일 때 [N, S, d] 크기의 텐서는 입력 시퀀스가 된다

---------------------
364p
트랜스포머 인코더
인코더는 입력 시퀀스를 받아 여러개의 계층으로 구성된(※근본 논문은 6개) 인코더 계층을 거쳐 연산을 수행
각 인코더 계층은 멀티 헤드 어텐션과 순방향 신경망으로 구성되며, 입력 데이터에 대한 정보를 추출하고
다음 계층으로 전달

입력 임베딩에 위치 인코딩을 더해준다.
그것이 첫 입력이며 이후 모든 multi-head self attention의 경우
입력으로 이전 레이어의 출력을 사용한다.

---
해당 입력으로 각 헤드가 사용할 Query, Key, Value를 만들어 내는데
책에서는 입력 임베딩에서 선형변환으로, 블로그에서는 행렬곱으로 만든다고 되어있다.
둘은 같다.

self-attention에서 Q, K, V는 동일한 근원에서 나오지만 같지는 않다.
각각 다른 가중치 행렬을 곱하기 때문이다. 이 가중치 행렬은 학습이 가능하다.
(??? 그러나 무엇을 정답으로 하여 학습하는가, 오차 역전파로 자동으로? 여러 블로그에서, 처음에는 임의의 값을 가진다고한다)
각 가중치 행렬은 [d_model, d_model/num_heads] 크기로 되어 (※ d_model/num_heads은 근본 논문 기준)
d_model 크기의 임베딩을 집어넣으면 d_model/num_head 크기의 벡터로 바뀐다
문장에는 단어가 S개 있으므로 [S, d_model/num_head] 크기의 행렬이 3개 Q, K, V로 나온다

---
{ 무시해도 될듯? 이후로는 기존 어텐션과 동일 ※ 단, 행렬로 들어가므로 병★렬처리가능하며, 어텐션 분포 뽑는 과정이 약간 다르다
쿼리 벡터는 현재 시점에서 참조하고자 하는 정보의 위치를 나타내는 벡터로, 인코더의 각 시점마다 생성된다
키 벡터는 쿼리와 비교되는 대상으로 쿼리 벡터를 제외한 입력 시퀀스에서 탐색되는 벡터로, 인코더의 각 시점마다 생성된다
값 벡터는 쿼리 벡터와 키 벡터로 생성된 어텐션 스코어를 얼마나 반영할지 설정하는 가중치 역할이다
}

Q와 K(의 전치)를 행렬곱하여 어텐션 스코어를 뽑아준다 (※ 책에서는 Q를 전치하는데, 행벡터냐 열벡터냐는 표기별로 다르니 신경쓰지 말것)
[S, d_model/num_head]와 [d_model/num_head, S] 행렬이 곱해지므로 [S, S] 크기의 행렬이 각 문장마다나온다

어텐션 분포는 마찬가지로 어텐션 스코어를 소프트맥스에 넣어서 이루어지는데,
벡터 차원이 커질 때 스코어 값이 같이 커지는 문제를 완화하기 위해 d^(0.5)로 나누는 보정을 해서 넣는다
따라서, distribution(Q, K) = softmax(QK^T/d^(0.5))

보정된 값을 V와 행렬곱하면 어텐션 값이 나온다
Attention(Q, K, V) = softmax(QK^T / d^(0.5)) * V

※ 어텐션 맵 하나는 [S, S]의 크기지만, N개 문장을 병렬 처리하면 [N, S, S] 어텐션 스코어 맵을 사용한다고 함
※ 책에서 나온 내용
※ ??? 그렇다면 대응하는 V는 [N, S, d_model/num_head]크기인가, 곱하면 [N, S, d_model/num_head] 가 나오는가?

---
멀티 헤드는 이런 셀프 어텐션을 여러번 수행하는것
따라서, Q, K, V 행렬은 i개의 헤드가 있을 때 i개 만큼 존재한다
※ 각각의 헤드에서 나온 각각의 셀프 어텐션 값들은 입력에 대한 i개 여러 관점을 의미하는듯하다

[N, S, d]텐서에 i개의 셀프 어텐션 텐서를 생성하고자 할때
헤더의 차원축을 생성하여(unsqueeze인가???)
[N, i, S, d/i] 텐서를 구성한다, 이는 i개의 셀프어텐션된 [N, S, d/i] 텐서를 의미한다
※ 즉, 위 [N, S, d_model/num_head]가 하나의 셀프 어텐션의 결과
이렇게 생성된 k개의 셀프 어텐션 값은 concatenate되어 [N, S, d]의 형태로 출력된다.

---
출력된 [N, S, d] 텐서는 덧셈 & 정규화 층을 거친다
통과하기 이전의 [N, S, d] 크기의 텐서와
통과한 이후의 같은 크기의 텐서를 더하여
학습시 발생하는 기울기 소실을 완화하고,
임베딩 차원 축으로 정규화 하는 계층 정규화를 적용
※ 계층 정규화는 채널을 축으로 하는 정규화, 배치에 하는게 아니다

---
그걸 다시 순방향 신경망에 넣음
순방향 신경망은 Linear층 혹은 1차원 합성곱 사용가능
그걸 다시 덧셈 및 정규화를 거침

---
트랜스포머 인코더 요약
1. 위치 인코딩 벡터와 입력 임베딩 벡터의 합을 입력으로 넣음
2. 입력에 각각 다른 3겹의 가중치 행렬을 사용하여 Q, K, V를 헤드 개수만큼 뽑음
3. 그걸 셀프 어텐션하고 병합
4. 입력값에 셀프 어텐션 값을 더하고 임베딩 차원으로 계층 정규화
5. 순방향 신경망에 넣고 학습
6. 4와 5의 결과물을 더하고 계층 정규화
7. 위 작업을 하는 인코더 블록을 여러 차례 거치고 결과를 디코더에 넣음

※!※! ??? 인코더의 마지막 순방향 신경망은 Positive-wise Fully Connected Feed-Forward라는 특이한 이름과 구조가 있으니 나중에 학습해보자???

----------------------------------
366p
트랜스포머 디코더

소스와 타겟중, 타겟 데이터의 입력 임베딩을 입력받는다.
위치 인코딩도 더해서 입력받게된다
마찬가지로 입력 시퀀스의 순서 정보를 알게된다.

입력 시퀀스는 디코더 각 블록의 첫 층인 마스크 멀티 헤드 어텐션으로 입력된다.
멀티 헤드 어텐션과 유사하지만 현재 위치 이전의 단어만 참조하도록 설계되었다
[첫 번째 쿼리 벡터] -> [첫 번째 키 벡터]
[두 번째 쿼리 벡터] -> [첫 번째 키 벡터, 두 번째 키 벡터]
와 같은 식으로 참조한다.

실제 구현에서는 소프트맥스로 어텐션 분포를 취하기 전에
마스크 영역에 매우 적은값인 -INF를 더해주어
소프트맥스 이후 해당하는 부분이 0인 어텐션 분포(= 어텐션 가중치)를 가지기 위함이다.
※ 책에서는 어텐션 스코어를 실제로는 어텐션 분포인것과 혼용한다

참고하면 안되는 영역의 가중치가 0에 가까우므로 V와 곱할 때
마스크 영역 이후의 입력 토큰의 정보를 참조하지 못하게된다.

---
마스크드 멀티 헤드 어텐션의 출력 텐서가 덧셈 & 정규화 과정을 거치면
같은 디코더 블록 내에 다음 멀티 헤드 어텐션의 Query로 들어간다.
이 때, Key와 Value는 인코더의 출력값을 사용한다
self가 아닌 cross attention을 한다. (※ 방식은 동일)

이후로 덧셈 & 정규화 -> 순방향 신경망 -> 덧셈 & 정규화까지는 동일하며
이 과정을 디코더 블록 수만큼 반복한다

---
마지막 디코더 블록의 출력 텐서 [N, S, d]는
선형 임베딩(= 선형 변환)과 소프트 맥스 함수를 적용해
각 타깃 시퀀스 위치마다 예측 확률을 계산할 수 있다.

---
{ 이건 좀 더 알아볼 필요가 있겠다
디코더는 타깃 데이터를 추론할 때 토큰 또는 단어를 순차적으로 생성시킨다.
순차 생성은 디코더의 입력에 빈 값을 의미하는 PAD 토큰을 사용하는 것이다.
}