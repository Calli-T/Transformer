계단함수: 임계값을 넘으면 1, 못넘기면 0/불연속점에서 미분불가능, 값도 극단적 -> 그래서 안씀
임계값함수: 임계값을 텀으면 x를 그대로 전달, 아니면 특정값으로 전달/입력에대한 함수의 기울기를 계산할 수 없어서 네트워크에 안?쓴다

시그모이드: 큰 값을 넣어도 -1~1사이 밖에 안나온다 -> 기울기 소실됨/
수학적으로는 도함수가 1도 안되서 계층이 많아지면 무조건 0에 가깝게 줄어듬->은닉충에서 활성화로 안씀/데이터가 양수만 나오는 경우 출력값의 중심이 0이 아니라서/
기울기가 모두 양수 or 음수라서 기울기가 지그재그로 변동->학습 떨어짐
결론적으로 ,이진분류 모델에서 출력층에서 사용한다.

하이퍼 볼릭 탄젠트: (e^x - e^-1)/(e^x + e^-1)
-1~1의출력범위, 0이 중심, 음수 반환 가능
입력이 4를 넘어가면 출력이 1에 수렴한다
그러나 기울기 소실 여전함

ReLU: 0보다 작같->0 반환, 0보다 크면 선형함수에 값을 대입
죽은뉴런: 가중치 갱신으로 가중치 합이 음수가 되면, 더 이상 값을 갱신 못해서 (x <= 0에서 도함수는 0) 죽은 뉴런이 된다.

LeakyReLU: ReLU랑 같지만 음의 가중치 합일 때도 작은 값을 출력하여 기울기를 갱신한다.
PReLU: LekayReLU랑 비슷하지만 음의 가중치 합일 때 기울기가 고정값이 아니고 학습을 통해 갱신되는 매개변수이다. 따라서 음일 때 학습 데이터 세트에 영향을 받게된다

ELU: 지수함수 사용, 부드러운 곡선의 형태
기존 ReLU와 변형은 0에서 끊어지지만, ELU는 음의 기울기에서 비선형 구조를 가져서
입력 0에도 출력값이 급변하지 않는다, 경사 하강법의 수렴 속도가 비교적 빠르다
더 복잡하므로 학습 완료까지의 전체시간은 늘어난다.

입력이 음일 때 negative_slope * (e^x - 1), 양에서 x

소프트맥스 함수: 차원 벡터에서 특정 출력값이 k 번 째 클래스에 속할 확률을 계산하는 활성화 함수
네트워크의 출력을 가능한 클래스에 대한 확률 분포로 매핑