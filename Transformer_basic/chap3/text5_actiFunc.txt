122p
활성화 함수: 인공신경망에서 은닉층을 활성화하기 위한 함수
활성화: 뉴런의 출력값을 선형에서 비선형으로 변환

노드별로 같은 영향을 가진것은 아니다.
활성화 시키는 정도가 다름

비선형구조인 이유: 선형일 경우 미분 과정에서 항상 상수가 나와서 역전파가 안됨
활성화 함수는 입력을 정규화 하는 과정

정규화&표준화 관련 정보
https://heeya-stupidbutstudying.tistory.com/entry/%ED%86%B5%EA%B3%84-%EC%A0%95%EA%B7%9C%ED%99%94%EC%99%80-%ED%91%9C%EC%A4%80%ED%99%94

정규화는 여러 변수들 간에 영향력의 크기를 같게하기위해 보다 정규적인 상태로 만드는것, 스케일링도 포함
표준화는 표준정규분포를 따르는 값들로 바꾸는것

이진분류 입력된 값을 두 그룹으로 분류하는 작업

'비선형'이 뭔가?

123p
시그모이드 : 1/ (1+(e^(-x)))
시그모이드는 로지스틱 회귀에 자주 사용된다. 임계값 0.5를 기준으로 2 그룹으로 나눈다. 기울기 소실 있음, 출력값이 0~1이라 기울기 폭주 없음

이진 분류에서 손실함수로 MSE보다는 BCE(이진 교차 엔트로피)를 사용하는 이유는,
MSE는 예측과 실제값의 차이가 작으면 계산되는 오차가 작아서 학습이 어렵기 때문이다.
BSE는두 로그 함수가 동시에 사용되는데, 하나는 실제값이 0일때적용, 다른 하나는 실제값이 1일 때 적용한다
BCE1 = -Y_i*log(Y_i_hat)
BCE2 = (1-Y_i)*log(1_Y_i_hat)
보다시피 실제 Y값은 0, 1 중 하나라서 무조건 한 식이 날아간다

log a(N)에서 N(진수)이 0에 가까워질수록 값은 무한에 가까워진다
실제값이 0이면 BCE2만이 남게되는데, 예측이 잘못될수록 1-Y_i_hat이 0에 가까워져서 손실함수값이 (무한에가깝게)매우커진다
실제값이 1이면 BCE1만이 남게되는데, 예측이 잘못될수록 Y_i_hat이 0에 가까워져서 손실함수값이 (무한에가깝게)매우커진다

127p 설명 그림 3.16에서 ~ 부터 기울기 관련 내용 한 번 더 읽어 볼것