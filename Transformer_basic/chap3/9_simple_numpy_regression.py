import numpy as np

# 데이터 선언
# (30, 1) 벡터/ (30, )이나 (30, 2)에도 적용가능... 단변량? 다변량?
x = np.array(
    [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20],
     [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]])

y = np.array([[0.94], [1.98], [2.88], [3.92], [3.96], [4.55], [5.64], [6.3], [7.44], [9.1],
              [8.46], [9.5], [10.67], [11.16], [14], [11.83], [14.4], [14.25], [16.2], [16.32],
              [17.46], [19.8], [18], [21.34], [22], [22.5], [24.57], [26.04], [21.6], [28.8]])

# 하이퍼파라미터 초기화
weight = 0.0
bias = 0.0
learning_rate = 0.005
# 이번 예제에서 0.007이면 발산해서 값 터진다. 0.005정도로 하면 값을 학습을 많이 시켜야함 0.006이면 값이 좀 튀다 돌아감, 0.001은 천천히 오차가 바뀜

# epoch
for epoch in range(10000):
    # 가설과 손실함수
    y_hat = weight * x + bias
    cost = ((y - y_hat) ** 2).mean() # 30개를 동시에 하므로 batch 30

    # 가중치들 갱신
    weight = weight - learning_rate *((y_hat - y) * x).mean() # 가중치로 편미분한 값 (y_hat - y) * x
    bias = bias - learning_rate * (y_hat - y).mean() # 편향으로 미분한 값 (y_hat - y)

    # 학습 기록 출력
    if(epoch + 1) % 1000 == 0:
        print(f'Epoch : {epoch+1:4d}, Weight: {weight:.3f}, Bias: {bias:.3f}, Cost: {cost:.3f}')