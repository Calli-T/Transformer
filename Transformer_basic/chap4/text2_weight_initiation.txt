170p
적절한 초기화는 기울기 폭주나 소실 문제 완화가능
모델의 수렴  속도 향상을 통한 학습 프로세스 개선 가능

상수 초기화: 같은 숫자로 모든 가중치 초기화
그렇게 하면 대칭 파괴 현상이 일어나 학습에 문제가 생긴다.
대칭 파괴: 초기 가중치가 동일한 상수일 때 모든 노드가 동일한 출력을 생성하여 네트워크가 어려워지는 현상

무작위 초기화: 초기 가중치의 값을 무작위 값이나 특정 분포 형태로 초기화
ex) 정규 분포의 형태로 가중치의 값을 초기화
무작위, 균등, 정규 분포, 잘린 정규 분포, 희소 정규 분포 초기화 등등이 있음
-> 대칭 파괴 문제는 발생 안한다
--> 그러나 계층이 많으면 활성화 값이 양 끝단에 치우쳐 기울기 소실 현상이 발생

172p
제이비어&글로럿 초기화
해당 초기화는 균등 분포나 정규 분포를 사용해 가중치를 초기화 하는 방법이다.
각 노드의 출력 분산이 입력 분산과 동일하도록 가중치를 초기화하는 방법

해당 페이지의 U(-a, a)는 uniform 즉 균등분포를 의미한다. (알파벳 U가 맞는지, 맞다면 필기체가 뭔지는 모르겠다)
균등분포는 이산확률변수와 연속확률변수모두 가능하며 연속의 경우 U(a, b)인 균등분포가 [a, b] 구간에서 균등한 확률을 가진다.

균등 분포의 경우
W = U(-a, a)에서 무작위로 선택,
a = gain * (6/(fan_in+fan_out))^0.5
fan_in: 이전 계층의 노드 수
fan_out: 다음 계층의 노드 수
gain: scale을 의미

정규 분포의 경우
W = N(0, std^2)에서 무작위로 선택,
std = gain * (2/(fan_in+fan_out))^0.5

입력 데이터의 분산이 출력 데이터에서 유지되도록 가중치를 초기화
-> 시그모이드와 하이퍼볼릭 탄젠트를 활성화 함수로 사용하는 네트워크에서 효과정

카이밍 & 허 초기화
균등 분포의 경우
W = U(-a, a)에서 무작위로 선택
a = gain * (3/fan_in)^0.5

정규 분포의 경우
W = N(0, std^2)에서 무작위로 선택
std = gain/fan_in^0.5

제이비어 초기화 처럼 각 노드의 출력 분산이 입력분산과 동일하도록 초기화
그러나 현재 계층의 입력 뉴런으로만 기반한다.
제이비어의 단점을 극복했다고 하는데, 뭔지 모르겠다.

https://076923.github.io/posts/AI-8/
https://velog.io/@cha-suyeon/DL-%EA%B0%80%EC%A4%91%EC%B9%98-%EC%B4%88%EA%B8%B0%ED%99%94Weights-Initialization#xavier-%EC%B4%88%EA%B8%B0%ED%99%94
https://gr-st-dev.tistory.com/426
https://techblog-history-younghunjo1.tistory.com/236
여기에 이론적 기반이 있다고 한다.
입력값이 음수일 때 분산을 더 크게 주어 분산을 유지해 렐루에 더 적합하다? 이게 무슨 소리...

직교초기화
특이값분해를 활용해 자기 자신을 제외한 나머지 모든 열, 행 벡터와 직교하면서
동시에 단위 벡터인 행렬을 만드는 방법

LTSM, GRU, RNN에서 주로 사용된다고 한다.
직교 행렬의 고윳값의 절댓값은 1 ->
행렬곱을 여러 번 수행하더라도 기울기 폭주나 소실x

가중치 행렬의 고윳값이 1에 가까워지도록해 RNN에서 Gradient Vanishing을방지,
직교 초기화는 모델이 특정 초기화 값에 지나치게 민감해짐, 순방향 신경망에서는 사용x

176p
torch.apply가 가중치 초기화 범용 적용함수
해당 함수에 가중치 초기화 함수를 넣음
초기화 함수는 module 매게변수 추가, 초기화 메서드에서 선언한 모델의 매개변수를 의미
module.weight/module.bias로 사용
객체 식별함수 isinstance로 선형 변환 함수인지 확인하고 선형 변환 함수일 때 가중치와 편향을 초기화
하위 모듈에 재귀적으로 적용됨

self.modules은 네트워크의 모든 모듈을 반복자로 반환가능, 이걸로도 초기화 가능하다