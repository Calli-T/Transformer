def get_hparams(project_name='SinChangSeop'):
    from torch import device, cuda
    # 신창섭 epoch 모델을 둘로 쪼갠 것 기준
    device = device('cuda' if cuda.is_available() else 'cpu')

    hparams = {
        "project_name": project_name,
        "model_pt_epoch": None,  # 학습'된' epoch

        # "raw_wave_path": "raw/L-O-V-E-[cut_12sec].wav",  # "raw/L-O-V-E_[cut_6sec].wav",
        "raw_wave_dir_path": "raw/source",

        # for vocoder, NsfHiFiGAN
        # "vocoder": "nsf_hifigan.NsfHifiGAN",
        "device": device,
        "vocoder_ckpt": "models/nsf_hifigan/model", #"models/nsf_hifigan_20221211/model",  # "models/nsf_hifigan/model"
        "audio_sample_rate": 44100,
        "audio_num_mel_bins": 128,
        "fft_size": 2048,
        "win_size": 2048,
        "hop_size": 512,
        "use_nsf": True,
        "fmax": 16000,
        "fmin": 40,

        # for self_regressive_phonetic, HuBERT
        "hubert_gpu": True,
        "pt_path": 'models/hubert_soft.pt',

        # for Pitch Extractor, CREPE
        "f0_bin": 256,
        "f0_max": 1100.0,
        "f0_min": 40.0,
        # "audio_sample_rate": 44100,
        # "hop_size": 512,

        # for embedding & denoise model & optimizer
        "model_path": f"models/singer/{project_name}",

        # for condition integrate & preprocess
        "max_frames": 42000,
        "max_input_tokens": 60000,
        "pitch_norm": "log",
        # "emb_model_path": f"models/singer/{project_name}/embedding_model_epochs_{pt_epoch}.pt",

        "hidden_size": 256,

        # for wavenet
        # "hidden_size": 256,
        "residual_layers": 20,
        "residual_channels": 384,
        "dilation_cycle_length": 4,
        # "audio_num_mel_bins": 128,
        # "device": device,
        # "wavenet_model_path": f"models/singer/{project_name}/wavenet_model_epochs_{pt_epoch}.pt",

        # for diffusion
        "schedule_name": "linear",
        "steps": 1000,  # 1000,

        # for postprocess, pitch normalization
        "spec_max": [0.37696552276611328125, 0.7611109614372253418, 0.96147447824478149414, 0.94839864969253540039,
                     1.0016924142837524414, 1.0556684732437133789, 0.99750626087188720703, 0.88581115007400512695,
                     0.80999422073364257812, 0.99481767416000366211, 0.86467456817626953125, 0.88771653175354003906,
                     0.98057854175567626953, 0.97079831361770629883, 0.85024666786193847656, 0.94042497873306274414,
                     0.93353897333145141602, 0.82874661684036254883, 0.93965864181518554688, 0.92198848724365234375,
                     0.87757027149200439453, 0.83953630924224853516, 0.90773725509643554688, 0.9040305018424987793,
                     0.86565601825714111328, 0.91427761316299438477, 0.76131439208984375, 0.86426794528961181641,
                     0.81200361251831054688, 0.76683658361434936523, 0.65447574853897094727, 0.58945637941360473633,
                     0.73730242252349853516, 0.58153253793716430664, 0.52254378795623779297, 0.51248180866241455078,
                     0.66857695579528808594, 0.44008105993270874023, 0.4819163978099822998, 0.39687556028366088867,
                     0.37198197841644287109, 0.28313672542572021484, 0.36706569790840148926, 0.31333264708518981934,
                     0.28922358155250549316, 0.38932779431343078613, 0.27716818451881408691, 0.43567618727684020996,
                     0.25070706009864807129, 0.17472685873508453369, 0.2503413856029510498, 0.29124423861503601074,
                     0.35157439112663269043, 0.33553272485733032227, 0.33644786477088928223, 0.2060880511999130249,
                     0.34557881951332092285, 0.23908776044845581055, 0.25744685530662536621, 0.24397583305835723877,
                     0.12321889400482177734, 0.23025861382484436035, 0.12629531323909759521, 0.073224186897277832031,
                     0.23700372874736785889, 0.26172262430191040039, 0.17404615879058837891, 0.23840382695198059082,
                     0.33827310800552368164, 0.31357243657112121582, 0.22102297842502593994, 0.10551225394010543823,
                     0.20495238900184631348, 0.33991160988807678223, 0.43778112530708312988, 0.18093897402286529541,
                     0.16904622316360473633, 0.13515196740627288818, 0.015710860490798950195, 0.19318474829196929932,
                     0.026703160256147384644, 0.19713811576366424561, -0.090489596128463745117, 0.12009355425834655762,
                     -0.096993677318096160889, 0.043296497315168380737, -0.056618817150592803955,
                     -0.053609434515237808228,
                     0.029818575829267501831, -0.20580041408538818359, -0.15144784748554229736, -0.18208479881286621094,
                     -0.22843039035797119141, -0.038739603012800216675, -0.072572365403175354004,
                     -0.14683867990970611572,
                     -0.15509653091430664062, -0.14012479782104492188, -0.17522758245468139648, -0.2233289182186126709,
                     -0.34439447522163391113, -0.33541303873062133789, -0.41686710715293884277, -0.41590991616249084473,
                     -0.64841097593307495117, -0.69038063287734985352, -0.55461424589157104492, -0.53616082668304443359,
                     -0.69005733728408813477, -0.63245695829391479492, -0.67213106155395507812, -0.73040419816970825195,
                     -0.87495023012161254883, -0.85097140073776245117, -1.1127724647521972656, -1.0957421064376831055,
                     -1.1007906198501586914, -1.2266260385513305664, -1.1821234226226806641, -1.2567455768585205078,
                     -1.2328075170516967773, -1.2310401201248168945, -1.2517137527465820312, -1.4006371498107910156,
                     -1.5443300008773803711, -1.5130364894866943359, -1.5695074796676635742, -1.7890214920043945312, ],
        "spec_min": [-4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656,
                     -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, -4.9999947547912597656, ],
        "keep_bins": 128,
        "mel_vmax": 1.5,
        "mel_vmin": -6.0,

        # for train & infer
        "use_norm": True, # True일경우, infer도 norm 해주는거 잊지말 것
        # for train & dataset
        "train_dataset_path_input": f"train_dataset/{project_name}/raw",
        "train_dataset_path_output": f"train_dataset/{project_name}/separated",
        "train_dataset_path_f0": f"train_dataset/{project_name}/f0",  # f0만 따로 저장해두는 장소
        "train_dataset_path_spec_minmax": f"train_dataset/{project_name}",

        "use_extract": False,  # True,  # MR제거인데 일단 적용안되는 문제가 있음

        "batch_size_train": 1,  # dummy
        "BATCH_SIZE_TRAIN": 16,
        "LEARNING_RATE": 0.0005,  # 04,
        "WEIGHT_DECAY": 0.00001,
        "train_target_epochs": 100000,
        "number_of_savepoint": 3,
        "save_interval": 1,
        "val_ratio": 0.1,
    }

    return hparams


hparams = get_hparams('opencpop')

'''
"spec_max": [0.37696552, 0.76111096, 0.9614745, 0.94839865, 1.0016924, 1.0556685, 0.99750626, 0.88581115, 0.8099942,
                 0.9948177, 0.86467457, 0.88771653, 0.98057854, 0.9707983, 0.85024667, 0.940425, 0.933539, 0.8287466,
                 0.93965864, 0.9219885, 0.8775703, 0.8395363, 0.90773726, 0.9040305, 0.865656, 0.9142776, 0.7613144,
                 0.86426795, 0.8120036, 0.7668366, 0.65447575, 0.5894564, 0.7373024, 0.58153254, 0.5225438, 0.5124818,
                 0.66857696, 0.44008106, 0.4819164, 0.39687556, 0.37198198, 0.28313673, 0.3670657, 0.31333265,
                 0.28922358, 0.3893278, 0.27716818, 0.4356762, 0.25070706, 0.17472686, 0.2503414, 0.29124424, 0.3515744,
                 0.33553272, 0.33644786, 0.20608805, 0.34557882, 0.23908776, 0.25744686, 0.24397583, 0.123218894,
                 0.23025861, 0.12629531, 0.07322419, 0.23700373, 0.26172262, 0.17404616, 0.23840383, 0.3382731,
                 0.31357244, 0.22102298, 0.105512254, 0.20495239, 0.3399116, 0.43778113, 0.18093897, 0.16904622,
                 0.13515197, 0.01571086, 0.19318475, 0.02670316, 0.19713812, -0.090489596, 0.120093554, -0.09699368,
                 0.043296497, -0.056618817, -0.053609435, 0.029818576, -0.20580041, -0.15144785, -0.1820848,
                 -0.22843039, -0.038739603, -0.072572365, -0.14683868, -0.15509653, -0.1401248, -0.17522758,
                 -0.22332892, -0.34439448, -0.33541304, -0.4168671, -0.41590992, -0.648411, -0.69038063, -0.55461425,
                 -0.5361608, -0.69005734, -0.63245696, -0.67213106, -0.7304042, -0.87495023, -0.8509714, -1.1127725,
                 -1.0957421, -1.1007906, -1.226626, -1.1821234, -1.2567456, -1.2328075, -1.2310401, -1.2517138,
                 -1.4006371, -1.54433, -1.5130365, -1.5695075, -1.7890215, ],
    "spec_min": [-4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948,
                 -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, -4.9999948, ],
'''

'''"spec_max": [0.5272776484489441, 0.9114222526550293, 1.1117855310440063, 1.0987094640731812, 1.1520036458969116,
                 1.205979347229004, 1.1478168964385986, 1.0361219644546509, 0.9603051543235779, 1.1451283693313599,
                 1.0149853229522705, 1.0380277633666992, 1.1308894157409668, 1.1211094856262207, 1.000557541847229,
                 1.0907360315322876, 1.0838499069213867, 0.9790574312210083, 1.089969515800476, 1.0722992420196533,
                 1.0278816223144531, 0.9898473620414734, 1.0580487251281738, 1.054341435432434, 1.015966534614563,
                 1.0645880699157715, 0.911625325679779, 1.0145788192749023, 0.9623145461082458, 0.9171468019485474,
                 0.8047866225242615, 0.7397677898406982, 0.8876128792762756, 0.7318429350852966, 0.6728546619415283,
                 0.6627925038337708, 0.8188887238502502, 0.5903922319412231, 0.6322281360626221, 0.5471858382225037,
                 0.5222923755645752, 0.4334467053413391, 0.5173780918121338, 0.46364495158195496, 0.4395354688167572,
                 0.5396381616592407, 0.42747896909713745, 0.5859876275062561, 0.40101614594459534, 0.3250364363193512,
                 0.400651216506958, 0.4415549337863922, 0.5018853545188904, 0.48584455251693726, 0.4867593050003052,
                 0.3564012944698334, 0.49589088559150696, 0.3893994688987732, 0.4077587425708771, 0.3942872881889343,
                 0.273531049489975, 0.3805701732635498, 0.2766077518463135, 0.22353535890579224, 0.38731473684310913,
                 0.4120328426361084, 0.3243582844734192, 0.388715535402298, 0.4885849356651306, 0.4638833701610565,
                 0.37133342027664185, 0.25582262873649597, 0.3552630543708801, 0.4902227818965912, 0.5880915522575378,
                 0.33124953508377075, 0.31935736536979675, 0.28546303510665894, 0.16602271795272827, 0.3434958755970001,
                 0.17701327800750732, 0.34745070338249207, 0.05982062965631485, 0.2704027593135834, 0.05331733450293541,
                 0.19360707700252533, 0.0936945453286171, 0.09670297801494598, 0.18013012409210205,
                 -0.05549187958240509, -0.0011373111046850681, -0.03177299723029137, -0.07811982184648514,
                 0.11156868189573288, 0.07773900777101517, 0.0034726052545011044, -0.004783670883625746,
                 0.010186700150370598, -0.02491397224366665, -0.07301700115203857, -0.19408515095710754,
                 -0.18510079383850098, -0.26655352115631104, -0.26560330390930176, -0.4981132745742798,
                 -0.5400726199150085, -0.40430617332458496, -0.3858506381511688, -0.5397453904151917,
                 -0.4821416139602661, -0.5218197703361511, -0.580097496509552, -0.7246339321136475, -0.700655996799469,
                 -0.9624589681625366, -0.9454383254051208, -0.9504799842834473, -1.0763174295425415,
                 -1.0317856073379517, -1.1064244508743286, -1.0824952125549316, -1.0807260274887085,
                 -1.1013997793197632, -1.2503222227096558, -1.394013524055481, -1.362666130065918, -1.492540717124939,
                 -1.6387512683868408],
    "spec_min": [-4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126, -4.99999475479126,
                 -4.99999475479126, -4.99999475479126, -4.99999475479126],'''

'''
import argparse

from torch import device, cuda

device = device('cuda' if cuda.is_available() else 'cpu')


def get_parsed_dict():
    parser = argparse.ArgumentParser()
    keys_int = ['IMAGE_SIZE', 'in_channels', 'out_channels', 'num_res_blocks', 'num_channels', 'num_heads',
                'BATCH_SIZE_TRAIN', 'DATASET_REPETITION',
                'EPOCHS', 'save_interval', 'BATCH_SIZE_SAMPLE', 'steps']
    keys_float = ['dropout', 'LEARNING_RATE', 'WEIGHT_DECAY', 'EMA', ]
    keys_str = ['data_path', 'model_path', 'schedule_name', "attention_resolutions"]
    keys_bool = ['learn_sigma', 'use_scale_shift_norm', ]

    for key in keys_int:
        parser.add_argument(f'--{key}', default=None, type=int, help='')
    for key in keys_float:
        parser.add_argument(f'--{key}', default=None, type=float, help='')
    for key in keys_str:
        parser.add_argument(f'--{key}', default=None, type=str, help='')
    for key in keys_bool:
        parser.add_argument(f'--{key}', default=None, type=bool, help='')

    return vars(parser.parse_args())


def get_default():
    return {
        "device": device,
        # for model
        "IMAGE_SIZE": 256,
        "learn_sigma": True,
        "in_channels": 3,
        "out_channels": 6,
        "dropout": 0.0,
        "num_res_blocks": 2,
        "num_channels": 128,
        "num_heads": 1,
        "use_scale_shift_norm": False,

        "attention_resolutions": "16",
        "channel_mult": (1, 1, 2, 2, 4, 4),
        # for training
        "BATCH_SIZE_TRAIN": 2,
        "DATASET_REPETITION": 1,
        "data_path": "./datasets/mint_mini",
        "model_path": "./models/mint_mini",
        "LEARNING_RATE": 0.0001,
        "WEIGHT_DECAY": 0.00001,
        "EPOCHS": 200000,
        "save_interval": 2000,
        "EMA": 0.999,
        # for sampling
        "BATCH_SIZE_SAMPLE": 16,
        # for diffusion
        "schedule_name": "linear",
        "steps": 1000,
    }


def get_hparams():
    defaults = get_default()
    args_parsed = get_parsed_dict()

    for key in args_parsed:
        if args_parsed[key] is not None:
            defaults[key] = args_parsed[key]

    if defaults["IMAGE_SIZE"] == 256:
        defaults["channel_mult"] = (1, 1, 2, 2, 4, 4)
    elif defaults["IMAGE_SIZE"] == 64:
        defaults["channel_mult"] = (1, 2, 3, 4)
    elif defaults["IMAGE_SIZE"] == 32:
        defaults["channel_mult"] = (1, 2, 2, 2)

    attention_ds = []
    for res in defaults["attention_resolutions"].split(","):
        attention_ds.append(defaults["IMAGE_SIZE"] // int(res))

    defaults["attention_resolutions"] = tuple(attention_ds)

    return defaults
'''
