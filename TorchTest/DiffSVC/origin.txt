DiffSVC
https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/diffsvc/
https://arxiv.org/pdf/2105.13871
논문리뷰와 논문이다

{ 논문리뷰 발췌 내용
SVC란 content와 멜로디를 그대로 두고 노래의 목소리만 다른 가수
최근의 SVC 모델은 content encoder를 학습시켜 source로부터 content feature를 추출시킨다음
conversion model을 학습시켜 content feature를 다른 feature로 변형 시킨다고한다
(이 때 content는 음성이며 content encoder의 결과물은 content vecter이다)
encoder와 conversion model은 연결하여 AE처럼 학습 시키는 경우도 있고, 각각 학습하는 경우도 있다.

각각 학습 시키는 경우 자동 음성 인식 모델 즉 ASR를 content encoder로 사용한다.
ASR 모델은 end-to-end 모델이거나 히든마르코프-심층신경망 하이브리드 모델이라고 한다.

conversion model은 GAN이나 회귀 모델을 사용하는데
GAN의 경우 content feature에서 바로 waveform(파형)을 만들어 내고,
regression model이 사용된 경우
content vector(feature) 추출 -> 멜 스펙토그램등의 주파수 성분의 특성으로 변환 -> 추가로 학습시킨 vocoder로 생성
DiffSVC의 경우 후자의 경우이며 회귀 모델 대신 Diffusion model을 conversion model에 사용한다
}

멜스펙토그램이란?
https://judy-son.tistory.com/6
결론적으로 시간에 따른 주파수 성분의 변화를 잘 보여주는 방법이다.
x축이 시간, y축이 주파수(증가폭이 로그 스케일임), 그래프의 색은 소리의 크기이다
mel(f) = 2595 log2(1+f/700)

일반적인 SVC의 순서는(gemini 피셜)
멜스펙토그램 - 인코딩 - 변환 모델 - 디코딩
DiffSVC의 경우가 아니다!!!!

ASR 모델의 출력은 phonetic posteriorgram이며,
Diffusion model은 content, melody, loudness의 feature를 조건으로 받아
가우스 잡음으로부터 멜 스펙토그램을 점진적으로 복구한다고한다
(복구하는게 멜 스펙토그램이라면, 이걸 다시 디코딩을 해야겠네?)

모델 구조 설명 구간
{
    {   이 셋은 elementwise하게 더해져 conditioner e가 되며, 이는 diffusion decoder의 추가 토큰이 된다.
        입력으로 PPG, Log_F0, and Loudness

        원본 데이터를 ASR에 넣으면 PPG가 되며, ※ PPG는 음성 후방 확률 그림으로, 시간 경과에 따른 음소 별 확률을 나타낸다.
        ※ 원본 모델은 중국에서 만들어졌으니 중국어 SVC를 위한 음소(phonemes)를 ground truth table로 사용했다고한다.
        이를 FC 레이어로 이루어진 PPG Prenet에 입력한다.

        melody는 기본 주파수 값에 로그를 씌운 contour(Log-F0 = Log Fundamental Frequency)로 표현된다.
        ※ 기본 주파수란 여러 주파수가 섞인 음에서 주기가 가장 긴(= 주파수가 가장 낮은)음이다. 이를 기본으로 음의 높낮이를 계산한다.
        ※ 음색은 기본 주파수에서 그 사이 겹친 다양한 파형의 조합에 따라 파동의 모양이 달라져서 다른 것이다.
        ※ 옥타브는 로그 스케일의 밑을 2로 사용한다. 즉 한 옥타브가 올라갈 때마다 같은 음에서 주파수가 2배로 늘어난다.

        loudness는 A-weighting 방식을 사용한다.
        이는 인간의 인지를 반영해 (정확히는 40폰 등첨감곡선, 같은 크기로 느껴지는 소리를 주파수 별로 나타낸 곡선 기반) 데시벨에 보정을 한 값이다.
        기본 데시벨 값에 더해주면 되며, 그 값은
        A_weighting(f) = 20 log10(1 + (12994/f)^4) + 20 log10(1 + (6300/f)^2) - 20 log10(1 + (f/20)) - 20 log10(1 + (f/25))이다.

        이후 melody와 Loudness는 각각 f_0와 l로 표현되며, 둘 다 임베딩이 되고 처리 방식이 같다.
        둘 다 256 bin(원문도 그렇지만, bit가 아니다!)으로 양자화 한 다음 각각의 임베딩용 룩업 테이블을 통과해서 만들어진다.
    }

    { 시간단계 t
        사인파 임베딩과 비슷하다? 사인파 위치 인코딩이라고한다, 128차원이다
        t_emb(t_ = [sin(10^((0x4)/63))t, ... , sin(10^((63x4)/63))t, cos(10^((0x4)/63))t, ... , cos(10^((63x4)/63))t]
        어째 GDL책에서 본 거랑 굉장히 유사한데?
    }
}


