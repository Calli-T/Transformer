DiffSVC
https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/diffsvc/
https://arxiv.org/pdf/2105.13871
논문리뷰와 논문이다

{ 논문리뷰 발췌 내용
SVC란 content와 멜로디를 그대로 두고 노래의 목소리만 다른 가수
최근의 SVC 모델은 content encoder를 학습시켜 source로부터 content feature를 추출시킨다음
conversion model을 학습시켜 content feature를 다른 feature로 변형 시킨다고한다
(이 때 content는 음성이며 content encoder의 결과물은 content vecter이다)
encoder와 conversion model은 연결하여 AE처럼 학습 시키는 경우도 있고, 각각 학습하는 경우도 있다.

각각 학습 시키는 경우 자동 음성 인식 모델 즉 ASR를 content encoder로 사용한다.
ASR 모델은 end-to-end 모델이거나 히든마르코프-심층신경망 하이브리드 모델이라고 한다.

conversion model은 GAN이나 회귀 모델을 사용하는데
GAN의 경우 content feature에서 바로 waveform(파형)을 만들어 내고,
regression model이 사용된 경우
content vector(feature) 추출 -> 멜 스펙토그램등의 주파수 성분의 특성으로 변환 -> 추가로 학습시킨 vocoder로 생성
DiffSVC의 경우 후자의 경우이며 회귀 모델 대신 Diffusion model을 conversion model에 사용한다
}

멜스펙토그램이란?
https://judy-son.tistory.com/6
결론적으로 시간에 따른 주파수 성분의 변화를 잘 보여주는 방법이다.
x축이 시간, y축이 주파수(증가폭이 로그 스케일임), 그래프의 색은 소리의 크기이다
mel(f) = 2595 log2(1+f/700)

일반적인 SVC의 순서는(gemini 피셜)
멜스펙토그램 - 인코딩 - 변환 모델 - 디코딩
DiffSVC의 경우가 아니다!!!!

ASR 모델의 출력은 phonetic posteriorgram이며,
Diffusion model은 content, melody, loudness의 feature를 조건으로 받아
가우스 잡음으로부터 멜 스펙토그램을 점진적으로 복구한다고한다
(복구하는게 멜 스펙토그램이라면, 이걸 다시 디코딩을 해야겠네?)

※ 정황상 DiffSVC는 DDPM기반 모델이며, 잡음을 복구할 때 추가 조건(음성을 포함한)을 달아주는 식으로 변형한 모델인듯하다
※ UNet대신 WaveNet으로 잡음을 예측한다
모델 구조 설명 구간
{
    {   이 셋은 elementwise하게 더해져 conditioner e가 되며, 이는 diffusion decoder의 추가 토큰이 된다.
        입력으로 PPG, Log_F0, and Loudness

        원본 데이터를 ASR에 넣으면 PPG가 되며, ※ PPG는 음성 후방 확률 그림으로, 시간 경과에 따른 음소 별 확률을 나타낸다.
        ※ 원본 모델은 중국에서 만들어졌으니 중국어 SVC를 위한 음소(phonemes)를 ground truth table로 사용했다고한다.
        이를 FC 레이어로 이루어진 PPG Prenet에 입력한다.

        melody는 기본 주파수 값에 로그를 씌운 contour(Log-F0 = Log Fundamental Frequency)로 표현된다.
        ※ 기본 주파수란 여러 주파수가 섞인 음에서 주기가 가장 긴(= 주파수가 가장 낮은)음이다. 이를 기본으로 음의 높낮이를 계산한다.
        ※ 음색은 기본 주파수에서 그 사이 겹친 다양한 파형의 조합에 따라 파동의 모양이 달라져서 다른 것이다.
        ※ 옥타브는 로그 스케일의 밑을 2로 사용한다. 즉 한 옥타브가 올라갈 때마다 같은 음에서 주파수가 2배로 늘어난다.

        loudness는 A-weighting 방식을 사용한다.
        이는 인간의 인지를 반영해 (정확히는 40폰 등첨감곡선, 같은 크기로 느껴지는 소리를 주파수 별로 나타낸 곡선 기반) 데시벨에 보정을 한 값이다.
        기본 데시벨 값에 더해주면 되며, 그 값은
        A_weighting(f) = 20 log10(1 + (12994/f)^4) + 20 log10(1 + (6300/f)^2) - 20 log10(1 + (f/20)) - 20 log10(1 + (f/25))이다.

        이후 melody와 Loudness는 각각 f_0와 l로 표현되며, 둘 다 임베딩이 되고 처리 방식이 같다.
        둘 다 256 bin(원문도 그렇지만, bit가 아니다!)으로 양자화 한 다음 각각의 임베딩용 룩업 테이블을 통과해서 만들어진다.
    }

    { 시간단계 t
        사인파 임베딩과 비슷하다? 사인파 위치 인코딩이라고한다, 128차원이다
        t_emb(t_ = [sin(10^((0x4)/63))t, ... , sin(10^((63x4)/63))t, cos(10^((0x4)/63))t, ... , cos(10^((63x4)/63))t]
        어째 GDL책에서 본 거랑 굉장히 유사한데?

        여튼 128차원 벡터를 (FC레이어 + Swish함수)x2번을 거쳐 출력한다
        (출력물의 형태는 몰?루 멜 스펙토그램을 1x1합성곱에 넣은것과 형태가 동일한 모양이다)
    }

    {   Diffusion decoder
        잡음 예측에 자기회귀적 합성곱 신경망(딜레이드 컨볼루션)인 Wavenet을 쓴다
        https://james-scorebook.tistory.com/entry/WaveNet-A-Generative-Model-for-Raw-Audio 참고

        잡음낀 멜 스펙토그램, timestep t(의 변형), embedding(아까 3개 정보 뭉친거)를 받아 잡음을 예측한다

        음성을 멜 스펙토그램으로 만들 때의 세부 사항은 Experiments에 있다.
        ※ 만들 때는 라이브러리, 다시 음성으로 디코딩할 때는 Hifi-GAN!!!
        ※ prophesier 기준 librosa라이브러리를 사용한다.
    }
}
※ 이 모델이 다 끝나고, 결과물은 잡음이 없겠지만 여전히 멜 스펙토그램이므로
이를 waveform으로 변환시키려면 모델이 필요하고, 이를 위한 vocoder로 Hifi-GAN을 쓴다고한다.

------------------------------------------------------------------------------------------------------------------------------------------------------

Diff-SVS
대체 왜 그런지 모르겠으나 디퓨전 모델을 쓰는 SVC 모델들이 죄다 Diff-SVS 기반이다...???
SVC의 원작자의 논문에는 샘플만 있고 깃허브가 없다
SVS의 깃허브와 논문 주소는 다음과 같다

DiffSinger
https://github.com/MoonInTheRiver/DiffSinger?tab=readme-ov-file
깃허브와
https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/diffsinger/
논문리뷰

{   SVS 구현체 모음
    한글 Diff-SVS 구현체는 여기에 있다
    https://github.com/wlsdml1114/diff-svc

    자주 사용하는 프로페시아의 모델
    https://github.com/prophesier/diff-svc

    SVS의 근원 깃허브
    https://github.com/MoonInTheRiver/DiffSinger

    openvpi 버전
    https://github.com/openvpi/DiffSinger
}

논문 리뷰 & 번역

---------- 초록 ----------
{   이미 알거나 딱히 안중요한 내용인듯
    SVS는 악보를 입력으로 받아 고품질이고 표현력 있는 노래 목소리를 합성하는 모델이다.
    기존의 SVS 모델은 간단한 손실함수 L1, L2나 GAN을 사용해 음향 특징(멜 스펙트로그램 등)을 재구성하나
    과도한 smoothing과 불안정한 학습 문제로 자연스럽지 못하다.

    DiffSinger는 DPM을 기반으로 하는 노래 합성 모델이다.
    DiffSinger는 악보를 조건으로 잡음을 멕 스펙트로그램으로
    반족 변환하는 매개화된 마르코프 체인이다
    변분 경계를 암시적으로 최적화 함으로 안정적으로 학습할 수 있고
    현실적인 출력을 생성할 수 있다.
}

음질과 추론 속도 향상을 위해
(간단한 손실함수로 학습된 사전 지식을 더 잘 활용할 수 있는) 얕은 확산 기작을 도입합니다.
구체적으로 DiffSinger는 멜 스펙트로그램의 실제 확산경로와
간단한 멜 스펙트로그램 디코더가 예측한 확산 경로의 교차점에 따라
총 확산 단계에서 작은 얕은 단계에서 생성을 시작합니다.
-> 핵심 기작(메커니즘) 중 하나인듯
또한 경계 예측 방법을 제안하여 교차점을 찾고 얕은단계를 적응적으로 결정합니다
-> 위와 관계되는 핵심 기작 중 하나인듯

나머지는 노래 합성 작업 성능이 뛰어나다는 내용과
TTS로의 일반화를 얘기하는듯



---------- 서론 ----------

{ 딱히 안중요
    노래 합성(SVS)**은 음악 악보에서 자연스럽고 표현력 있는 노래 목소리를 합성하는 것을 목표로 하며,
    연구 커뮤니티와 엔터테인먼트 산업에서 점점 더 주목받고 있습니다.
}
SVS의 파이프라인은 일반적으로 악보를 조건으로 주어 음향 특징(멜 스펙트로그램 등)을 생성하는 음향 모델과
음향 특징을 파형으로 변환하는 Vocoder로 구성되어 있습니다.
-> 음성 그 자체에서 멜 스펙트로그램을 생성하는 SVC와는 좀 다른 부분,
멜 스펙트로그램을 Vocoder로 파형으로 변환하는 것은 같다.
나중에 SVC의 것으로 Encoder를 변형하면, SVS를 SVC로 바꾸어 만들 수 있을까?

{ 구현에는 딱히 안중요
    기존의 노래 음향 모델은 L1이나 L2와 같은 간단한 손실 함수를 사용하여 음향 특징을 재구성합니다.
    그러나 이 최적화는 잘못된 단봉 분포(uni-modal distribution)을 기반으로 하여 흐릿하고 과도하게 부드러운 출력을 생성합니다.
    기존 방법이 GAN을 사용하여 이 문제를 해결하려 했으나, 효과적인 GAN을 훈련하는 것이
    불안정한 Discriminator로 인해 때때로 실패할 수 있습니다.
    이런 문제들은 합성된 노래의 자연스러움을 저해합니다.
}

{ 덜 중요한 내용
    Recently부터 fields까지의 내용은 디퓨전 모델 좋아요, 디퓨전 모델은 이렇게 생겼어요에 관한내용,
    DDPM 논문 정독하고 왔으니 필요없다
}

본 연구에서는 확산 모델을 기반으로 하는 음향 모델인 DiffSinger를 제안합니다.
DiffSinger는 음악 악보를 조건으로 노이즈를 멜 스펙트로그램으로 변환합니다.
DiffSinger는 적대적인 피드백 없이 ELBO를 최적화하여 효율적으로 훈련할 수 있으며,
실제 멜 스펙트로그램을 생성하여 지상 진실 분포와 강력하게 일치합니다.
-> 딴건 다 아는 내용이나 ground-truth는 명백한 사실인데, 무엇을 의미하는지를 알아야겠다.
악보를보고 생성한 것(=ground truth)과 실제 음성의 mel spectrogram이 일치률이 높은 모양이다.

(음질을 향상시키고 추론 속도를 높이기 위해
간단한 손실 함수로 학습된 사전 지식을 더 잘 활용할 수 있는)
얕은 확산 메커니즘을 도입합니다
구체적으로, (ground-truth의 mel-spectrogram인) M과
(간단한 mel-spectrogram 디코더가 예측한) ~M의
확산 궤적(trajectories)에서 교차점이 존재함을 발견했습니다.
확산 단계가 충분히 클 때 M과 ~M을 확산 과정으로 보내면
유사하게 뒤틀린(similar distorted) 멜 스펙트로그램을 생성할 수 있습니다.
-> 아마도, 정방향 확산 과정에서 T가 충분히 크다면, 뒤틀린(아마도 잡음을 얘기하는 모양이다)
멜 스펙트로그램이 생성되며 서로 유사한 모양을 가지게 되는 모양이다
(하지만 왜곡된 멜 스펙트로그램이 가우시안 흰색 노이즈가 되는 깊은 단계에 도달하지 않을 때)
-> 확산 과정은 모든 단계에서 정규분포를 따르지 않던가??? 왜 이리 설명한거지
요약: 악보 예측 멜 스펙트로그램과 실제 멜 스펙트로그램은 확산 단계 T가 많을 때 유사합니다.
그러나

그러므로 추론 단계는
1. 간단한 멜 스펙트로그램 디코더로 악보를 받아 멜 스펙트로그램을 생성하고
2. 확산 과정의 얕은 단계 k에서 샘플 ~Mk를 계산하고 ※ 멜 스펙트로그램에 k단계 만큼 잡음을 먹인다는뜻
3. (가우스 백색 소음 말고) ~Mk에서 역방향 확산 과정을 시작하며, 잡음 제거 과정을 k회 반복합니다.
그 밖에 경계 예측 신경망을 이 교차점에 위치시켰으며, k를 적응적으로 결정할 것입니다.
-> 추론 단계에서 사용하는 k값은 '적응적으로' 결정된다는게, 모델의 실행 과정에서 변경되는 값인
것으로 추측한다. 아까 말하는 ground-truth와 유사해지는 단계 t값이 바로 k를 의미하는듯하다.

얕은 확산 기작은 가우스 백색 소음보다 더 나은 시작점을 제공하며,
역방향 확산 과정의 부담을 완화하여 합성된 오디오의 품질을 향상 시키고 추론 속도를 가속합니다.
-> 여기 말하는 가우스 백색 소음이라는게, N(0, I)에서 뽑은 무작위 값인 것으로 추측한다.

마지막으로, SVS의 파이프라인이 TTS와 유사하기 때문에 일반화를 위해
DiffSinger에서 조정한 DiffSpeech도 구축했습니다.
-> 코드에 있나 설마?
아래 내용은 우리 SVS랑 우리 TTS 좋아요 내용인듯

논문의 기여는 DPM 기반 SVS 제안/얕은 확산 기작 제안/TTS 확장 이 3개를 주장한다.



----------- 확산 모델 ----------

확산 확률 모델 이론을 소개한다
-> DPM 모델 요약인듯

확산 과정
{
    데이터 분포를 q(y_0)로, 샘플 y_0 ~ q(y_0)인것으로 정의한다.
    확산 과정은 고정된 매개변수의 마르코프 체인이며 y_0를 잠재 y_T로 T단계를 거쳐 변환한다.
    식은
    q(y_1:T|y_0) := Π_product(t=1, T, q(y_T|yt-1) 이며 ※ 이거 대문자 T가 아니라 소문자 t아닌가???
    각 확산 단계 t ∈ [1,T]에서 작은 가우스 잡음이 y_t-1에 추가되어 y_t가 생성되며
    이는 분산 스케줄 β = { β_1,...,β_T }를 따르며
    각 단계의 식은
    q(y_t|y_t-1) := N(y_t; sqrt(1-β_t)*y_t-1, β_tI) 이다.
    만약 β가 잘설계되었고 T가 충분히 크다면, q(y_T)는 등방성 가우스 분포가 된다. ※ 공분산행렬이 대각선 성분만을 가져 각 변수들 간에 관계가 없다는 뜻이다.
    게다가 확산 과정 q(y_t|y_0)는 O(1) 시간에 닫힌 형식으로 계산될 수 있다는 특별한 특징이 있습니다.
    q(y_t|y_0) = N(y_t; sqrt(α_bar)y_0, (1-α_bar)I),
    where α_bar := Π_product(s=1, T, α_s), α_t := 1 - β_t.
}

역방향 확산 과정
{
    역방향 과정은 y_t부터 y_0까지 학습가능한 파라미터 θ로된 마르코프 체인입니다.
    정확한 역변환 분산 q(y_t-1|y_t)는 추적불가능하기 때문에,
    이를 파라미터 θ를 가진 신경망으로 근사합니다. (θ는 모든 t번째 단계에 공유됩니다)
    그러므로 전체 역변환 과정은 다음과 같이 정의될 수 있습니다.
    p_θ(y_0:T) := p(y_T) * Π_product(t=1, T, p_θ(y_t-1|y_t))
}

학습
{
    θ를 학습하기 위해, 우리는 음의 로그 우도에 대한 변분 경계를 최소화 합니다.
    식은 다음과 같습니다
    E_q(y_0)[-log p_θ(y_0)] >=
    E_q(y_0, y_1,...,y_T)[log q(y_1:T|y_0)-log p_θ(y_0:T)] =: L
    효과적인 학습은 L의 무작위 항을 확률적 경사 하강법 (SGD)으로 최적화 하는 것이며 식은 다음과 같습니다.
    L_t-1 = D_KL(q(y_t-1|y_0) || p_θ(y_t-1|y_t)) ※ Eq. (3)
    where q(y_t-1|y_t, y_0) = N(y_t-1; ~μ_t(y_t, y_0), ~β_t*I)
    ~μ_t(y_t, y_0) :=
    y_0 * (sqrt(α_t-1_bar)β_t)/(1-α_t_bar) + y_t * (sqrt(α_t)*(1-α_t-1_bar))/(1-α_t_bar)
    where ~β_t := (1-α_t-1_bar)/(1-α_t_bar) β_t
    등식 3번은 다음과 동등합니다.
    L_t-1-C = E_q[(1/(2*σ_t^2)) * ||~μ_t(y_t, y_0)-μ_θ(y_t, t)||^2] ※ t단계에서 예측 평균과 실제 평균의 MSE에 가중치가 곱해진 값이다
    where C is a constant.
    그리고 등식 1번을 y_t(y_0, ε) = sqrt(α_t_bar)y_0 + sqrt(1-α_t_bar)*ε로 재매개변수화 하고
    매개변수화를 다음식
    μ_θ(y_t, t) = (1/sqrt(α_t)) * (y_t-(β_t/sqrt(1-α_t_bar))*(ε_θ(y_t, t)))
    선택함으로써 등식 4를 다음과 같이 간편화 할 수 있습니다.
    E_(y_0,ε)[((β_t^2)/(2σ_t^2α_t(1-α_t_bar))||ε-ε_θ(sqrt(α_t_bar)y_0+sqrt(1-α_t_bar)ε,t||^2]
    ※ 평균 차이의 식을 잡음 차이로 변경한 것이다.
    ※ DDPM기준 L_simple이라고 한 번 더 간소화 한 것도 존재함
    우리는 σ_t^2를 ~β_t로 설정할 수 있으며 ε~N(0,I)와 ε_θ(·)는 신경망의 출력이다.
}

샘플링
{
    y_T를 p(y_T)~N(0, I)에서 추출하고 역과정을 돌려 데이터 견본을 생성한다.
}



---------- DiffSinger ----------

그림 2에 그려진대로, DiffSinger는 확산 모델을 기반으로한다.
M이 mel-spectrogram이고 x가 M에 대응되는 악보일 때, SVS 작업이 조건부 분포 p_θ(M_0|x)를 모델링하기에
확산 잡음제거기의 역방향 과정에 x를 조건으로 추가할 수 있습니다.
이 장에서 먼저 DiffSinger의 naive version를 3.1절에서 서술하며,
그런 다음 모델의 성능과 효율을 향상시키기 위해 새로운 얕은 확산 매커니즘을 소개하며, 3.2절에
마지막으로 얕은 확산 매커니즘에 요구되는 교차점의 경계를 적응적으로 찾는 경계 예측 신경망을 서술합니다. 3.3절에서

3.1 DiffSinger의 naive version
(그림 2에서 점선이 그려진 상자를 제외한) DiffSinger의 Naive version의
(그림 2a에서 그려진) 학습 절차에서,
DiffSinger는 확산 과정의 t번째 단계에서 mel-spectrogram M_t를 입력으로
악보 x와 t를 조건으로 하여 (등식 6번에서의) 무작위 잡음 ε_θ(·)을 예측합니다.
-> M_t가 y_t인 모양
(그림 2b에서 그려진) 추론절차에서,
(이전의 확산 모델이 그랬던것처럼) N(0, I)에서 샘플링한 가우시안 백색 잡음에서 시작합니다.
그러고 절차는 중간 단계의 샘플에 T번만큼 잡음제거하며, 이는 두 단계를 거칩니다.
1) ε_θ(·)를 예측하고 2)이것과 등식2/등식5번을 사용하여 M_t에서 M_t-1를 생성합니다.
그리고 그 식은 다음과 같습니다. ※ DDPM 논문의 Algorithm 2에 해당하는 내용인듯하다.
M_t-1 = (1/sqrt(α_t)) * (M_t - ((1-α_t)/(sqrt(1-α_t_bar)))*ε_θ(M_t,x,t)) + σ_tz,
where z ~ N(0, I) when t > 1, and z =0 when t - 1.
마침내, 멜스펙트로그램 M은 x에 대응되게 생성될수있다.

-> DiffSinger의 Naive version에서, 입출력 형식이 좀 특이하기야 하겠다만
일반적인 DDPM 형식을 따라가는듯하다

3.2 얕은 확산 메커니즘
간단한 오차(함수)로 학습된 이전의 음향 모델이 추적불가능한 단점을 가지고 있더라도,
여전히 ground-truth data 분포와 강력하게 연결된 샘플을 보여주며,
이는 DiffSinger에 풍부한 사전 지식을 제공할 수 있습니다.
-> 미주의 내용은 다음과 같다
The samples fail to maintain the variable aperiodic parameters,
but they usually have a clear “skeleton” (harmonics) matching the
ground truth
샘플들이 음악의 기본 구조인 스켈레톤(뼈대?)는 잘 맞추지만, 세부적인 음색이나 리듬에 관련된
변수 비주기적 파라미터는 제대로 유지하지 못한다는 것을 의미
-
이 연결을 탐구하고 사전 지식의 더 나은 사용방법을 찾아내기 위하여,
확산 과정을 활용하여 경험적인 관찰을 수행합니다.
-> 실험을 해봤다는 소리겠지?
(그림 3에서 보인것과 같이)
1) t가 0일 때, M은 인접화음(neighboring harmonics)간 풍부한(rich) 세부사항을 가지고 있으며,
이는 합성된 노래 목소리에 자연스러움에 영향을 끼칠 수 있습니다.
그러나 ~M은 1장에서 소개한 것과 같이 over-smoothing 되어있습니다.
※ Gemini 피셜, over-smoothing은 데이터나 신호를 너무 부드럽게 만들어 원본 데이터의 중요한
특징을 잃어버리는 것을 말한다. 소리에는 복잡한 주파수 성분 손실 등이 아닐까
2) t가 증가함에 따라, 두 개의 과정은(아마 M과 ~M을 말하는 모양) 구분이 불가능해집니다.
우리는 이 관찰을 그림 4에다 그렸으며, 이는 ~M 다양체(manifold)에서 가우스 잡음 다양체까지의 궤적은
M 다양체에서 가우스 잡음 다양체까지의 궤적과 (확산 단계 t가 충분히 클 때) 교차한다.
-> 저기서 말하는 manifold가 뭘까??? 데이터 매니폴드? 고차원의 데이터가 낮은 차원에 밀집된 데이터
매니폴드를 의미한다면, latent space와 유사한 개념이 아닐까

이 관찰에서 영감을 받아 얕은 확산 기작을 제시합니다.(가우스 백색 잡음에서 시작하는거 대신에요)
역방향 확산 과정은 그림 4에서 두 궤적이 교차하는 지점에서 시작할겁니다.
그러므로 역방향 과정의 부담은 명백히 경감될 것입니다.
특히, 추론 과정에서 우리는
1) 보조 디코더를 사용하여 ~M를 생성합니다. 이는 악보 인코더의 outputs을 L1(손실)을 사용하여 훈련됩니다.
2) 중간 샘플을 얕은 단계 k에서 확산 과정을 통해 생성합니다. 이는 그림 2b에서 점선 상자안에 나타나있으며,
등식 1번을 따릅니다.
~M_k(~M, ε) = sqrt(α_k_bar)~M + sqrt(1-α_k_bar)ε,
where ε~N(0,I), α_k_bar := Π_product(s=1, k, α_s), α_k := 1-β_k
※ 정방향 과정의 재매개변수화된 식이다.
만약 교차점 경계 k가 적절히 선택된다면, ~M_k와 M_k는 같은 분포에서 온 것으로 볼 수 있다.
3) 역방향 과정을 ~M_k에서 시작하며, 잡음 제거 과정을 k회 반복하여 완수한다.
얕은 확산 메커니즘을 사용한 학습과 추론의 절차는 Algorithm 1과 2에 각각 설명되어있다.
-> 그래서 k는 어떻게 찾는데 그래??? -> 3.3절의 그것인가???
두 궤적의 교차점에 대한 이론적 증명은 보충에 쓰여있다.
-> 다음에 봅시다.

3.3 경계 예측
우리는 그림 4에서 교차점을 찾고 k를 적응적으로 결정하기위해 경계예측기(BP)를 제안합니다.
구체적으로, 경계 예측기는 분류기/(멜-스펙트로그램에 등식 1과 같이 잡음을 추가하는) 모듈로 이루어져있습니다.
주어진 t ∈ [0, T]에 대하여, M_t를 1로 ~M_t를 0으로 표시하고, 경계예측기를
크로스 엔트로피 오차를 사용해 t단계 멜-스펙토그램이 M인지, ~M인지 판단하도록 훈련시킵니다.
오차 L_BP를 학습은 다음과 같이 쓰일 수 있습니다.
L_BP = - E_M,t[log BP(M_t, t) + log(1-BP(~M_t, t))], ※ 내부 식은 크로스 엔트로피고, M과 t에 대한 기대값인 모양이다.
where Y is training set of mel-spectrograms, M ∈ Y, t ∈ [0, T]
경계 예측기가 학습된 다음에는 k를 예측하여 결정할 수 있으며, 이는 샘플이 1로 분류될 확률을 의미한다.
-> 1로 분류된다는것은 ~M이 아닌 M 즉 원본에 대한 멜스펙트로그램으로 보이도록이라는 뜻인 듯하다.
요약: 경계 예측기는 진짜 가짜 멜스펙트로그램 섞어서 크로스엔트로피로 학습합니다.

모든 M ∈ Y에 대하여, 우리는 가장 빠른 단계 k'에 대해
[k', T]에 속한 단계 t의 95%는 다음을 만족하도록 찾습니다:
BP(M_t, t)와 BP(~M_t, t)의 차이가 임계값 미만입니다.
그러면 우리는 평균 k'를 교차 경계 k로 사용할 수 있습니다.
->위 4줄은 대체 뭔소리야
->-> t ∈ [k', T]인 t에서
M_t와 ~M_t의 경계값의 차이(margin)를 확인하고,
이값이 구간의 95%에서 임계값을 넘지 않도록 k'를 설정한다. ※ 값을 늘려가는거니 k'가 어느 시점에서 그만늘어나게 될것이다.
k'의 평균을 k로 선택한다는건 (k' + T) / 2를 한다는건지, 여러번 시도해서 평균을 낸다는건지는 잘 모르겠다. ※ 코드가 알려주겠지
요약: 학습된 경계 예측기로 M과 ~M의 차이가 적은 구간까지 k를 옮기면 적절한 k'를 찾을 수 있습니다.

경계 예측에서 KL발산을 비교하는 것으로 보충하는 더 쉬운 트릭도 있습니다.
경계 예측이 (전체 데이터셋에 대한) 하이퍼파라미터 k를 고르는 (데이터셋) 전처리 과정이라 고려하면,
사실 k는 검증 데이터셋을 브루트 포싱하여 찾아낼 수도 있습니다.
요약: 사실 k' 찾기는 브루트 포스도 됩니다.

3.4 모델구조
인코더의 경우
인코드는 악보를 condition sequence로 바꾼다. ※
이는 다음과 같은 요소로 이루어져있다.
1) 음소 id를 임베딩 시퀀스로 매핑하는 가사 인코더와
이 시퀀스를 언어학적 숨겨진 시퀀스(원문은 linguistic hidden sequence)로 변환하는 일련의 트랜스포머 블록들
2) 언어학적 숨겨진 시퀀스를 기간에 따른 멜-스펙트로그램의 길이 정보에 맞게 확장하는 길이 조절기
3) pitch ID를 pitch 임베딩으로 변환할 음높이(pitch) 인코더
마지막으로, 인코더는 언어학적 시퀀스와 피치 시퀀스 더하여 컨디셔닝 시퀀스 E_m으로 만들어진다.
※

스텝 임베딩의 경우
확산 단계 t는 잡음제거기 ε_θ에 들어갈 또다른 조건 입력이다. 이는 등식 6에서도 알 수 있다. ※ 등식 6은 DDPM의 잡음버전 오차함수 L이다. L_simple수준으로 단순화 된건 아님
이산 단계 t를 연속적이고 숨겨진 것으로 변경하기위해, 사인파 위치 임베딩(바스와니 논문에 그거!)를 사용한다.
두 개의 선형 계층을 C개의 채널을 가진 step embedding E_t로 만들기 위하여 사용한다.
->

보조 디코더의 경우
간단한 멜-스펙트로그램 디코더를 보조 디코더로 소개하였으며,
이는 순전파 트랜스포머 블럭(FFT)들로 이루어져있고
~M를 최종 결과로 생성한다. ※ 멜-스펙트로그램의 예측 버전이다.
※ 이는 FastSpeech의 멜 스펙트로그램 디코더와 같다.

잡음 제거기의 경우
잡음 제거기 ε_θ는 M_t를 입력으로 받아 잡음 ε를 예측한다.
이는 step-embedding E_t와 음악 조건 E_m으로 조건화된 확산 과정에서 추가된 것이다.
확산 모델은 구조적 제약이 없으므로, 잡음 제거기의 설계는 여러 선택이 있을 수 있다.
-> UNet도 되려나
우리는 비인과적 Wavenet을 우리의 잡음 제거기로 들였습니다.
-> 비인과적???
잡음 제거기는 H_m 채널의 M_t를 C개 채널을 가진 입력 은닉 시퀀스 H로 변환하는 1x1합성곱층과
잔차연결이 있는 N개의 합성곱 블럭으로 구성되어있다.
그리고 각 합성곱 블럭은 다음과 같이 구성되어있다.
1) E_t를 H에 element-wise하게 더하는 연산
2) H를 C에서 2C 채널로 변환하는 비절차적 합성곱 신경망
3) E_m을 2C 채널로 변환하는 1x1합성곱 신경망
4) 입력과 조건들의 정보를 merge하는 gate unit
5) merged hidden을 각각 C채널을 가진 2개의 branch로 나누는 residual block
하나의 branch는 다음 H로 사용, 다른 하나는 최종 결과로 사용되는 "skip hidden"
이는 잡음 제거기로 하여금 여러 계층적 단계의 특성들을 예측하여 최종 예측에 사용하는것을 가능케한다.
※ ??? 좀 알아봐야할듯

경계 예측기의 경우
경계 예측기 안의 분류기(classifier) 다음과 같은 구성요소가 있다.
1) E_t를 제공하는 step embedding
2) ResNet (합성곱층과 선형층이 쌓인)
이는 t번째 단계 멜-스펙트로그램과 E_t를 받아 M_t와 ~M_t 두 갈래로 분류한다.

보충 정보에 모델 구조와 설정의 상세가 있다고 한다.



---------- 실험 ----------

실험 setup(퍼옴)

- 데이터셋: PopCS -
1) 중국어 팜송 데이터셋, 노래 117개, 총 5.89시간
2) 모든 노래는 24kHz, 16bit로 샘플링됨
3) 정확한 악보를 얻기 위해 각각의 전체 노래를 DeepSinger로 문장 조각으로 나누고
노래 조각과 대응되는 가사 사이의 음소 레벨의 alignment를 얻기 위하여
Montreal Forced Aligner tool (MFA) 모델을 학습시킴
4) Parselmouth로 waveform에서 F_0 (fundamental frequency)를 추출하여 pitch 정보로 사용
5) 대부분 길이가 10~13초인 노래 조각 1651개로 만들어 사용

- Implementation Details -
1) 중국어 가사를 음소로 변환하기 위해 pypinyin 사용
2) Mel-spectrogram: hop size 128, frame size 512, sample rate 24kHz, mel bin H_m은 80
3) Mel-spectrogram은 [-1, 1]로 선형적으로 스케일링됨. F0는 평균이 0이고 분산이 1이 되도록 정규화
4) 가사 인코더: 임베딩 차원은 256, Transformer block은 FastSpeech2와 동일한 세팅
5) Pitch 인코더: look up table 크기는 300, pitch 임베딩은 256, C=256
6) Denoiser: conv layer 20개 (kernel size 3, dilation 1), T = 100, β는 beta_1 = 10^-4에서 β_T로 선형으로 증가
7) Auxiliary decoder: FastSpeech2의 mel-spectrogram decoder와 동일한 세팅
8) Boundary predictor: conv layer 5개, threshold 0.4

- 학습: 2단계 -
1) Warmup stage: 악보 인코더와 함께 auxiliary decoder를 16만 step동안 학습시킨 후,
auxiliary decoder를 사용하여 boundary predictor를 3만 step동안 학습시켜 k를 얻음
2) Main stage: DiffSinger을 Algorithm 1로 수렴할 때까지 학습시킴 (16만 step)

- 추론 -
사전 학습된 Parallel WaveGAN (PWG)을 vocoder로 사용하여 생성된 mel-spectrogram을 waveform으로 변환
