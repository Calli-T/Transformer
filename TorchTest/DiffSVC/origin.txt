DiffSVC
https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/diffsvc/
https://arxiv.org/pdf/2105.13871
논문리뷰와 논문이다

{ 논문리뷰 발췌 내용
SVC란 content와 멜로디를 그대로 두고 노래의 목소리만 다른 가수
최근의 SVC 모델은 content encoder를 학습시켜 source로부터 content feature를 추출시킨다음
conversion model을 학습시켜 content feature를 다른 feature로 변형 시킨다고한다
(이 때 content는 음성이며 content encoder의 결과물은 content vecter이다)
encoder와 conversion model은 연결하여 AE처럼 학습 시키는 경우도 있고, 각각 학습하는 경우도 있다.

각각 학습 시키는 경우 자동 음성 인식 모델 즉 ASR를 content encoder로 사용한다.
ASR 모델은 end-to-end 모델이거나 히든마르코프-심층신경망 하이브리드 모델이라고 한다.

conversion model은 GAN이나 회귀 모델을 사용하는데
GAN의 경우 content feature에서 바로 waveform(파형)을 만들어 내고,
regression model이 사용된 경우
content vector(feature) 추출 -> 멜 스펙토그램등의 주파수 성분의 특성으로 변환 -> 추가로 학습시킨 vocoder로 생성
DiffSVC의 경우 후자의 경우이며 회귀 모델 대신 Diffusion model을 conversion model에 사용한다
}

멜스펙토그램이란?
https://judy-son.tistory.com/6
결론적으로 시간에 따른 주파수 성분의 변화를 잘 보여주는 방법이다.
x축이 시간, y축이 주파수(증가폭이 로그 스케일임), 그래프의 색은 소리의 크기이다
mel(f) = 2595 log2(1+f/700)

일반적인 SVC의 순서는(gemini 피셜)
멜스펙토그램 - 인코딩 - 변환 모델 - 디코딩
DiffSVC의 경우가 아니다!!!!

ASR 모델의 출력은 phonetic posteriorgram이며,
Diffusion model은 content, melody, loudness의 feature를 조건으로 받아
가우스 잡음으로부터 멜 스펙토그램을 점진적으로 복구한다고한다
(복구하는게 멜 스펙토그램이라면, 이걸 다시 디코딩을 해야겠네?)

※ 정황상 DiffSVC는 DDPM기반 모델이며, 잡음을 복구할 때 추가 조건(음성을 포함한)을 달아주는 식으로 변형한 모델인듯하다
※ UNet대신 WaveNet으로 잡음을 예측한다
모델 구조 설명 구간
{
    {   이 셋은 elementwise하게 더해져 conditioner e가 되며, 이는 diffusion decoder의 추가 토큰이 된다.
        입력으로 PPG, Log_F0, and Loudness

        원본 데이터를 ASR에 넣으면 PPG가 되며, ※ PPG는 음성 후방 확률 그림으로, 시간 경과에 따른 음소 별 확률을 나타낸다.
        ※ 원본 모델은 중국에서 만들어졌으니 중국어 SVC를 위한 음소(phonemes)를 ground truth table로 사용했다고한다.
        이를 FC 레이어로 이루어진 PPG Prenet에 입력한다.

        melody는 기본 주파수 값에 로그를 씌운 contour(Log-F0 = Log Fundamental Frequency)로 표현된다.
        ※ 기본 주파수란 여러 주파수가 섞인 음에서 주기가 가장 긴(= 주파수가 가장 낮은)음이다. 이를 기본으로 음의 높낮이를 계산한다.
        ※ 음색은 기본 주파수에서 그 사이 겹친 다양한 파형의 조합에 따라 파동의 모양이 달라져서 다른 것이다.
        ※ 옥타브는 로그 스케일의 밑을 2로 사용한다. 즉 한 옥타브가 올라갈 때마다 같은 음에서 주파수가 2배로 늘어난다.

        loudness는 A-weighting 방식을 사용한다.
        이는 인간의 인지를 반영해 (정확히는 40폰 등첨감곡선, 같은 크기로 느껴지는 소리를 주파수 별로 나타낸 곡선 기반) 데시벨에 보정을 한 값이다.
        기본 데시벨 값에 더해주면 되며, 그 값은
        A_weighting(f) = 20 log10(1 + (12994/f)^4) + 20 log10(1 + (6300/f)^2) - 20 log10(1 + (f/20)) - 20 log10(1 + (f/25))이다.

        이후 melody와 Loudness는 각각 f_0와 l로 표현되며, 둘 다 임베딩이 되고 처리 방식이 같다.
        둘 다 256 bin(원문도 그렇지만, bit가 아니다!)으로 양자화 한 다음 각각의 임베딩용 룩업 테이블을 통과해서 만들어진다.
    }

    { 시간단계 t
        사인파 임베딩과 비슷하다? 사인파 위치 인코딩이라고한다, 128차원이다
        t_emb(t_ = [sin(10^((0x4)/63))t, ... , sin(10^((63x4)/63))t, cos(10^((0x4)/63))t, ... , cos(10^((63x4)/63))t]
        어째 GDL책에서 본 거랑 굉장히 유사한데?

        여튼 128차원 벡터를 (FC레이어 + Swish함수)x2번을 거쳐 출력한다
        (출력물의 형태는 몰?루 멜 스펙토그램을 1x1합성곱에 넣은것과 형태가 동일한 모양이다)
    }

    {   Diffusion decoder
        잡음 예측에 자기회귀적 합성곱 신경망(딜레이드 컨볼루션)인 Wavenet을 쓴다
        https://james-scorebook.tistory.com/entry/WaveNet-A-Generative-Model-for-Raw-Audio 참고

        잡음낀 멜 스펙토그램, timestep t(의 변형), embedding(아까 3개 정보 뭉친거)를 받아 잡음을 예측한다

        음성을 멜 스펙토그램으로 만들 때의 세부 사항은 Experiments에 있다.
        ※ 만들 때는 라이브러리, 다시 음성으로 디코딩할 때는 Hifi-GAN!!!
        ※ prophesier 기준 librosa라이브러리를 사용한다.
    }
}
※ 이 모델이 다 끝나고, 결과물은 잡음이 없겠지만 여전히 멜 스펙토그램이므로
이를 waveform으로 변환시키려면 모델이 필요하고, 이를 위한 vocoder로 Hifi-GAN을 쓴다고한다.

------------------------------------------------------------------------------------------------------------------------------------------------------

Diff-SVS
대체 왜 그런지 모르겠으나 디퓨전 모델을 쓰는 SVC 모델들이 죄다 Diff-SVS 기반이다...???
SVC의 원작자의 논문에는 샘플만 있고 깃허브가 없다
SVS의 깃허브와 논문 주소는 다음과 같다

DiffSinger
https://github.com/MoonInTheRiver/DiffSinger?tab=readme-ov-file
깃허브와
https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/diffsinger/
논문리뷰

{   SVS 구현체 모음
    한글 Diff-SVS 구현체는 여기에 있다
    https://github.com/wlsdml1114/diff-svc

    자주 사용하는 프로페시아의 모델
    https://github.com/prophesier/diff-svc

    SVS의 근원 깃허브
    https://github.com/MoonInTheRiver/DiffSinger

    openvpi 버전
    https://github.com/openvpi/DiffSinger
}

논문 리뷰 & 번역

---------- 초록 ----------
{   이미 알거나 딱히 안중요한 내용인듯
    SVS는 악보를 입력으로 받아 고품질이고 표현력 있는 노래 목소리를 합성하는 모델이다.
    기존의 SVS 모델은 간단한 손실함수 L1, L2나 GAN을 사용해 음향 특징(멜 스펙트로그램 등)을 재구성하나
    과도한 smoothing과 불안정한 학습 문제로 자연스럽지 못하다.

    DiffSinger는 DPM을 기반으로 하는 노래 합성 모델이다.
    DiffSinger는 악보를 조건으로 잡음을 멕 스펙트로그램으로
    반족 변환하는 매개화된 마르코프 체인이다
    변분 경계를 암시적으로 최적화 함으로 안정적으로 학습할 수 있고
    현실적인 출력을 생성할 수 있다.
}

음질과 추론 속도 향상을 위해
(간단한 손실함수로 학습된 사전 지식을 더 잘 활용할 수 있는) 얕은 확산 기작을 도입합니다.
구체적으로 DiffSinger는 멜 스펙트로그램의 실제 확산경로와
간단한 멜 스펙트로그램 디코더가 예측한 확산 경로의 교차점에 따라
총 확산 단계에서 작은 얕은 단계에서 생성을 시작합니다.
-> 핵심 기작(메커니즘) 중 하나인듯
또한 경계 예측 방법을 제안하여 교차점을 찾고 얕은단계를 적응적으로 결정합니다
-> 위와 관계되는 핵심 기작 중 하나인듯

나머지는 노래 합성 작업 성능이 뛰어나다는 내용과
TTS로의 일반화를 얘기하는듯



---------- 서론 ----------

{ 딱히 안중요
    노래 합성(SVS)**은 음악 악보에서 자연스럽고 표현력 있는 노래 목소리를 합성하는 것을 목표로 하며,
    연구 커뮤니티와 엔터테인먼트 산업에서 점점 더 주목받고 있습니다.
}
SVS의 파이프라인은 일반적으로 악보를 조건으로 주어 음향 특징(멜 스펙트로그램 등)을 생성하는 음향 모델과
음향 특징을 파형으로 변환하는 Vocoder로 구성되어 있습니다.
-> 음성 그 자체에서 멜 스펙트로그램을 생성하는 SVC와는 좀 다른 부분,
멜 스펙트로그램을 Vocoder로 파형으로 변환하는 것은 같다.
나중에 SVC의 것으로 Encoder를 변형하면, SVS를 SVC로 바꾸어 만들 수 있을까?

{ 구현에는 딱히 안중요
    기존의 노래 음향 모델은 L1이나 L2와 같은 간단한 손실 함수를 사용하여 음향 특징을 재구성합니다.
    그러나 이 최적화는 잘못된 단봉 분포(uni-modal distribution)을 기반으로 하여 흐릿하고 과도하게 부드러운 출력을 생성합니다.
    기존 방법이 GAN을 사용하여 이 문제를 해결하려 했으나, 효과적인 GAN을 훈련하는 것이
    불안정한 Discriminator로 인해 때때로 실패할 수 있습니다.
    이런 문제들은 합성된 노래의 자연스러움을 저해합니다.
}

{ 덜 중요한 내용
    Recently부터 fields까지의 내용은 디퓨전 모델 좋아요, 디퓨전 모델은 이렇게 생겼어요에 관한내용,
    DDPM 논문 정독하고 왔으니 필요없다
}

본 연구에서는 확산 모델을 기반으로 하는 음향 모델인 DiffSinger를 제안합니다.
DiffSinger는 음악 악보를 조건으로 노이즈를 멜 스펙트로그램으로 변환합니다.
DiffSinger는 적대적인 피드백 없이 ELBO를 최적화하여 효율적으로 훈련할 수 있으며,
실제 멜 스펙트로그램을 생성하여 지상 진실 분포와 강력하게 일치합니다.
-> 딴건 다 아는 내용이나 ground-truth는 명백한 사실인데, 무엇을 의미하는지를 알아야겠다.
악보를보고 생성한 것(=ground truth)과 실제 음성의 mel spectrogram이 일치률이 높은 모양이다.

(음질을 향상시키고 추론 속도를 높이기 위해
간단한 손실 함수로 학습된 사전 지식을 더 잘 활용할 수 있는)
얕은 확산 메커니즘을 도입합니다
구체적으로, (ground-truth의 mel-spectrogram인) M과
(간단한 mel-spectrogram 디코더가 예측한) ~M의
확산 궤적(trajectories)에서 교차점이 존재함을 발견했습니다.
확산 단계가 충분히 클 때 M과 ~M을 확산 과정으로 보내면
유사하게 뒤틀린(similar distorted) 멜 스펙트로그램을 생성할 수 있습니다.
-> 아마도, 정방향 확산 과정에서 T가 충분히 크다면, 뒤틀린(아마도 잡음을 얘기하는 모양이다)
멜 스펙트로그램이 생성되며 서로 유사한 모양을 가지게 되는 모양이다
(하지만 왜곡된 멜 스펙트로그램이 가우시안 흰색 노이즈가 되는 깊은 단계에 도달하지 않을 때)
-> 확산 과정은 모든 단계에서 정규분포를 따르지 않던가??? 왜 이리 설명한거지
요약: 악보 예측 멜 스펙트로그램과 실제 멜 스펙트로그램은 확산 단계 T가 많을 때 유사합니다.
그러나

그러므로 추론 단계는
1. 간단한 멜 스펙트로그램 디코더로 악보를 받아 멜 스펙트로그램을 생성하고
2. 확산 과정의 얕은 단계 k에서 샘플 ~Mk를 계산하고 ※ 멜 스펙트로그램에 k단계 만큼 잡음을 먹인다는뜻
3. (가우스 백색 소음 말고) ~Mk에서 역방향 확산 과정을 시작하며, 잡음 제거 과정을 k회 반복합니다.
그 밖에 경계 예측 신경망을 이 교차점에 위치시켰으며, k를 적응적으로 결정할 것입니다.
-> 추론 단계에서 사용하는 k값은 '적응적으로' 결정된다는게, 모델의 실행 과정에서 변경되는 값인
것으로 추측한다. 아까 말하는 ground-truth와 유사해지는 단계 t값이 바로 k를 의미하는듯하다.

얕은 확산 기작은 가우스 백색 소음보다 더 나은 시작점을 제공하며,
역방향 확산 과정의 부담을 완화하여 합성된 오디오의 품질을 향상 시키고 추론 속도를 가속합니다.
-> 여기 말하는 가우스 백색 소음이라는게, N(0, I)에서 뽑은 무작위 값인 것으로 추측한다.

마지막으로, SVS의 파이프라인이 TTS와 유사하기 때문에 일반화를 위해
DiffSinger에서 조정한 DiffSpeech도 구축했습니다.
-> 코드에 있나 설마?
아래 내용은 우리 SVS랑 우리 TTS 좋아요 내용인듯

논문의 기여는 DPM 기반 SVS 제안/얕은 확산 기작 제안/TTS 확장 이 3개를 주장한다.