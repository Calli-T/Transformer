fs2로 decoder_inp를 만들어 내기 위해 알아보는 작업들
알아낸 내용들
{ 1. condition을 만들 때 나오는 f0_pred(f0_denorm)이 어디서, 왜 만들어지는가에 관한 내용
    1). use_pe는 audio sample_rate가 24000일 경우에만 true이다
    따라서 44,100 Hz로 진행할 경우
    이는 infer_tool.py의 def infer안에서 use_pe를 확인해봐도 false로 나온다

    2). use_pe가 false일 경우, batch['f0_pred']는 모델의 출력에서 f0_denorm를 가져오게된다
    infer_tool.py, 165 line에 있다

    3). nsf-hifigan은 spec2wav 작업에서 f0와 mel을 모두사용한다
    그리고 그 f0는 f0_pred이다
    이는 infer_tool.py, 199 line에 있다

    4). f0_pred(=f0_denorm)를 만드는 것은 fs2.py line 229이다
    즉, 해당 파일의 코드들은 condition(wavenet의 입력 가운데 하나)을 만들 뿐 아니라
    f0(nsf-hifigan의 입력 중 하나,)의 예측도 같이 수행해낸다

    5). f0_denorm을 만드는 코드가 속한 함수는 add_pitch이며,
    이는 pitch 정보를 embedding하기 위한 함수이고,
    forward에서 use_pitch_embed == True(hparams에 정의되어있다)를
    만족하여 실행되는 동작이다.

    결론: condition을 만드는 과정에서 f0_pred(=f0_denorm)도 만든 다음,
    mel을 wav로 만들 때 mel과 함께 nsf_hifigan의 입력으로 쓰는 것이다.

    추가) f0_gt는 가져온 f0에 후처리를 하는 것인듯하다
    f0를 가져오는 과정 자체는 crepe의 그것이다
    그러나 f0_gt는 어디다 쓰는지 알 수 없으며
    infer_tool.py / plot.py / fs2.py 세 파일에서만 보인다
    결론: f0_gt는 딱히 쓰이는 곳이 없는 것 같다
}

2. 모델에서 실제로 사용되는 레이어
line 53부터 모델의 상태에 관한 내용
class의 FastSpeech2 def __init__()에서 보이는 파라미터들의 내역은 다음과 같고,
config_nsf.yaml에서 발췌 (갱신 필요)
use_uv: false
use_energy_embed: false
use_pitch_embed: true
audio_num_mel_bins: 128
pitch_type: frame
※ else의 코드인 pitch_predictor는 쓰이는 곳이 존재하지 않는다

결론: mel_out의 Linear와 pitch_embed의 Embedding 딱 2개만 사용한다

해야할 일
1. fs의 condition 제작 구현
위 2번에 필요한 레이어가 있으니 모델을 올려두고, 거기서 state_dict에서 가져오도록 해야할듯
모델에 넣기 전에 infer_tool.py에서 어떻게 재료 손질을 하는지 보면 될듯
266 line부터 시작

fs는 일단은!
일단은!
for_model의 방법과 같이 원본 모델에서 필요한 부분만 읽어오는 것으로 하자

2. f0_pred(f0_denorm) 제작 구현
곁다리로 만들지만, 중요한 값이다.

3. 디렉터리 구조 개편?
for_input_process를 for_pre_post_process로 바꾼다.
fs2(의 레이어 몇 개만 따온것)는 cond와 spec2wav에 모두 쓰이며
wav2spec의 함수는 (코드 쓴 것에 따라 다르지만) nsf-hifigan class에 종속되어있다.

혹은 모두 wav2spec은 stand alone하게 분리 가능하므로,
f0_denorm도 (이건 모델의 레이어와는 관계없는 함수이다) 분리시키고,

nsf_hifigan을 for_post_process로 분리, 원본 input_process에서 nsf_hifigan제거
등등 좀 분리를 시키는 작업을 하면된다

4. 병합된 원본 모델을 세부 나누는 코드
그리고 원본과 다르게 모델을 분리해서 사용하므로,
GaussianDiffusion을 fs2와 wavenet으로 분리하여 저장하는 코드를 만들자
또한 wavenet쪽은 이미 원본 모델에서 필요한 부분만 읽어오는 코드로 만들어져있으므로,
분리저장된 모델을 읽는 쪽으로 선회해야한다

5. 디렉터리 구조 개편 & hparams의 path 변경 및 통합
모델을 한 데 몰아서 다 저장하는 것으로 처리,
이에 따른 path 등등 수정,
hparams의 값들 하나의 파일로 병합

의문점
fs2.py의 233 line에서 f0를 f0_to_coarse에 넣는데,
대체 왜 infer_tool.py의 get_pitch_crepe에서도 f0_to_coarse를 사용해서 값을 주는지?
fs2의 forward에는 일반 f0밖에 안받으며, coarse는 위와 같이 forward 함수 '안에서'
처리한다.
-> 나중에 coarse_f0는 시작할 때 받아올 필요는 없는 것으로 처리하자

1번 구현중, 그런데 infer_tool.py의 get_item부터 하고 넣어야 collate가 작동하는
것으로 보인다
추후 구현 ㄱㄱ

----- 1205 -----
몇 가지 정보가 추가로 있는데, 일단 여러 곡이 하나의 batch로 들어가는 구조는 아닌듯하다
음악파일이 생각보다는 큰 모양이다
용량을 추측해 볼 수 있는 코드가 infer_tool.py에 존재한다
각각의 cond 재료는 'max_frames'만큼 절삭해서 들어가며, (값은 42,000)
hubert만 'max_input_tokens'만큼 절삭해서 들어간다. (값은 60,000)
이 숫자들 간의 상관 관계는 나중에 알아보자
hubert는 값 1당 0.02초(20ms)를 의미한다
'max_sentences'는 학습할 때 쓴다
결론: 학습코드쓸 때 저 3개의 hparams 값을 다시 확인해보고, 지금은 일단 코드를 그대로 쓰자

일단 추후 구현 해야할게
1. 학습용으로 음원 자르기
2. 노래가 크면 자르기
3. 디렉토리 내부의 모든 음원에 대해 전부 입력으로 만들기
-> 이건 잘라서 '다른 노래조각과 같이 묶인' batch로 만들 수도 있긴함

일단은, 일단은 1개 음원에 대해서 작업중

-----
f0는 특이한 처리사항이 있음
기본 주파수? 이건 log로 처리한다고 hparams에 적혀있고,
이를 처리하는 전용 코드도 pitch_utils.py에 norm_f0와 norm_interp_f0 함수로 있다
tensor로 다른 cond를 바꿀 때, f0는 log로 바꾸는 작업도 같이 수행할것

일단 cond 원료를 f0보간, tensor화 하는 것 까지 작업했음
collate와 fs2에 넣는것해야하고
!나중에 denorm도 만들어야함

collate작업할 때 맨 앞 텐서의 맨 앞축의 차원이 1인데, 이거 배치가 아니었다
노래는 무조건 1개만 들어가도록 되어있다. 혹은 제작자가 그렇게 바꿔놨거나 ㅋㅋㅋㅋ
혹시 학습 때 여러개를 동시에 넣는 것인가?????