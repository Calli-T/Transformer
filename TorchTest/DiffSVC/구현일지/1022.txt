SVC_task.py의 50줄에
모델선언(class guassianDiffusion)
들어가는게 여럿 있다

{ class GaussianDiffusion, network/diffusion.py

    { # 생성자 매개변수
        ※0 # phone_encoder이 뭔지는 후첨 설명에, 실제 어디서 사용하는지는 '충격적 진실'에 후술

        out_dims는 멜 밴드 수를 말하는것같고,

        denoise_fn은 잘알려진 잡음제거함수, wavenet사용

        timesteps는 확산에 쓰는 시간단계 같고, 기본 1000
        K_step은 뭐지? 이것도 기본 1000

        loss_type은 l2, 이건 뭐 2차원 노름일거고

        spec_min과 spec_max는 뭔지 모르겠다. 숫자가 쭉 나열됨
        스펙트럼의 최소와 최대값으로 추정함
    }

    { # __init__(self, ...)
    ※1 전체적으로 lucidrains의 class GaussianDiffusion과 상당히 유사하다

        FastSpeech phone_encoder와 out_dims를 입력으로 주어 self.fs2로 선언한다.
        ※2 여기선 오로지 FS모델의 입력으로만 phonetic encoder(=HuBERT)를 사용한다.
        ※3 원본 DiffSinger의 FastSpeech2MIDI는 주석처리되어있다. GPT 피셜, 해당 모델은 MIDI정보를 받아 음성을 생성한다고 한다.
        {   # class FastSpeech2, /modules/fastspeech/fs2.py
            ※4 '충격적 진실' 이 프로젝트의 FastSpeech2의 생성자에서는 phonetic encoder를 주석 처리해놨다.
                대신 forward함수에서 매개변수로 받아서 쓴다

            그럼 뭘 받아서, 뭘 되돌려주는지 알아보자
            일단 diffusion.py에서 self.fs2에 관한 코드는 사실상 결과를 반환받는 ret 하나 뿐이다.
            ※5 cwtf0_norm이라는 함수가 있긴하더라. 혹시 나중에 사용할 일 있으면 후술함
            ※6 fs2의 입력에 사용되는 것들은 너무 복잡하니, 뭘 출력하고 이걸 어디다 쓰는지에 집중하고 나중에 알아보자
        }
        fs의 입출력은 아래 함수에서
    }
    { # def forward, diffusion.py
        ret은 fs2에 forward를 한 값이며,
        diffusion.py에 ret과 관련된 코드는 ret['decoder_inp']와 ret['mel_out']이다.
        ※7 아래의 ret['mel_out']은 ret의 정보를 사용하는게 아닌 것으로 보인다. 아마 음소 정보가 존재하는 구간에만 멜 스펙트로그램을 생성하는 장치인듯하다.
        cond = ret['decoder_inp']인 것으로 보아하니 조건이나, 임베딩일지도?
        이는 fs2.py의 FastSpeech2.py의 코드에서 확인할 수 있다.

        {   # def forward, /training/task/fs2.py, class FastSpeech2
            여기서 반환하는 ret은 사실 dict다

            'no_fs2'옵션이 true이며, skip_decoder도 true이다.
            즉 fs2의 대부분(혹은 전부?)를 생략한다. - 적어도
            encoder_out을 후처리하는게 fs2의 forward과정인 모양인데, 그냥 hubert를 쓴다.
            ※8 hubert에 이것저것 임베딩을 더하는걸 보니, 여기서 입력으로 넣은 hubert가 모델 그 자체는 아니고 모델의 출력값인듯 하다.

            use_spk_embed와 use_spk_id가 모두 false이므로, 둘 다 씹고 else로 가며
            spk_embed_dur, spk_embed_f0, spk_embed는 모두 0이다
            -> 무시할거 리스트

            ret['mel2ph']는 forward에 입력한 mel2ph 그대로이다.
            ※9 그냥 가지고 있다가 통합만 시켜주나봄

            decoder_inp는 먼저 hubert벡터에 패딩을 넣어준다.
            decoder_inp = F.pad(encoder_out, [0, 0, 1, 0])
            ※10 F.pad가 어떤식으로 동작하는지는 /DiffSVC/tests/test1.py를 보라
            HuBERT의 출력이 [N, S, D]를 가지는데, 이중 S는 음원을 윈도우-슬라이드로 자른 개수이고,
            이것의 시작쪽에 패딩을 하나 넣어준다.
            ※11 패딩으로 추가되는 값은 기본적으로 0

            133줄 decoder_inp = ... 부터 다시 시작
            여차저차해서 decoder_inp를 condition embedding으로 만드는 과정이 fs2.py의 133 line 아래로 있다.

            135줄
            mel2ph의 모든 차원을 슬라이싱 ... 하고,
            mel2ph의 차원 축을 끝에 하나 추가해주고(mel2ph[..., None]),
            마지막 차원축을 encoder의 채널 수에 맞춰서 repeat한다.
            mel2ph가 [N, S]차원이면 끝에 [N, S, 1]차원으로 바꾸고,
            encoder의 채널 수가 D차원이라면, 결론적으로
            mel2ph는 D회만큼 반복되어 [N, S, D] 차원이 된다.
            요약: [N, S] 차원의 mel2ph를 encoder 출력 차원수 D만큼 반복하여 [N, S, D] 텐서로 만든다. 이름은 mel2ph_
            ※12 gather 연산의 준비 작업이다

            136p의 gather는 좀 복잡하긴 하나, 텐서, 지정 차원축, 거기에 대한 인덱스로 값을 가져와서 텐서의 내부 구성을 변환시키는 텐서이다.
            자세한건 공식 문서 ㄱ
            decoder_inp가 [N, S, D] 차원으로, gather 함수의 차원축이 1차원이므로 S 즉 시간 혹은 시퀀스에 대하여 동작하며
            out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1라는 공식 문서의 가이드에 따라,
            배치와 차원이 같되, 시퀀스의 인덱스가 mel2ph_의 것인 것으로 대체된다.
            N은 배치고, 배치안의 같은 파일 안에서만 움직일테니
            [S+1, D]텐서를 입력으로, [S, D]텐서를 인덱스로 사용한다고 생각해보자
            (GPT 피셜)
            디코더가 멜 스펙트로그램의 타임스텝에 맞는 인코더 출력 정보를 사용할 수 있도록
            정렬된 입력을 생성하는것이 이 코드의 목적이라고 한다.
            ※13 관련정보 즉 wavenet의 구조와 조건을 주는법
            https://joungheekim.github.io/2020/09/17/paper-review/
            (추측)
            S+1로 만드는 이유는 (음소에서) 패딩인 0을 고려하기 위함인가?

            138~146 line
            주파수, 특히 f0를 임베딩으로 추가하는 코드로 추정된다
            use_pitch_embed는 true, use_energy_embed는 false이므로, f0정보를 사용하는것으로 추측된다
            138 line의 tgt_nonpadding은 mel2ph의 음소 정보에서 패딩이 '아닌' 부분을 골라내고
            pitch_inp는 아까 만든 음소 정보 de_in_or와 var_embed와 spk_embed_f0를 더해서 만든다
            그 가운데 var_embed는 그냥 값이 0이며(관련 코드가 주석처리됨),
            spk_embed_f0_id는 forward의 매개변수에 들어있다.
            요약: var는 의미없고 음소정보랑 기본주파수 정보를 더해(element-wise) 임베딩을 만든다
            그리고 tgt_target으로 아까만든곳에서 음소가 패딩 즉 0인걸 거른다
            이후 use_pitch_embed가 true임에따라

            { # 무시되는 값이다
                pitch_inp_ph를 제작하는데,
                이건 encoder_out + spk_embed_0 + src_nonpadding인데,
                src_nonpadding은 hubert의 값에 0이 아닌(패딩이 아닌) 값을 요약한다.
                decoder_inp를 원래 값과(아까 패딩해둔 그거)와 add_pitch함수의 반환값을 더해 만들어준다.
            }

            { # add_pitch, 144줄
                이름만 봐서는 decoder_inp에 피치 값을 더해주는 함수인듯

                매개변수로 decoder_inp, f0, uv, mel2ph, ret을 쓴다
                근데 encoder_out즉 아까 만든 pitch_inp_ph는 쓸모없는 값이다

                decoder_inp는 안쓰는 값이며,

                mel2ph == 0일 때 pitch_padding이 True이다
                ret에 기본주파수 f0를 denorm_f0함수로 역정규화 해주고 dict에 추가한다.
                (if 문으로 패딩 처리도 해준다)

                pitch는 f0_to_coarse함수에 f0_denorm, hparams를 넣어주어 처리하며
                이를 unsqeeze하여 ret의 pitch_pred에 넣어준다.
                ※14 f0_to_coarse는 기본 주파수 f0를 조잡한 형태로 변환하는 형태의 함수라고 한다.

                pitch_embedding은 self.pitch_embed에 pitch를 넣어 임베딩으로 만들고, 이를 반환한다
                ※15 pitch_embed 함수는 __init__에서 선언되며, 이는 pitch를 가지고 룩업테이블에 넣어 임베딩으로 변환한다
                Embedding은 파이토치의 기본 기능중 하나이다.
            }

            ret['decoder_inp']는 기존 decoder_inp와 spk_embed를 더한 다음 tgt_nonpadding을 곱한 값이며(즉, 패딩을 거른 값이며)
            no_fs2는 true이므로 아래의 if문은 무시

            최종적으로 ret에 값들을 반환한다
            이는 156 line이며,
            다시 diffusion의 forward 함수로 돌아와 어디에서 뭐가 쓰이는지 확인해보자
        }

        받아온 ret의 decoder_inp의 형태 [N, S, D]를 [N, D, S]로 치환한 것이 cond이다
        hubert.shape의 첫 값으로 배치 크기 N을 뽑아오고, 가속 장치 정보를 뽑아온다

        추론이 아닐 때(= 학습중일 때) Batch2Loss.module4에 넣어 학습하는듯하다.
        클래스명을 보니 Batch째로 넣어 Loss를 가져오는듯하다.
        매개변수로는
        [   self.p_losses (loss 뽑아주는 함수),
            self.norm_spec 함수의 반환값(ref_mels를 매개로 받아 self.spec_min값을 사용해 처리하는듯),
            cond, ret, self.K_step, b, device]를 사용한다.]

        {   # class Batch2Loss.module4
            해당 클래스는 여러 학습 모듈을 static 함수 형태로 모아둔 class다
            def module4는 ret에 'diff_loss'항목을 추가해주는 함수이며, 주석에는 이렇게 적혀있다
            spec을 입력으로, decoder_inp를 조건부로 사용하여 확산(모델)을 학습한다.
            norm_spec은 normalized된 spec이라되어있다.
            spec, 그러니까 specification이 정확하게 뭔지는 모르겠다???

            timestep t는 0~K_step사이 무작위 값을 batch_size만큼 뽑아주고,
            norm_spec은 전치, 차원축 추가를 통해 [N, C, H, W]식의 텐서 모양을 잡아주는 모양이다.
            주석에는 [B, 1, M, T]라 되어있다.
            추측) [N, S, D]를 [N, 1, D, S] 즉 배치/채널/차원/시퀀스로 바꾼게 아닌가싶다.

            마지막 줄에는 ret에 'diff_loss'를 추가해주는데, 가져온 오차만드는 함수를 사용한다.
            매개변수로 norm_spec, t, decoder_inp_t를 사용한다

            {   # def p_losses, diffuion.py 207 line
                ※16 norm_spec의 정체는 바로 x_0 값! q_sample에서 후술
                t와 cond는 매개로 받아 왔고,

                잡음은 무작위로 생성해주고,
                그걸로 x_T를 q_sample함수로 뽑아줍니다.
                ※17 재매개변수화 트릭 이미 적용되어있고, 이를 쉽게하기 위한 배열?텐서?도 이미 __init__에서 처리해둔듯
                denoise_fn을 사용한 x_recon을 만들어준다
                ※18 reprise) denoise_fn은 여기선 wavenet

                그 외에는 loss_type에 따라 L1식 오차냐, L2식 오차냐에 따라르며
                x_recon과 noise를 비교하여 오차를 뽑아낸다
                ※19 nonpadding은 옵션인듯
            }
        }

        한편, 학습이 아닌 추론의 경우 255 line부터 보면된다
        use_gt_mel는 아무래도 ground truth mel spectrogram인듯하고, 이걸 infer.py의 기본옵션은 false이니 무시하고

        264 line부터 시작
        time step을 K_step으로 잡고
        shape를 [N, C(1), D(멜 밴드 대역수), S(cond.shape[2]이니 아마도 시퀀스길이???)]로 잡고
        무작위 잡음을 shape와 device에 맞춰 생성한 다음
        pndm가속 여부에 따라 p_sample_plms혹은 p_sample함수를 사용하여 생성한다.

        어느 쪽이든 생성 다하면 다시 축을 전치하고, 역정상화(denorm)하여 음악으로 만들어낸다음
        이를 ret에 실어 보내주면 끝이다.
    }
}

{ #phone_encoder가 뭔가?
    이는 TtsTask에서 발견할 수 있으며, 결론적으로는 그냥 HuBERT
    그럼 HuBERT는 뭔가? 자기 지도 학습 모델인 BERT를 음성에 확장한 것이다.
    https://velog.io/@9e0na/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Speech-HuBERT-Self-Supervised-Speech-RepresentationLearning-by-Masked-Prediction-of-Hidden-Units-2021-Summary
    HuBERT는 BERT에서 왔는데,
    이는 트랜스포머 인코더로 되어 있어 시퀀스 각 단어의 문맥을 고려한 벡터표현을 생성한다.
    같은 의미에서 단어를 음성으로 바꾸면, 음성 시퀀스를 요약한 벡터 표현을 생성할 수 있다.
    HuBERT의 핵심 아이디어는 K-means clustering을 사용해 음성 단위(음소)를 스스로 분리해내는 것이다.
    요약: 해당 프로젝트의 음소정보 인코더는 HuBERT를 사용하며, 이는 음소 정보를 자기지도 학습으로 분리해낸다.

    실습 내용 추가+) 이를 실습해본 코드는 TorchTest/DiffSVC/KOHubert/test1.py에 존재한다.
    윈도우와 슬라이드가 존재하는 형태로, 음성 정보를 여럿으로 쪼개어
    음소에 해당하는 벡터?를 만들어내는듯
    [N, D, S]가 아니고 [N, S, D] 차원축을 사용하는듯하다.
    벡터의 차원은 768이며 초당 16k번 샘플링되었는데, 잘 찾아보면 다른 모델도 있을듯
    그리고 해당 프로젝트에서 사용한 세부 사항과 다를 수 있다.
}

{ fs2의 입력은 어디서 오는가? 에관한 고찰(진행중)
ret을 받기위해 fs2에 입력으로 들어가는건 hubert, mel2ph...등등 여러가지이나,
            diffusion.py에서는 이 'hubert'등 여러 매개변수가 어디에서 오는지 알려져있지 않다.
            diffusion.py의 def forward는 SVC_task.py의 run_model에서 활용됨이 나타나있다.
            ※5 당연한 얘기이지만, forward함수는 .forward의 형식이 아닌 인스턴스에 바로 괄호열고 사용하는 형태이다.
            ※6 def run_model(self, ...):, training/task/SVC_task.py 에서 사용하는데,
            어디서 오는건지는 나중에서 알아보고 ret의 출력에만 집중하자
}