이제 입력을 만들어내는것은 전부 다했으니
큰 틀에서 순서는
1. forward test
2. infer test
3. training test이다

그전에 간단히 모델 구분하고, 디렉터리 분리하는 작업 수행중
utils/divide_model.py에 모델을 가르는 작업 수행중이다

※ (gpt 피셜) model1의 출력을 model2의 입력으로 쓸 때,
양쪽의 패머리터가 모두 optimizer에 등록되어있다면
back propagation 과정에서 torch가 자동으로
model1과 model2를 동시에 학습시킨다고 한다.

wavenet(혹은 denoise_fn)의 입력은 다음과 같다
torch.Size([2, 1, 128, 300])
torch.Size([2])
torch.Size([2, 256, 300])
torch.Size([2, 1, 128, 300])
거기에 단 주석은 다음과 같다
'''
:param spec: [B, 1, M, T]
:param diffusion_step: [B, 1] -> [B, ] 실제로는 1축 텐서이니 앞의 ,1은 무시할것
:param cond: [B, M, T] -> 이거 M이 mel-band 수가 아니고, hidden 값과 관련있는 것 같다
'''

한편, embedding model의 출력은 다음과 같다
torch.Size([1, 518, 256])
torch.Size([1, 518])
위는 cond, 아래는 f0_denorm일 것이다
아마 cond는 [1, T, M]일 것이다
{   VRAM / batch관련 잡소리
    Batch의 크기는 1로 사실상 고정이며??? 10~15초의 음성일 것이다
    ! 나중에 VRAM의 크기를 보고 BATCH의 크기를 자동으로 정하는 코드를 짜자
    24,000 sample_rate에 6sec raw wave 기준 cond모델은 VRAM 12GiB에서 31%를 잡아먹더라
    ! 나중에 모델별로 배치를 다르게 하는 작업도 가능할듯? 내부에 모델이 7개나 있다
    ! VRAM의 최대 사용량을 정하는 옵션을 만들 수도 있을 듯 계산만 잘하면
}
cond는 입력하기전 코드에서 transpose(1, 2)처리하여 하는데, [1, T, M]을 [1, M, T]
1은 배치 크기이니, 원본과 맞다!!!
spec을 맞춰줘야 하는데,이건 mel이 어떻게 나오나 코드를 볼 필요가 있다
test1_~~~.py를 보니 출력이 (518, 128)로 나오며 이는 [T, M]이다

코드에서 diffusion.py의 264 line 부터 보면
(use_gt_mel은 false이다, 원본 음원을 노이즈로 사용하지 '않는다')
원본 음원의 cond를 액기스만 뽑아서 사용하며, mel은 폐기되고
무작위 노이즈에서 시작한다 아 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ
어느 개인의 음성 특징은 '모델에' 저장되는 것이었다
생각해보면 그게 맞긴 하겠지

그리고 __init__.py를 제외하면
{
    for_input_preprocess에서 실제로 사용되는 파일은 cond_integrate.py밖에 없다
    나중에 EmbeddingModel에 model load를 파일을 쪼개서 하나의 파일 크기를
    줄일 수도 있기는? 하다
    temp_hparams에서 대부분을 의존하지만, cond_integrate 파일 만큼은 거기 적힌
    path용 함수 2종을 사용할 필요가 없다.
    대신, 모든 경로는 절대 경로로 집어 넣어줘야한다
    원래 temp_hparams에 있는 절대 경로 제작 함수가 상위 디렉토리에는 없으므로,
    새로 제작해서 hparams 파일과 같이 넣어주던지, utils 디렉토리를 만들어 새로 빼줘야할
    지는 조금 고민해보자
}
결론: cond_integrate는 hparams와 raw파일의 절대경로를 외부 입력으로 처리해도된다
단, 모델들은 해당 디렉터리의 하위 디렉터리 내부에 있으므로, 최상위 디렉터리에
models로 빼고 싶다면 hparams를 새심하게 닦는 수밖에 없다
일단은, 일단은 각 모델이름이 디렉터리이 이름인 디렉터리에 모델의 저장파일을 둔다.

!그런데 학습되는 것은 Embedding / denoise 모델 2개이므로, 최소한 얘네 둘은
최상위 경로로 따로 뺄 필요가 있다
최상위 경로의 디렉터리를 models로하고, 학습하는/안하는 모델의 디렉터리를 따로 만들어서
안하는 모델의 경로를 해당 디렉터리에 몰아도 딱히 상관은 없겠더라
그리고 test[번호]_어쩌고.py는 나중에 싹 날려도 괜찮다

Wavenet_mel에는 모델과 cond모델의 관한 모든 테스트가 있기 때문에
최상위 디렉터리를 아예 새로 파는 것도 괜찮겠다
결론: 힘들게 리팩토링 하지말고 테스트 디렉터리 버리고 새 디렉토리로 가자
어차피 다른 디렉터리의 내용, 주로 DDPM 같은것과 합쳐야한다
대신, 디렉터리에 하위 디렉터리 구조를 잘 생각해야한다
생각해둔 구조는 다음과 같다 (대괄호는 dir)
이 디렉터리 구조를 계속해서보강해나가자
DiffSVC

-[conds_before_embedding]
--[하위모델1코드만]
--[하위모델2코드만]
--[하위모델3코드만]
--[하위모델4코드만]
--[하위모델5코드만]
--conds2tensor.py ※ model_load와 사용은 다른곳으로 코드 옮김. 즉, 텐서화만
-[diffusion]
--[wavenet코드만]
---net.py
---load.pt ※ temp_load_ckpt.py는 무시, test1.py 의 코드를 개조 -> 걍 torch 기능으로 대체한 다음 삭제
---act_func.py ※ 나중에 net.py에 통합하거나, torch 기본 기능으로 전환후 삭제
--[EmbeddingModel코드만]
---cond_tensor2embedding.py
--DDPM.py ※ GaussainDiffusion이나, 기타 다른 샘플링 기법 자체를 이름으로가능
-[raw]
--music1.wav
--music2.wav
--music3.wav
-[output]
--music1_[project_name]_steps_[steps].wav ※ flac인지 wav인지는 코드좀
--music2_[project_name]_steps_[steps].wav ※ 보고 처리하자
--music3_[project_name]_steps_[steps].wav ※ 기타 양식도 마찬가지
-[models]
--wavenet_model_steps_[steps].pt
--embedding_model_steps_[steps].pt
--hubert_soft.pt
--nsf_hifigan.pt
--xiaoma_pe_[steps].pt
-hparams.py
-arg2hparams.py
-infer.py
-training.py