모델의 선언에서 들어가는 매개변수들은 생성 코드들에서 기본값들로 채워놓고, 필요한 값은 덮어 씌우자,

모델에 forward에 들어가는 값은 하나는 x_t이고 다른 하나는 배치개수 * t인 텐서이다
tensor([937.7500, 937.7500, 937.7500, 937.7500, 937.7500, 937.7500, 937.7500,
        937.7500, 937.7500, 937.7500, 937.7500, 937.7500, 937.7500, 937.7500,
        937.7500, 937.7500], device='cuda:0')
tensor([937.5000, 937.5000, 937.5000, 937.5000, 937.5000, 937.5000, 937.5000,
        937.5000, 937.5000, 937.5000, 937.5000, 937.5000, 937.5000, 937.5000,
        937.5000, 937.5000], device='cuda:0')
tensor([937.2500, 937.2500, 937.2500, 937.2500, 937.2500, 937.2500, 937.2500,
        937.2500, 937.2500, 937.2500, 937.2500, 937.2500, 937.2500, 937.2500,
        937.2500, 937.2500], device='cuda:0')

※ _scale_timesteps이란 함수가 있어서 아래 형태의 t를 위 형태로 바꿔버린다
    def _scale_timesteps(self, t):
        if self.rescale_timesteps:
            return t.float() * (1000.0 / self.num_timesteps)
        return t
무슨 num_timesteps를 쓰던 간에 [0, 1000] 사이로 스케일링한다

tensor([3773, 3773, 3773, 3773, 3773, 3773, 3773, 3773, 3773, 3773, 3773, 3773,
        3773, 3773, 3773, 3773], device='cuda:0')

emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))

time_embed_dim = model_channels * 4
        self.time_embed = nn.Sequential(
            linear(model_channels, time_embed_dim),
            SiLU(),
            linear(time_embed_dim, time_embed_dim),
        )

정현파 임베딩과는 다른 방식으로 시퀀스 t를 전달한다고 한다

정현파 임베딩은 간단하고 효과적이며 학습시간이 단축되지만, 표현능력이 제한되고 데이터에 특화되지 않았다
학습 기반 임베딩은 데이터에 특화된 표현이 가능하며 유연성이 높고, 학습시간이 소요되며 과적합 가능성이 있다고 한다.

정현파 임베딩은 Transformer모델에서 주로 사용한다고들 한다
학습 기반 임베딩은 다양한 신경망에서 사용된다고 한다

할일 1. 모델에 맞춰 UNetModel선언하도록 매개변수 조절
-> Unconditional CIFAR-10 with the L_vlb objective and cosine noise schedule에 맞춤

할일 2. timesteps를 scaling하여 보내주도록 코드 수정
할일 3. model forward가 동작하는지 확인
할일 4. 생성한 값들 픽셀 [0, 1] / [0, 255] scaling 정보 확인