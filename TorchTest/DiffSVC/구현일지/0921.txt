칸-바야시의 구현은 다 좋은데 import해도 코드로 쓰는게 아니고, 쉘 스크립트를 조작하여 사용하는 것이다
이를 위해 명령어를 파싱하는 구조까지 존재한다.
제법 복잡하고 여러 상황에 대응하기 때문에, 다른 방법을 찾아보거나 코드를 자세히 보자

라이브러리의 코드를 뜯어보면 parse 아래에 사용법이 있으니,
거기에는 모델, 런치, 디코드, 파싱 등등의 코드가 있다.
이부분만 적절히 뜯어 가져가면 사용에 문제는 없을것이다.

가지치기가 끝난 코드에 대해서는
실행 부분에서 어디가 뭐하는 코드인지는 잠깐 보도록하자
목표는 wav->mel->wav이며, wav는 뭘로 mel로 바꾸는지도 알아봐야한다

할일
1) Analysis-synthesis 예제 깃허브에 나와있는대로 사용
2) 코드 가지치기해서 디렉터리에 담고, 모델과 같이 두기
3) 그 과정에서 멜 스펙트로그램 변환 방법과 형식 찾기 밑 호환성 분석
4) 추후 pwg 학습을 위해 코드의 실행 흐름을 따라 간단히 주석
5) 쉘 스크립트를 인식하는 코드를, 함수화 하여 원하는 대로 '파이썬코드 상에서' 사용가능하도록 개조


{
1)의 과정은
    음원파일 가져오기 ->
    wav로 변형(sample rate를 일단 맞추긴했으나 필수인지는 모르겠음, 모노채널로 변경 필수) ->
    모델 가져오기 -> process->normalize->decode
    하는 방법은 ana-syn1.txt에, 파형보기는 show_wave2.py에 적어놓음

    파형 시각화 방법 참고자료
    https://dacon.io/competitions/official/235616/codeshare/1277
    그 외 음원 가져오는건 Toyproject_AI어쩌고 북마크 디렉터리에 넣어둠

    사용한 모델은
    vctk_parallel_wavegan.v1
    kss_parallel_wavegan.v1

    kss구린데 문장 수가 달라서 그런듯
    VCTK는 44000문장, kss어쩌고는 770문장이고 둘다 학습은 400,000번함
    VCTK 롱버전은 1,000,000번 학습 했는데 좀 낫더라
}

{
2)의 과정에서
일단 멜스펙트로그램을 뽑아낸다음 거기서 소리를 만들어내는것은 맞는것같다. 코드를 더 자세히 살펴볼 필요가 있다.
특히 f0는 대체 뭐에다 쓰는지원
PCM은 고정 16 bit인듯 하다?
채널은 확실히 모노 채널만 학습가능하다, 다중 채널 지원 모델은 추후 작업으로 해보자

패키지의 bin 디렉터리의 코드에서 decode, normalize, preprocess만 딱 쓰려고 가져왔고,


아직 안한것들, 해야한다.
하위 패키지로 있는 코드들이 있기때문에 코드 가져오고, import 정리 해줘야한다
그리고 argparser로 파싱한 것들 대시('-')가 언더스코어('_')로 바뀐채로 들어가있으니 조심
필요 없는 코드는 모조리 날리자
그리고 멜 스펙트로그램를 어디에 넣어 어떻게 만드는지 좀 확인해두자, 이건 추후 ~M과 M을 구분하는 분류기를 만드는데 필요하다.
(잊지말것은, 이것은 단지 Vocoder를 만드는 과정일 뿐이며
mel-spectrogram을 voice로 바꾸는 과정이다.
voice2mel은 학습 과정에서만 쓰며, 나중에는 mel을 아예 만들어내야한다.
좀 더 자세한 멜-스펙트로그램의 사용법은 task 별로 다르며 그건 vocoder 다 만들고 나서 생각하자)

나중에 코드 쓰고, 학습하고, 이런 것들은 저걸로 해도 되고 딴데서 코드 긁어와도 상관 없을듯
아니면 뭐 PCM이나 샘플레이트, 채널 맘대로 조절가능한 모델 가져오던지

코드 통째로 gemini에 넣으면 뭐하는 코드인지 알려주니 참고
}











중요한 의문점 하나,
Diff-SVS는 악보? 를 입력으로 받는다고 하는데 이것을 기반으로 한 SVC는 대체 뭘 입력으로 받고,
Conversion은 어떻게 하는가??? 애시당초에 그 '악보'라는것의 정체는 무엇인가? 시간별 주파수텐서?
Diff-SVC의 논문에서는 입력에 화자의 발성 세기, 기본 주파수의 로그값, 음소정보를
역방향 확산 과정에 넣어 멜스펙트로그램에 넣는듯하다.
추측하는 바로는, 이 확산 과정이 특정인의 목소리를 닮아가도록
즉 Diffusion에 목소리의 정체성이 담기도록 설계가 된듯하다.