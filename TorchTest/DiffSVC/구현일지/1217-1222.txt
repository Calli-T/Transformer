----- 짚고 넘어갈 사실들 -----

1. hubert, f0, mel2ph가 조건으로 사용됨
2.wavenet은 그럼 뭘 학습하는가?
(gemini 피셜)
 - 음소, 음절, 단어 단위의 발음 특징
 - 음성 신호의 시간적 패턴???
 - 음높이, 음색, 강세
 - 억양
 - 그 외 잡음 적응
(gpt 피셜)
억양은 음압변화와 주파수 변화를 모두 아우르는 개념이며
wavenet은 음소를 어떤 음압으로 바꿀 건지 결정할 수 있다.
이 때, wavenet은 sequence 모델이므로 음소간 시간적 관계를 학습하여
음소를 어떤 음압으로 바꿀지 결정하는 것 뿐만 아니라,
음소간의 전환에서 자연스럽게하는 음압 변화를 결정할 수 있다


----- 점검 -----

덜 만든 기능들
1. training
2. (학습, 추론 상관없이) 디렉터리 내부의 모든 음원을 wav로 (추가로 sampling rate 변경)
3. wav를 잘라서 배열로 만들어 사용하기 (추가로 VRAM 크기에 따른 옵션) + 추론 결과물 다시 붙이기
4. 샘플링 (가속) 기법
5. argparser(최소한 모델/학습재료/추론재료/결과물 디렉토리라도 선택할 수 있도록)
6. wav말고 flac등 다른 확장자로 생성
7. 생성 결과물의 저장 이름 변경
8. dirname를 사용하여 자동으로 가장 높은 epoch의 모델 장착

다듬어야할 부분들
1. spec_min, spec_max을 cpop이 아닌 다른 데이터셋의 것으로 변경
2.

확장 기능 계획
1. 가사만 변형하는 모델 제작
2. 그럴듯한 UI 제작 - electron
3. Window에서 동작하는 설치패키지
4. 테스트 배포용 AMD
5. vocoder 다른거 사용
6. wavenet 대신 딴거 사용? transformer 기반 모델이라던가

탐구 대상
1. wavenet의 구조
2. vocoder의 구조
3. hubert의 세부 구조 - 클러스터링 위주로
4. diffusion에서의, condition embedding(cfg와 별개로 학습할 것)
5. diffusion 샘플링 (가속) 기법 이론 배우기, PNDM부터 시작
6. crepe의 원리

----- 1220 -----

학습기를 제작 중인데 다음과 같은 코드에서 학습 기능이 파편적으로 나눠져 있다
run.py/train_pipeline.py/base_task.py/SVC_task.py
그리고 최종적으로 학습기는 pl_utils.py에 있으며,
이건 답이없다, DDP를 비롯하여 무수한 옵션, 무수한 기능이 있으며, 나에게는 그 모든것이
'지금 당장은' 필요치 않다

training 기능을 제작하기위해 필요한 것들을
함수, hparams, class 상관없이 모조리 꼽아보자면
1. optimizer
2. loss type
3. dataloader
4. wav slicer
5. training steps
6. zerograd / backward / 등등

원본의 코드에서 training용으로 음성을 자를 때는 sep_wav.py,
이를 이진화하는 preprocessing/binarize.py
대체 왜 이진화?

와중에 놀라운 사실을 하나 발견했는데,
일단 get_raw_cond에서 가장 시간이 많이 들어가는 작업은 crepe이다
(mel2ph/mel/wav/hubert는 별로 시간이 안들어간다)
그래서인지 해시에 한 번 f0로 만든거 다시 안하는 작업을 만들어놨다


순서는 dir의 모든 wav를 44100hz/mono/wav로 제작하고 ->
파일을 쪼개서 따로 저장하며 ->
이를 wav/mel/hubert/mel2ph/f0로 바꾼다
저걸 싹 npz로 저장하면, 92초에 24.3 MiB가 들어간다
2시간짜리는 1901.7 MiB이다

잘라서 싸그리 다 램에 들고 있을 것인가?
시간이 많이 들어가는 f0만 저장해둘 것인가?
아니면 모조리 npz로 저장해둘 것인가?
일단 6분 조금 넘는 영상은 'HuBERT' 조차 넘지 못했다. 15.80 GiB이다

아무래도 전부 10~15초 사이로 쪼개고
그것들의 wav/mel/hubert/mel2ph/f0중 f0는 필수로 저장, 나머지는 넣을지말지 고민해야할듯하다
일단 training은 음원을 잘라서저장하고 잘린 음원의 f0도 저장해야한다는 것이 명확하며

infer의 경우를 예외로 둘 것인지를 생각해보아야한다

44100hz, mono, 16bit, 102초 wav는 27.9 MiB이다
들어가는 시간은 wav, mel/f0/hubert/mel2ph가 각각
0.1715초
25.3734초
0.5912초
0.0022초 이며,
f0'만'을 저장했을 경우 같은 파일에 70.5 KiB가 저장된다
... f0만 저장해야겠다


----- 1222 -----

sep_wav는 '목소리 추출이 완료된 wav' 또는 '완전 무보정 상태의 mp4'만 받을 수 있다

exist_separated는 각각 10~15초 정도로 분할된 wav파일이 있는지,
exist_f0_npy는 그 파일들로 만든 f0 npy 파일이 있는지 알아보는 것이다

로직은
1. path(아마도 /train_dataset)/separated/final을 확인해서 없으면 만들어준다.
2. 음원의 리스트를 만들어 (diff class의 train 함수내로) 가져온다
3. path(아마도 /train_dataset)/f0를 확인해서 없으면 만들어준다. (이 때, 모델의 전용 함수 사용)
    get_raw_cond의 내용을 개조하여
    3-1) wav, mel, f0를 얻고
    3-2) 이를 path(아마도 /train_dataset)/f0에 npy로 하나씩 다 저장해준다
        이 때, 파일 이름은 원본이름(확장자제외)_f0.npy이다
4. 음원 리스트를 활용하여 cond를 만든다
    get_raw_cond -> tensor -> collated -> embedding의 순서를 따르되,
    training은 f0부터 만드므로 해당 과정이 필요없다
    헷갈리게 get_raw_cond에 if를 걸지말고,
    get_raw_cond_train을 따로 만든다
    해당 함수는 원본과 대체로 같지만 f0만큼은 npy로 가져온다
    -> if 걸고 걍 만들었으며, f0 load하는거 만들었다

- 현재 여기까지 작업 -

5. cond와 음원을 활용하여 mel을 만든다
딴건 다 코딩 그럴듯하게 할만한데 input용 B1MT 텐서에 노이즈 먹이기가 쉽지는 않다
6. 지상 진실의 mel을 만든다
7. 이를 mel_out과 비교하여 학습한다.


----- 1224 -----
mel.shape: (2325, 128)
mel_out.shape: torch.Size([1, 2325, 128])
collated mel:

그냥 collated_tensor의 mel을 gt_mel로 만들면 되겠는데?
차원 1인 차원축이 앞에 추가된거랑, 텐서화 된거까지 완벽함
근데 입력 차원이 (B, 1, M, T)라 걍 get_raw_cond에서 새로 만들자
loss는 gt_mel과 mel_out을 비교하자

일단 원본은 학습 때 배치가 10개던데, 10개 다 안들어갈듯? 아마 하나씩 넣는 모양
음원 개수가 173개고, 93116 steps을 기준으로 * batch_size / epoch하면
170.xxx가 나온다, 즉 epoch와 steps는 다른 개념으로 적용하며
batch_size * steps = epoch * 음원수, (아마 배치 크기가 안되는건 버리는듯)

f0가 역시나 문제였으며
f0를 제외한 임베딩 과정 까지는 VRAM을 그렇게 안잡아먹는다
VRAM은 9% (12 GiB임), GPU는 72%까지 사용하더라

근데 이거 나중에 여러 파일 학습하도록 개조해야한다 !!!!!!

아니 음원에 norm안박았다 세상에
그전에, denorm의 코드를 좀 수정했다
spec_min, max의 텐서 모양을 x에 맞춰 expand하도록(반복하여 채워넣도록)한다
다음은 denorm의 코드와 출력이다
print(x.shape, self.spec_min.shape, self.spec_max.shape)
print(type(x), type(self.spec_min), type(self.spec_max))
torch.Size([1, 2325, 128]) torch.Size([1, 1, 128]) torch.Size([1, 1, 128])
<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>
norm B1MT 맞추기랑, noise넣기랑 해야함