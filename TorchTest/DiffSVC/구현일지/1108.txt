IDDPM의 cosine schedule 만드는 코드를 시험해보는중
거기선 alpha_bar_t+1를 람다 함수로 만들고, 이를 alpha_bar_t로 나누어 alpha_t를 만든다음
이를 1에서 빼어 값을 만들어 내고 있다
효과적으로 값이 튀는것을 방지하는듯?
그것은 그렇고, alpha_cumprod는 [0, 1] 사이의 값이므로 이를 미리 생성하는 것으로, 0으로 튀는것을 방지한다

역과정이 5번을 넘어가면 색이 검어지기 시작하며, 100정도면 그냥 검은색으로 나온다
DDPM 논문 3.3에서는 [-1, 1]로 {0, 1, ..., 255}를 스케일링하고,
코드에서도 마찬가지인데, 뭔가 다른 코드가 필요한듯하다
일단 내 코드에서는 데이터셋 전체에 대해 normalize했다가 확산 다 끝나고 denormalize하는것이 전부이며, 그 과정에서 [-1, 1] clipping이나
스케일링은 없다

아래는 서로 상반대는 가설은 아니다
가설 1
혹시 학습 과정에서 scaling을 해야하는 것인가?
그래서 모델을 완전히 새로 학습해야하는 것인가?

가설 2
clip(x_start, -1, 1)을 걸면 .pt로 된 다른 모델을 load했을 때 내 코드와 호환이 될까? DDPM 깃허브에 모델들 말이다

가설 3
내 코드에 뭔가 추가해서 정상적으로 굴릴 수 있을 까?

가설 4
clip을 걸고 새로 학습 시키면 내 코드도 정상적으로 동작할 것인가?

가설 5
그렇다면 아예 학습 과정의 코드도 새로 짜야하는가?

일단 코드나 논문을 볼 때 매 확산 과정마다 clip을 걸어주는것은 맞는것같다
DDPM의 논문 3.3을 정독하고 다시 시도해보자