ema를 토치로 구현해둔게 존재한다
https://github.com/lucidrains/ema-pytorch

평균/표준편차로 비정규화 하는과정에서 해당값은 IMAGENET의 국룰 값을 따르는 방법이 있고
그 값은 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]이다.
이 디렉터리의 예시는 그냥 데이터셋의 평균과 표준편차를 따로 계산해뒀다가 따른다

추정하기로는, 어짜피 손실함수의 값에 따라 곱해지는 수가 최적화 될것이므로 국룰값도 적당히 가능한듯하다

ddpm 모델에서 '학습'이 이뤄지는것 U-Net 뿐이다
실제로 forward 함수도 U-Net(혹은 이를 카피한 EMA U-Net)뿐
그렇다면, ddpm 클래스가 nn.Module일 필요가 있는가? 그렇지는 않다!
이전의 토치예제들에서 모델 클래스를 선언하고 학습은 밖에서 했다

그렇다면 ddpm 클래스가 클래스로 만들어진 이유는 무엇인가?
반드시 그럴 필요가 없지만, 내수용 메서드와 필드로 구분하기 좋기 때문이다
그건 unet이나 ema-unet을 안에 둘 다 가지고 있어야하고,
함수들이 mean과 std등등을 사용하기 때문이다
역방향 확산 과정의 diffusion time등은 매개변수가 아니라 클래스 내부 변수로 바꿔도 괜찮을듯?
클래스 고유의 하이퍼 파라미터로 가지는것이다

그렇다면, 학습해야할 모델(클래스)가 다른 클래스 안에 있다면,
학습용 함수도 클래스의 메서드로 넣어버리면 그만이 아니겠는가?

denoise함수는 unet을 사용한다
학습(train, test 둘 다)과 역방향 확산 과정에 denoise 함수를 사용한다
역방향 확산 과정은 생성 함수에서 사용한다
denoise 함수는 잡음 예측 및 제거 함수이다
따라서, 생성과 학습 모두에서 denoise 함수를 사용하므로 잡음 예측 및 제거를 한다

잡음 제거 및 예측은 unet에서 한다
이를 위해서는 잡음낀 이미지와 잡음의 분산이 필요하다
잡음의 분산은 잡음비와 같은 값이고(noise_rates == noise_variance)
잡음비는 전부 확산 스케줄 함수에서 나온다
확산스케줄은 B_t(혹은 α_t)가 단계 t에 따라 변하는 방식이다
즉, 잡음의 분산을 가져오는 방식은 생성이나 학습이나 같다
단지 [channel, height, width]인 이미지가 무작위 잡음인가, 이미지 본연에서 나온값인가의 차이

그렇다면 unet은 뭘하고, 잡음의 분산은 무엇을 의미하는가?
unet의 하는일은 잡음 예측이다
잡음은 잡음비와 예측된 잡음의 곱이므로 둘 다 필요하다
잡음의 분산은 잡음에 곱해질 잡음비를 의미한다

잡음비는 그 자체로 표준 정규 분포를 따르지는 않고,
정규 분포를 따르는 두 집단 즉 이미지와 잡음을 더했을 때 가법성을 갖추기 위해
신호비와 잡음비를 표준편차로 하고, 둘의 제곱 즉 분산을 더하면 1이된다
그렇게 더하면 신호비*신호 + 잡음비*잡음은 평균은 0 분산이 1인 집단이 된다
따라서 임의대로 선택하되, (signal_rates^2)+(noise_rates^2) = 1이 되도록 선택하면된다
그걸 선택하는 과정이 확산 스케줄이다

그렇다면, unet에 들어가는 잡음의 분산은 이미지의 분산과 더해 1인 값인 동시에
지정된 확산 스케줄(여기서는 오프셋 코사인)을 따르는 값인것이다

diffusion_times은 균등분포로 [0, 1]안의 값을 무작위로 뽑는다
놀랍게도, torch.rand는 균등분포
torch.randn이 정규분포다

오차 함수와 최적의 경사하강법
텐서플로우 예제는 Metrics.Mean으로,
픽셀 채널 값 각각 예측과 실제 값의 차의 평균을 사용하였다
케라스 구현에서는 L1 loss 즉 차의 절대값의 평균을 사용하였다
케라스 모델 기본은 RMSprop이다
https://velog.io/@tnsida315/SGD-RMSprop-Adam

파이토치 ema 구현 예제 코드
https://www.zijianhu.com/post/pytorch/ema/

파이토치 weight 접근법에 관하여
https://velog.io/@olxtar/PyTorch-class%EB%A1%9C-%EC%83%9D%EC%84%B1%EB%90%9C-model%EC%9D%98-layer-weights-%EC%A0%91%EA%B7%BC%EB%B2%95#02-_modules

파이토치의 연산함수 뒤에 붙은 _는 in-place 함수를 의미한다
in-place 함수란, 텐서를 즉시 바꾸는 함수

누가 구현해둔 ddpm
https://github.com/KyujinHan/DDPM-study

어텐션을 사용한 ddpm, 연결고리 1!!! (github)
https://ostin.tistory.com/128

act func로 swish를 쓴다?
SiLU와 swish는 같은 대상을 다른 이름으로 쓰는듯, 요새 대세 픽인듯?
SiLU는 시?그모이드에 입력값을 곱한것

오차함수로 L1 norm을 쓰는 이유는, 전체적인 weight감소보다
나중에 골라내는게 중요해서 그런것인가?

구면 선형 보간 방법은 두 잡음을 더 했을 때 분산이 일정하게 유지된다고한다
원리는 몰라도 정규 분포의 가법성을 이용한듯하다

ratio
지수이동평균 = 지금값*승수 + 이전값(1-승수)
이전값-(1-승수)*(이전값-지금값)
= 이전값-(1-승수)*이전값 +(1-승수)*지금값
= 이전값-1* 이전값 + 승수*이전값 + (1-승수)*지금값
= 승수*이전값 + (1-승수)*지금값

확산과정을 확인할 때
NCHW규격에 맞추자
unsqueeze쓰면 steps, N, C, H, W
안쓰고 concat 때리면? -> 충분히 한꺼번에 denormalize가능
(어쨌거나 모든 과정을 다 이미지로 바꾸긴하니까)
2N C H W
3N C H W...