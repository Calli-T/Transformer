변분 추론이란 무엇인가?
다음 주소를 참소하여 그게 뭔지나 알아보자
내가 생각하기로 어려운 순서대로/정보를 조합하면 알 수 있을듯
이거 교차 검증도 필요해보인다.
https://datascienceschool.net/03%20machine%20learning/18.02%20%EB%B3%80%EB%B6%84%EB%B2%95%EC%A0%81%20%EC%B6%94%EB%A1%A0.html
https://velog.io/@chulhongsung/VI
https://greeksharifa.github.io/bayesian_statistics/2020/07/14/Variational-Inference/

변분 추론이란 무엇인가?

x란 확률 변수가 있고, 이 변수의 특성의 상당 부분은 잠재 변수 z에 의해 설명 된다(= z를 변수로 두는 분포)라고 하자
이 때 p(x)는 증거 / p(z)는 사전확률 / p(x|z)는 우도를 / p(z|x)는 사후확률로 둔다.

추론의 목표는 x의 실현 값인 데이터를 바탕으로 z의 사후확률분포 즉 p(z|x)를 구하는 것이다.
그러나많은 경우(아마 정확한 Z를 알 수 없기 때문일 것이다) p(z|x)를 구할 수가 없다.

따라서 Posterior(사후확률) p(z|x)를 알기 쉬운 분포 q(z)로 근사하는 방법을 사용한다. ※ p(z|x) -> q(z)
구체적인 식은 다음과 같다.
※ 데이터 X에 대한 데이터의 로그-주변부 확률분포라고 한다???
※※ LaTex쓸 줄 모르니 python함수로 대체한다. 적분한다는 뜻임 https://jimmy-ai.tistory.com/289 여기 참고
log p(X) = sympy.integrate(q(Z)*log(p(X,Z)/q(Z)), Z) - sympy.integrate(q(Z)*log(p(Z|X))/q(Z), Z)
이 때 첫째 항은 L(q), 두 번째 항은 KL(q||p)라고 쓴다.
L(q) = sympy.integrate(q(Z)*log(p(X,Z)/q(Z)), Z) ※ integrate의 첫 째 매개변수가 f, 둘 째 매개변수가 변수임
KL(q||p) = -sympy.integrate(q(Z)*log(p(Z|X))/q(Z), Z)

L(q)는 분포함수 q(Z)를 입력하면 수치가 출력되는 범함수이며, ELBO(Evidence Lower Bound)라고 한다
KL(q||p)는 분포함수 q(Z)와 p(Z|X) 간의 차이 즉 쿨백-라이블러 발산이다.
이는 항상 0보다 크거나 같기 때문에, L(q)는 log p(X)의 하한이 된다. 즉 log p(X) >= L(q)
반대로, log p(X)는 L(q)의 상한이다.
즉 증거는 ELBO와 쿨백-라이블러 발산의 합으로 구성되며 ELBO를 최대화하는 것은 KL-divergence가 최소화 되는것과 같은 것이다.
쿨백-라이블러 발산은 분포간의 차이 이므로 이를 최소화하는것은 옳게된 사후확률분포 p(Z|X)를 구하는 것과 같으며
위와 같은 방법을 '변분추론'이라고 한다

의문 1, log p(X)의 식은 어떻게 유도된건가
의문 2, 그렇다면 L(q) / ELBO / KL-divergence의 최적값을 찾아내는 방법은 무엇인가
