변분 추론이란 무엇인가?
다음 주소를 참소하여 그게 뭔지나 알아보자
내가 생각하기로 어려운 순서대로/정보를 조합하면 알 수 있을듯
이거 교차 검증도 필요해보인다.
https://datascienceschool.net/03%20machine%20learning/18.02%20%EB%B3%80%EB%B6%84%EB%B2%95%EC%A0%81%20%EC%B6%94%EB%A1%A0.html
https://velog.io/@chulhongsung/VI
https://greeksharifa.github.io/bayesian_statistics/2020/07/14/Variational-Inference/

변분 추론이란 무엇인가?

x란 확률 변수가 있고, 이 변수의 특성의 상당 부분은 잠재 변수 z에 의해 설명 된다(= z를 변수로 두는 분포)라고 하자
이 때 p(x)는 증거 / p(z)는 사전확률 / p(x|z)는 우도를 / p(z|x)는 사후확률로 둔다.

추론의 목표는 x의 실현 값인 데이터를 바탕으로 z의 사후확률분포 즉 p(z|x)를 구하는 것이다.
그러나많은 경우(아마 정확한 Z를 알 수 없기 때문일 것이다) p(z|x)를 구할 수가 없다.

따라서 Posterior(사후확률) p(z|x)를 알기 쉬운 분포 q(z)로 근사하는 방법을 사용한다. ※ p(z|x) -> q(z)
구체적인 식은 다음과 같다.
※ 데이터 X에 대한 데이터의 로그-주변부 확률분포라고 한다???
※※ LaTex쓸 줄 모르니 python함수로 대체한다. 적분한다는 뜻임 https://jimmy-ai.tistory.com/289 여기 참고
log p(X) = sympy.integrate(q(Z)*log(p(X,Z)/q(Z)), Z) - sympy.integrate(q(Z)*log(p(Z|X))/q(Z), Z)
이 때 첫째 항은 L(q), 두 번째 항은 KL(q||p)라고 쓴다.
L(q) = sympy.integrate(q(Z)*log(p(X,Z)/q(Z)), Z) ※ integrate의 첫 째 매개변수가 f, 둘 째 매개변수가 변수임
KL(q||p) = -sympy.integrate(q(Z)*log(p(Z|X))/q(Z), Z)

L(q)는 분포함수 q(Z)를 입력하면 수치가 출력되는 범함수이며, ELBO(Evidence Lower Bound)라고 한다
KL(q||p)는 분포함수 q(Z)와 p(Z|X) 간의 차이 즉 쿨백-라이블러 발산이다.
이는 항상 0보다 크거나 같기 때문에, L(q)는 log p(X)의 하한이 된다. 즉 log p(X) >= L(q)
반대로, log p(X)는 L(q)의 상한이다.
즉 증거는 ELBO와 쿨백-라이블러 발산의 합으로 구성되며 ELBO를 최대화하는 것은 KL-divergence가 최소화 되는것과 같은 것이다.
쿨백-라이블러 발산은 분포간의 차이 이므로 이를 최소화하는것은 옳게된 사후확률분포 p(Z|X)를 구하는 것과 같으며
이를 통해 q(Z)가 p(Z|X)와 최대한 가까워진다
위와 같은 방법을 '변분추론'이라고 한다

log p(X)의 유도 과정
q_star(x) = argmin(q∈Q)D_KL(q(z)||p(z|x))
즉 변분 추론은 q(z)와 p(z|x)간의 쿨백 라이블리 발산 값을 최소화하는 Q집합내의 q함수를 찾아내는 것이다
KL발산의 각 항을 풀고, 0이상의 값을 가짐을 활용하여 부등식을 만들면  log p(X) 즉 증거에 관한 식이된다. ※ Jensen 부등식으로도 추론 가능하다고 하는데 그게 뭔지는 모르겠다
D_KL(q(z)||p(z|x))
= sympy.integrate(q(z) * log( q(z)/p(z|x) ), z)
= sympy.integrate(q(z) * log( q(z)p(x)/p(x|z)p(z) ), z) ※ 베이즈 이론에 따라 p(z|x) = p(x|z)p(z) / p(x) <-> 1 / p(z|x) = p(x) / p(x|z)p(z)
= sympy.integrate(q(z) * log( q(z)/p(z) ), z) + sympy.integrate(q(z) * log(p(x)), z) - sympy.integrate(q(z)*log(p(x|z)), z) ※ log 안의 항을 q(z)/p(z), p(x), 1/p(x/z)로 세 개로 찢었다
= D_KL(q(z)||p(z|x)) + log(p(x)) - sympy.integrate(q(z)*log(p(x|z)), z)
※ 세 도막 중 첫 번째는 KL발산과 같다
※※ 두 번째는 z에 대해 적분하므로 log(p(x))는 상수로 빠져나오고, q(z)는 확률밀도함수이므로 적분하면 그대로 1만 나온다

이는 0보다 크거나 같은 값이므로 부등호를 넣어 정리하면
log(p(x)) >= sympy.integrate(q(z)*log(p(x|z)), z) - D_KL(q(z)||p(z|x))
여기서 우항을 ELBO라 부른다
여기서 KL발산과 KL발산을 푼 3가지의 성분요소를 식으로 나타내면 ※ 첫 식과 끝 씩을 가져온다
D_KL(q(z)||p(z|x)) = D_KL(q(z)||p(z|x)) + log(p(x)) - sympy.integrate(q(z)*log(p(x|z)), z) = log(p(x)) - ELBO
이를 정리하면
log(p(x)) = ELBO + D_KL(q(z)||p(z|x))
Evidence는 ELBO와 쿨백-라이블러 발산의 합으로 구성한다. 따라서 ELBO를 최대화하는것으로 발산을 최소화할(= 추론을 타당하게할) 수 있다 ※ 단 ELBO와 발산은 모두 q함수에 의존적이니 한 쪽만 최적화 할 수는 없다

https://greeksharifa.github.io/bayesian_statistics/2020/07/14/Variational-Inference/#21-elbo
여기서 KL읽고, 그다음에 CAVI(VAE에서 쓰는 방법이라고한다)
KL최적화 방법은 6개나 되고 생성 모델 Diffusion 방법에 따라서도 다르니 알아보자
그리고 KL이 뭔지도 나중에 알아봅시다

위 식은 KL발산에서 부터 시작하며, 아래는 전혀 다름

0814 추가)
변분 추론에서의 식은
주변 확률 즉 데이터 X와 잠재 변수 Z의 결합확률 분포 p(X) = sympy.integrate(p(X, Z), Z)의 식에서
로그를 씌우고 변분 분포 q(Z)를 도입하여 식을 변형하는 것으로 유도되었다.

log p(X)
= log(integrate(p(X, Z), Z))
= log(integrate(q(Z)*(p(X,Z)/q(Z)), Z)
여기서 젠슨 부등식을 쓰면 ※ 자연로그 미분 두 번 하면 음의 값이므로 오목함수로 두고 E[f(X)] <= f(E(x))

E_q(Z)[f(z)] = integrate(q(z)*f(z), z)로 두고
p(X,Z)= p(X|Z)p(Z)이므로
log(integrate(q(Z)*(p(X,Z)/q(Z)), Z)
= log(E_q(Z)[p(X|Z)p(Z)/q(Z)]) >= E_q(Z)[log(p(X|Z)p(Z)/q(Z)]
이 때 좌변은 log p(X)이고 우변은 ELBO라고 한다.

ELBO식을 전개하면
E_q(Z)[log(p(X|Z)p(Z)/q(Z)]
= integrate(q(Z) * ( log (p(X|Z)*p(Z) / q(Z)) ), Z) ※ 적분 내부의 log를 분해하면 아래의 식
= integrate(q(Z) * log p(X|Z), Z) + integrate(q(Z) * log p(Z)/q(Z), Z) ※ 오른쪽 p(Z)/q(Z)를 뒤집으면 로그 값은 마이너스가 되고, 이는 정확히 KL발산식의 형태와 일치하게된다
= E_q(Z)[log p(X|Z)] - KL(q(Z)||p(Z)






















( 좀 뒤죽박죽이라 무시
의문 1, log p(X)의 식은 어떻게 유도된건가
의문 2, 그렇다면 L(q) / ELBO / KL-divergence의 최적값을 찾아내는 방법은 무엇인가

(GPT)
변분 추론의 기본 개념 2개를 알고 가야한다
1. 잠재 변수 모델
데이터 X가 관측 가능한 변수라면, 잠재 변수 Z는 관측되지 않은 변수이며
이 모델에서 p(X)는 Z에 대해 적분된 형태로 표현된다.
p(X) = sympy.integrate(p(X∩Z), Z) = sympy.integrate(p(X|Z)p(Z), z)
2. 변분 근사
잠재 변수 Z에 대한 사후확률 p(Z|X)는 계산이 어려우므로 이를 근사하는 분포 q(Z)를 도입하고,
이를 최대한 가까이 한다
)