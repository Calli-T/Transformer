논문의 내용이다
https://arxiv.org/pdf/2006.11239
https://velog.io/@philiplee_235/Denoising-Diffusion-Probabilistic-Models

둘을 참고 했다

----- 서론 -----

DPM은 매개변수화 된 마르코프 체인이다.
`일정 시간 이후의 데이터에 대응되는 샘플`을 생성하는 `변분 추론`을 이용하여 학습되었다.
-> 이게 대체 뭔 소린가, `일정 시간`은 diffusion time을 / 변분 추론은 MLE 파생 방식이니 따로 알아보자
->-> 변분 추론이 뭔지는 알아봤는데 사용방법이 서로 다 다르니 생성모델별로 그 방법에 대해서나 알아봅시다

여튼 체인간의 전이? 변이?는 확산 과정을 역행하도록 학습한다고한다
-> 실제로도 정방향은 재매개변수화트릭으로 딸깍하는데, 역방향은 반복문 돌리더라, 그거 학습과정에 포함됨
->-> 확산 과정은 신호가 사라지지 않을 때까지 데이터에 잡음을 추가하는 '마르코프' 과정이라고 한다, 왜 마르코프?
->->-> 이 과정이 마르코프 체인인 이유는 2가지로 요약된다.
1. x_t는 오로지 x_t-1와 노이즈 ε에만 의존한다.
2. 노이즈 ε은 독립적이며 그 평균이 정규 분포를 따른다(행렬 곱으로 하던 그건 아닌모양, 정규분포는 확률밀도함수와 관련있다).
즉, 과거 상태와 무관하게 상태 전이가 현재 상태에만 기반한 확률적 전이이다

만약 확산 과정에서 작은 가우시안 잡음이 지속적으로 더해진다면,(forward)
표본뜨기가 일어나는 chain transition(사슬 전이, 아마 reverse process를 가리키는듯)를
조건부 가우스 잡음이 추가되는 과정으로 볼 수 있으며,
인공 신경망 매개화를 부분적으로 적용할 수 있다.
->역방향에서 조건부 가우스 잡음을 추가하는 과정은 AI를 때려박을 수 있다... 뭐 그런 얘기같다

확산모델은 똑바로 정의되고 효율적인 학습이 되지만 퀄리티가 어쩌고저쩌고... 생략

확산 모델에 특정 매개화를 적용하는 것은 다음 두 경우와 동치임을 보인다고 한다.
case1: 훈련 중 여러 잡음 단계와 denoising score matching
case2: 샘플링 중 annealed Langevin dynamics
-> 뭘 매개변수화 하는지랑, dsm과 ald???가 뭔지부터 알아야한다
->->https://wikidocs.net/230559 ald은 여기에 있다, score-based 모델과 연관이 있는 모양이다
->->-> DSM과 score-based models는 확률 분포에서 나온 스코어 함수를 학습하는 것을 목표로 하며,
스코어 함수는 log p(x)의 미분이다. 즉 s(x) = ∇_x(log p(x)) / 둘 다 자세한건 모델을 통해 알아보던지 다로 알아보던지하자
구체적인 방식은 4.2에 있다고 한다

그 외 내용들
DM은 양질의 sample을 생성하지만 다른 모델들과 비교해서log-likelyhood
이미지의 미세한 부분까지 묘사할 수 있다더라. 논문에서 손실 압축이라는 개념으로 보다 정교하게 분석한다고함
DM의 샘플링 과정은 점진적 디코딩의 한 종류이다(그리고 순차적으로 일어나는 자기회귀와 유사하며 자기회귀에서 일반적으로 일어나는 현상의 일반화 라고한다)
점진적 디코딩은 불완전한 이미지에서 이미지의 일부를 증분적으로 디코딩 하는 기능이라고 한다



----- 배경 -----
- 역과정 -
확산 모델은 잠재 변수를 가지는 모델이며 그 형태는 다음과 같다고 한다.
p_θ(x_0) = sympy.integrate(p_θ(x_0:T) dx_1:T)
x_1, x_2,...,x_T는 data x_0~q(x_0)와 같은 차원을 지니는 잠재 변수이다.
-> 아마 latent space의 벡터 길이, 측 차원을 얘기하는 모양이다
->-> q(x_0)는 원본 데이터의 분포를 의미한다. ※ 단, 원본이 아닌 근사치이다. 이를 처리하는 방법은 후술
q자체가 정방향 확산 과정을 의미하는 것이 '아니'며
q(x_1:T|x_0)가 정방향 확산 과정이다. 이를 diffusion process, forward pass 또는 approximate posterior라고한다

p_θ(x_0:T)는 결합 분포이며, 역과정으로 지칭한다.
-> 결합 분포에 대해서 자세하게 알아보자 주소는
https://hyunhp.tistory.com/175
https://ko.wikipedia.org/wiki/%EA%B2%B0%ED%95%A9%EB%B6%84%ED%8F%AC
결론적으로 결합분포란 확률 변수가 여러 개일 때 이들을 함께 고려하는 확률 분포라고 한다.
확률 분포의 일종이므로 결합 확률 분포라고도 하는듯
->-> 이산일 때와 연속일 때가 조금 다른듯 하다
이산의 경우 P(X = x and Y = y)로 표기한다 ※ 전통의 기호 &로 and를 대체 하는 경우도 많더라
P(X = x and Y = y) = P(Y = y|X = x)P(X = x) = P(X = X|Y = y)P(Y = y)

그리고 역과정 즉 p_θ(x_0:T)는
Gaussian transition을 배운 마르코프 체인으로 정의된다.
가우시안 전이(Gaussian transition)는 p(x_T) = N(x_T;0, I)로 시작한다.
-> p(x_T)는 x_T의 (이미지이니 픽셀의 값에 대한) 분포이며, 평균이 0이고 공분산 행렬이 단위 벡터인 '다변량 표준 정규 분포'이다
->-> 정방향 프로세스는 정규분포의 선형 결합에 관한 법칙에 따라 N(0, I)가 유지되도록 설계된다
p_θ(x_0:T) := p(x_T) * Π_product(t=1, T, p_θ(x_t-1|x_t))
p(x_T) = N(x_T;0, I)
p_θ(x_t-1|x_t) := N(x_t-1;μ_θ(x_t, t), Σ(x_t, t))
-> 세 번째 식의 좌변은 주어진 상태 x_t에서 이전 상태 x_t-1의 확률 분포를 나타낸다.
우변은 평균 μ_θ(x_t, t)과 공분산 행렬 Σ(x_t, t)를 갖는 정규 분포이며, θ를 모수로 사용한다
첫 째식은 전체 역과정에 관한 식이며, 세 번째 식을 반복한 것과 p(x_t)를 곱한것이다
->-> 이 식들의 나온 이론적 배경은 SDEs나 히든마르코프같은 심연의 지식을 포함한다. 나중에 알아보자???
하여튼 역과정 식이 저러하다는 배경만 알고 있자
결론: 역과정 식은 저리 생겼다

- 정과정 -
Diffusion과 다른 잠재 변수 모델과의 차이점은 근사 사후확률 q(x_1:t|x_0)에 있으며
이를 forward pass 또는 확산 과정이라고한다
(마르코프 체인 과정을 따르며 가우시안 잡음의 분산은 Β_1, Β_2, ..., Β_T에 의해 결정된다)
이는 마르코프 체인에 고정되어잇으며, 점진적으로 가우시안 잡음을 분산 스케줄 Β_1, Β_2, ..., Β_T에 따라 데이터에 추가한다

구체적인 수식은 다음과 같다
q(x_1:t|x_0) = Π_product(t=1, T, x_t|x_t-1)
q(x_t|x_t-1) := N(x_t; sqrt(1-Β_t)*x_t-1, Β_tI)
x_0 ~ q(x_0)
-> x_0는 확률분포 q(x_0)에서 나왔다. ※ 초기 이미지를 정규화 해서 q(x_0)를 다변량 표준 정규 분포로 두고 시작하는게 맞던가???
->-> x_t-1가 주어졌을 때 x_t의 조건부 확률에 관한 식에서 x_t에 관한 식을 세울 수 있다
x_t = sqrt(1-Β_t) * x_t-1 + ε ※ ε ~ N(0, Β_tI)
식의 좌우의 평균은 모두 0인 값이고, 분산은 각각 1-B_t과 B_t이므로 x_t는 다변량 표준 정규 분포를 따른다
즉 저 조건부 확률은 x_t-1가 다변량 표준 정규 분포를 따를 때 x_t도 그러하도록 스케일링한다.

훈련은 음의 로그 가능에 대한 일반적인 변분 경계를 최적화하는 방향으로 수행되었다.
그리고 그 식은 다음과 같다 ※ GPT피셜, 이는 ELBO와 젠슨 부등식의 응용이 맞다고 한다. 식이 비슷해서 넣어봄
E[-log p_θ(x_0)] <= E_q[-log p_θ(x_0:T) / q(x1:T|x_0)]
-> (GPT 피셜) 좌변은 x_0에 대한 모델의 로그 우도의 기댓값이며
우변은 변분 분포 q(x_1:T|x_0)와 결합 분포 p_θ(x_0:T) 사이의 차이를 나타내는 항이다.
※ 우변은 KL 발산의 변형이다

또한 우변은 다음과 같이 변형할 수 있다
E_q[-log p_θ(x_0:T) / q(x1:T|x_0)]
= E_q[-log p(x_T) - Σ_sum(t=1, T, log(p_θ(x_t-1|x_t) / q(x_t|x_t-1)) )]
-> 이는 역과정 식에서 시작부분 p(x_T)분포만 빼고, 각 과정을 sum(로그곱은 합이고, 각 식은 곱으로 되어있으므로)한것과같다
그리고 저 식을 L이라고 정의하더라 ※ (GPT 피셜) 사후 분포/증거 하한에 관련된 함수라고 하더라

정방향 과정의 분산인 B_t는 재매개변수화로 학습하거나 하이퍼파라미터 상수로 둘 수 있다.
그리고 역방향 과정의 표현력들은 p_θ(x_t-1|x_t)에 담긴 가우스 조건의 선택이 담긴 부분으로 보장되었습니다.
-> 역방향 과정에서 선택하는 조건들이 이미지 생성 능력을 좌지우지하는 모양이다???
왜냐하면 두 과정은 B_T가 작을 때 같은 함수 형태를 가지고 있기 때문입니다.
정방향 과정의 주목할만한 성질은 닫힌 형태의 임의 시간단계 t에
표본 x_t에 바로 접근할 수 있다는 겁니다.
-> admit이 ~에 접근할 수단이 되다로 사용된 모양이다.
이는 수식 α_t:=1-Β_t, a_t_bar:=Π_product(s=1, t, a_s)을 사용하여, 수식
q(x_t|x_0) = N(x_t; sqrt(a_t_bar)x_0, (1-a_t_bar)I)
-> 정규분포에 가법성에 따라 단계를 스킵하여 정방향 확산을 하나로 처리하는 수식, 원리는 논문에없다
그리고 x_t는 sqrt(a_t_bar)x_0 + sqrt(1-a_t_bar)*ε ※ ε ~ N(0, I)
로 나타낼 수 있다.

그러므로 효과적인 학습은 L의 무작위 조건들을 확률적 경사하강(SGD)으로 최적화하는 것으로 이루어진다.
-> 이거 오차역전파 최적화에 쓴다는 얘기가 아닐 가능성이 있다. 변분 추론 중에 이거 사용하는
방법이 있더라. https://ratsgo.github.io/generative%20model/2017/12/19/vi/
L을 다시 작성하여 분산을 감소시키면 추가 개선이 이루어진다. L의 식은 다음과 같다
-> 유도 과정이 길다보니 논문에도 부록에 있더라
위 식에서 L부터 시작한다
E_q[-log p_θ(x_0:T) / q(x1:T|x_0)]
= E_q[-log p(x_T) - Σ_sum(t=1, T, log(p_θ(x_t-1|x_t) / q(x_t|x_t-1)) )]
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t) / q(x_t|x_t-1))) - log(p_θ(x_0|x_1)/q(x_1|x_0))] ※ 역방향 마지막 과정/정방향 첫 과정에 해당하는 t=1의 사례를 따로 뺐다
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0) * q(x_t-1|x_0)/q(x_1|x_0))) - log(p_θ(x_0|x_1)/q(x_1|x_0))] ※ (Gemini) 이는 신묘한 마르코프 트릭을 사용하였다, 아래에 후첨 (20)
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0))) - Σ_sum(t=2, T, log(q(x_t-1|x_0)/q(x_t|x_0)) )  - log(p_θ(x_0|x_1)) + log(q(x_1|x_0))] ※ 20과 21번식의 중간단계, 소거를 위해 찢어놨다
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(q(x_t-1|x_0)/q(x_t|x_0)) ) + log(q(x_1|x_0)) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0))) - log(p_θ(x_0|x_1))]
여기서 앞의 로그 세 항은
log 1/p(x_T) * [q(x_2|x_0)/q(x_1|x_0) * q(x_3|x_0)/q(x_2|x_0) ... q(x_T|x_0)] * q(x_1|x_0)
= log 1/p(x_T) * [q(x_T|x_0) / q(x_1|x_0] * q(x_1|x_0)
= log 1/p(x_T) * q(x_T|x_0)
= log q(x_T|x_0)/p(x_T)
= -log p(x_T)/q(x_T|x_0)
이를 기반으로 L의 유도 과정을 이어나가면
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(q(x_t-1|x_0)/q(x_t|x_0)) ) + log(q(x_1|x_0)) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0))) - log(p_θ(x_0|x_1))]
= E_q[-log p(x_T)/q(x_T|x_0) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0))) - log(p_θ(x_0|x_1))]
여기서 쿨백-라이블러 발산을 적용하면 chap2의 식을 얻어낼 수 있다
DKL(q||p) = Σ_sum(x, q(x)*log(q(x)/p(x))
그럼 최종적으로
E_q[D_KL(q(x_T|x_0)||p(x_T)) + Σ_sum(t=2, T, D_KL(q(x_t-1|x_t, x_0||p_θ(x_t-1|x_t)))) - log(p_θ(x_0|x_1))]의 값을 얻게 된다 ※ q함수가 곱해지지 않고 KL발산으로 치환되는 이유는 아래의 후첨 설명 필요
여기서 첫 째항은 L_T 둘 째 항은 L_t-1, 마지막 항은 L0이다.
->여타 변분 추론에서 그런것 처럼 얘도 ELBO를 최대로 하는 모양

최종 E_q식은 KL 발산을 사용하여 p_θ(x_t-1|x_t)를 정방향 과정의 사후확률과 직접 비교하는데, 이는 x_0가 주어졌을 때 가능하다
-> 정방향 확산 과정에서 사용하던 분포 q에서 위치를 뒤집어 사후 확률로 만든다
x_t와 x_0를 조건으로 하여 x_t-1를 만들어내는 q분포와(q의 사후확률) 원래 역방향 확산 과정 p(x_t-1|x_t)를 KL발산으로 비교해야하고, 이건 x_0값이 존재해야 해석가능하다고 한다.
->-> 여기서 x_t에서 x_0로 갔다가 x_t-1로 가는 이론적 기반이 존재하게 된걸지도????
q(x_t-1|x_t, x_0) = N(x_t-1; ~μ(x_t, x_0), ~B_t * I)
where ~μ_t(x_t, x_0) = (sqrt(α_t-1_bar) * β)/(1-α_t_bar) * x_0 + (sqrt(a_t)*(1-a_t-1_bar)/(1-a_t_bar))*x_t and ~β_t = (1-a_t-1_bar)/(1-a_t_bar) * β_t
-> 이 식, x_0과 x_t의 가중 평균에관한 내용인 모양이다.
->-> 알파 베타 값은 잡음/신호비이며 linear나 cosine등 확산 과정에 따라 다를 수 있다.
->->-> IDDPM 코드 같은 경우는 긴 배열에다가 자르고 붙이던 그 녀석들 같은데? 이를 근사하여 사용하는 경우가 있으니 잘봐야할듯

따라서 최종 E_q식은 모든 KL 발산이 정규 분포 간의 비교이므로,
고분산 몬테카를로 추정치(이건 또 뭐야)대신 닫힌 형태의 식을 사용하여 라오-블렉웰 방식으로 계산될 수 있다.
-> 이게 뭔소린지는 차차 알아가보자

----- 본론: 확산 모델과 잡음제거 자기부호화기 -----

확산 모델은 잠재 변수 모델의 제한된 클래스로 나타날 것이나, 구현의 자유도를 높인다.
정방향 과정의 분산들 β_t와 역방향 과정의 모델 구조와 가우스 분포 파라미터화를 선택해야합니다.
-> 학습할 때 쓸 하이퍼파라미터 얘긴가?
우리의 선택을 안내하기 위해, 확산 모델과 잡음 제거 점수 매칭(3.2) 간의 새로운 명확한 연결을 확립하여,
확산 모델을 위한 단순화된 가중치 변분 경계 목적 함수를 도출했습니다.
-> DF와 DSM기법을 연결해 변분추론의 경계를 가를 가중치의 목적함수를 도출했다?
궁극적으로 우리 모델 설계는 단순성과 실험 결과(4장)에 의해 정당화 됩니다.
우리의 논의는 식 (5)의 항을 기준으로 분류됩니다.
-> ELBO 식이(에 부호 바뀐게) 식(5)였다

3.1 정방향 과정과 L_T
정방향 과정의 분산 β_t는 재매개변수화로 학습가능하다는걸 무시하고 그걸 상수로 고정해두었다 (4 Section에 자세한 내용이 있음)
그러므로 우리의 구현에서는 근사 사후확률 q는 학습가능한 파라미터들이 없다.
따라서 L_T는 학습과정에서 상수로 두고 무시될 것이다.
-> 뒤 파트를 보면 x_T가 항상 가우스 분포를 따르므로 q(x_T|x_0)와 p(x_T)가 거의(KL발산값이 10^-5정도의 차이) 유사하다고 한다

3.2 역방향 과정과 L_1:T-1
이제 p_θ(x_t-1|x_t) = N(x_t-1; μ_θ(x_t, t), Σ_θ(x_t, t)) for 1<t<T
안에 담긴 우리의 선택을 논의해보자
-> 저거 역방향 과정 한 스탭이다
먼저, 우리는 Σ_θ(x_t, t)를 훈련되지 않은 시간 의존 상수인 σ_t^2*I로 설정합니다.
실험적으로 σ_t^2 = β_t / σ_t^2 = ~β_t = (1-a_t-1_bar)/(1-a_t_bar) * β_t 는 비슷한 결과를 나타냈습니다.
-> 분산을 그냥 β_t로 쓰건 (이전 비율/지금 비율)을 곱해서 쓰건 별 차이 없던 모양
->-> 사실 t값에 따라 다르겠지만 별 차이 없이 곱하는 수가 1에 가까이 나오긴 할 듯???
->->-> 저 식 자체는 x_t가 주어졌을 때 x_t-1의 분포에 관한 식
첫 선택은 x_0가 다변량 표준 정규 분포를 따를 때 최적이며 두 번째 선택은 x_0가 한 점으로 결정론적으로 설정될 때 최적입니다.
이들은 좌표별 단위 분산을 갖는 데이터(분산이 I라는 소리인듯?)에 대한 역과정 엔트로피의 상한과 하한에 해당하는 두 극단적인 선택입니다
-> 이게 뭔소리고

두 번째로, μ_θ(x_t, t)를 표현하기 위해 (다음 L_t에 대한 분석에 자극받은) 특정 매개변수화를 제안합니다.
p_θ(x_t-1|x_t) = N(x_t-1; μ_θ(x_t, t), σ_t^2 * I) 식에서 L_t-1을 다음과 같이 쓸 수 있다.
L_t-1 = E_q[(1/2σ_t^2) * ||~μ_θ(x_t, x_0) - μ_θ(x_t, t)||^2] + C
C는 θ에 의존하지 않는 상수다. ※ 유도 과정은 하단에

그래서, μ_θ의 가장 직관적인 매개변수화는, ~μ_t 즉 전진 과정에서 사후 평균을 예측하는 모델이다
-> 이게 뭔... 소리지???

그러나 우린 8번 식을 4번식으로 재매개변수화 할 수 있다.
x_t(x_0, ε) = sqrt(a_t_bar)x_0 + sqrt(1-a_t_bar)ε for ε ~ N(0, I)
-> background에서 언급한 α_t_bar나 α_t := 1 - β_t를 이용한 loss func L_t-1항의 재매개변수화!
그리고 정방향 사후확률 공식인 7번에 적용함으로 8번식을 더 확장할 수 있으며, 다음 식과 같다
L_t-1 - C
= E_x0,ε[(1/2σ^2) * ||~μ( x_t(x_0, ε), 1/sqrt(α_t_bar) * (x_t(x_0, ε) - sqrt(1-α_t_bar)*ε) ) - μ_θ(x_t(x_0, ε), t)||^2]
= E_x0,ε[(1/2σ^2) * ||1/(sqrt(α_t) * (x_t(x_0, ε) - β/sqrt(1-α_t_bar) * ε) - μ_θ(x_t(x_0, ε), t)||^2]
-> 8번식은 오차함수, 4번식은 정방향 확산과정의 스킵, 7번식은 가중평균을 구하는데 사용하는 식이다
->-> 유도 과정은 아래에다가 써보자


















이걸 참고해봅시다
https://wikidocs.net/230583

변분 경계식 L의 유도 과정에서 (19)->(20)에는 신묘한 마르코프의 트릭이 적용되었다.
q(x_t-1|x_t) = q(x_t|x_t-1) * q(x_t-1) / q(x_t) ※ 베이즈 정리를 활용
q(x_t-1|x_t, x_0) = q(x_t|x_t-1, x_0) * q(x_t-1|x_0) / q(x_t|x_0) ※ x_0에 대한 조건을 위 식의 양변에 추가, DDPM은 초기 상태 x_0도 중요하기 때문?
이 때, q(x_t|x_t-1, x_0) ≈ q(x_t|x_t-1)이다.
DDPM 마르코프 체인을 가정하므로 현재 상태는 이전 상태에만 의존한다고 가정한다, 따라서 조건 2개중 x_0는 아무렴 상관없다
이를 정리하면
q(x_t|x_t-1, x_0) ≈ q(x_t|x_t-1) * q(x_t-1|x_0) / q(x_t|x_0)
<-> q(x_t|x_t-1) ≈ q(x_t|x_t-1, x_0) * q(x_t|x_0) / q(x_t-1|x_0)
<-> 1 / q(x_t|x_t-1) ≈ 1/q(x_t|x_t-1, x_0) * q(x_t-1|x_0)/q(x_t|x_0)
핵심은 베이즈 정리를 적용하여 순서를 바꾸고, 양변에 x_0조건을 붙인다음
마르코프 체인의 가정을 활용하여 조건 하나를 소거해버리는 것이다

논문의 (21)식에서 (22)식으로 넘어갈 때 로그의 마이너스를 뒤집어 분모<->분자끼리 뒤집고으면 KL발산과 유사한 항이 된다.
이 때 E_q는 Q분포에 대한 기대값을 구하므로, 그대로 KL발산으로 넘어가도 상관없는것같다. 다만 KL발산으로 치환하지 못하는 항도 있기는 하다.
KL발산은 정보이론에서 유래된 개념이다. 로그 비율 log(P(X)/Q(X))는 분표 P와 Q의 상대적 정보량을 측정하는데,
여기 E_p를 도입하여 E_p[log(P(X)/Q(X))]가 된다면, P분포 하에서 P와Q분포의 비율에 대한 로그 값의 평균을 구하는 것이라고 한다???????
이게 무슨 소린지 나중에 자세하게 알아보자?????

0816 추가
https://kyujinpy.tistory.com/95
여기에서는 Eq식에서 KL발산 항 2개는 밖으로 빼놓는다
저게 좀 더 자연스러운 것 같기는 하다

0819 추가
식 5에서 8로 넘어갈 때, 두 번째항 즉 L_t-1의 값 두 분포의 KL발산을 구하는 과정에서
양쪽의 분산 행렬이 모두 σ_t^2 * I = β_t * I로 같다고 가정하는(혹은 그렇게 Set하는, 3.2 section) 모양이다.
공분산 행렬이 동일한 다변량 정규 분포간의 KL 발산은 다음과 같은 형태로 단순화 된다고한다. (GPT피셜) 자세한건 다변량 정규 분포끼리의 KL발산식을 참고해보자
D_KL(N(μ_p, Σ) || N(μ_q, Σ)) = 1/2 [transpose(μ_p - μ_q) * inverse(Σ) * (μ_p - μ_q)]
이 때, 분산 행렬의 역행렬은 (1/σ_t^2) * I가 된다.
차의 전치와 차를 곱하면 두 평균 벡터의 L2 노름이 되고. 최종적으로
(1/2*σ_t^2)||μ_θ(x_t, x_0) - μ_θ(x_t, t)||^2가 된다. 이를 E_q에 넣으면 L_t-1항이 됨

식 9에서 식10으로의 변환과정
식 4에 따라, x_t(x_0, ε) = sqrt(α_t_bar)x_0 + sqrt(1-α_t_bar)ε이다
이를 x_0에 대하여 정리하면 다음과 같다 x_0 = (x_t(x_0, ε) - sqrt(1-α_t_bar)ε) / sqrt(α_t_bar)
x_0를 ~μ(x_t, x_0)에 정리하여 대입하면 다음과 같다. ※ https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ddpm/
-> 난 도저히 계산이 꼬여서 안되더라, 여기 보고함
~μ(x_t, x_0) = x_0 * (sqrt(α_t-1_bar)*β_t/(1-a_t_bar)) + x_t * (sqrt(a_t)*(1-a_t-a_bar)/(1-a_t_bar))
             = 1/sqrt(α_t_bar) * (x_t(x_0, ε) - sqrt(1-α_t_bar)*ε) * (sqrt(α_t-1_bar)*β_t/(1-a_t_bar)) + x_t * (sqrt(a_t)*(1-a_t-a_bar)/(1-a_t_bar)) ※ x_0에다 정리된 식 대입
             = x_t(x_0, ε) * ((sqrt(α_t-1_bar)/sqrt(α_t_bar) * β_t/(1-a_t_bar) + 1/sqrt(α_t) * a_t(1-a_t-1_bar)/(1-a_t_bar)) ※ x_t와 ε로 분리, x_t항에서 sqrt(a_t)항은 a_t로 변경하고 1/sqrt(a_t)로 계수를 빼줌(나중에 분리할 목적)
              + ε * (sqrt(α_t-1_bar) * β_t / ((1-a_t_bar) * sqrt(α_t_bar)) ※ 1/sqrt(α_t) = sqrt(α_t-1_bar) / sqrt(α_t_bar) = sqrt(α_t-1_bar / α_t_bar) = sqrt(Π_product(s=1, t-1, α_s) / Π_product(s=1, t, α_s)) = sqrt(1/α_t) 즉, 머리위에 줄 그어진놈들은 프로덕트의 연쇄로 인해 대부분이 약분되는것이다!!!!!!!!
             = 1/sqrt(α_t) * ( (β_t+α_t - α_t*α_t-1_bar)/(1-α_t_bar) * x_t(x_0, ε) - β_t/(1-α_t_bar) * ε) ※ α_t*α_t-1_bar = a_t_bar/β_t+α_t=1, 따라서 β_t+α_t - α_t*α_t-1_bar = β_t+α_t - a_t_bar = 1 - a_t_bar
             = 1/sqrt(α_t) * (x_t(x_0, ε) - β_t/(1-α_t_bar) * ε)