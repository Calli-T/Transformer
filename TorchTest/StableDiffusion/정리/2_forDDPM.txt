논문의 내용이다
https://arxiv.org/pdf/2006.11239
https://velog.io/@philiplee_235/Denoising-Diffusion-Probabilistic-Models

둘을 참고 했다

----- 서론 -----

DPM은 매개변수화 된 마르코프 체인이다.
`일정 시간 이후의 데이터에 대응되는 샘플`을 생성하는 `변분 추론`을 이용하여 학습되었다.
-> 이게 대체 뭔 소린가, `일정 시간`은 diffusion time을 / 변분 추론은 MLE 파생 방식이니 따로 알아보자
->-> 변분 추론이 뭔지는 알아봤는데 사용방법이 서로 다 다르니 생성모델별로 그 방법에 대해서나 알아봅시다

여튼 체인간의 전이? 변이?는 확산 과정을 역행하도록 학습한다고한다
-> 실제로도 정방향은 재매개변수화트릭으로 딸깍하는데, 역방향은 반복문 돌리더라, 그거 학습과정에 포함됨
->-> 확산 과정은 신호가 사라지지 않을 때까지 데이터에 잡음을 추가하는 '마르코프' 과정이라고 한다, 왜 마르코프?
->->-> 이 과정이 마르코프 체인인 이유는 2가지로 요약된다.
1. x_t는 오로지 x_t-1와 노이즈 ε에만 의존한다.
2. 노이즈 ε은 독립적이며 그 평균이 정규 분포를 따른다(행렬 곱으로 하던 그건 아닌모양, 정규분포는 확률밀도함수와 관련있다).
즉, 과거 상태와 무관하게 상태 전이가 현재 상태에만 기반한 확률적 전이이다

만약 확산 과정에서 작은 가우시안 잡음이 지속적으로 더해진다면,(forward)
표본뜨기가 일어나는 chain transition(사슬 전이, 아마 reverse process를 가리키는듯)를
조건부 가우스 잡음이 추가되는 과정으로 볼 수 있으며,
인공 신경망 매개화를 부분적으로 적용할 수 있다.
->역방향에서 조건부 가우스 잡음을 추가하는 과정은 AI를 때려박을 수 있다... 뭐 그런 얘기같다

확산모델은 똑바로 정의되고 효율적인 학습이 되지만 퀄리티가 어쩌고저쩌고... 생략

확산 모델에 특정 매개화를 적용하는 것은 다음 두 경우와 동치임을 보인다고 한다.
case1: 훈련 중 여러 잡음 단계와 denoising score matching
case2: 샘플링 중 annealed Langevin dynamics
-> 뭘 매개변수화 하는지랑, dsm과 ald???가 뭔지부터 알아야한다
->->https://wikidocs.net/230559 ald은 여기에 있다, score-based 모델과 연관이 있는 모양이다
->->-> DSM과 score-based models는 확률 분포에서 나온 스코어 함수를 학습하는 것을 목표로 하며,
스코어 함수는 log p(x)의 미분이다. 즉 s(x) = ∇_x(log p(x)) / 둘 다 자세한건 모델을 통해 알아보던지 다로 알아보던지하자
구체적인 방식은 4.2에 있다고 한다

그 외 내용들
DM은 양질의 sample을 생성하지만 다른 모델들과 비교해서log-likelyhood
이미지의 미세한 부분까지 묘사할 수 있다더라. 논문에서 손실 압축이라는 개념으로 보다 정교하게 분석한다고함
DM의 샘플링 과정은 점진적 디코딩의 한 종류이다(그리고 순차적으로 일어나는 자기회귀와 유사하며 자기회귀에서 일반적으로 일어나는 현상의 일반화 라고한다)
점진적 디코딩은 불완전한 이미지에서 이미지의 일부를 증분적으로 디코딩 하는 기능이라고 한다



----- 배경 -----
- 역과정 -
확산 모델은 잠재 변수를 가지는 모델이며 그 형태는 다음과 같다고 한다.
p_θ(x_0) = sympy.integrate(p_θ(x_0:T) dx_1:T)
x_1, x_2,...,x_T는 data x_0~q(x_0)와 같은 차원을 지니는 잠재 변수이다.
-> 아마 latent space의 벡터 길이, 측 차원을 얘기하는 모양이다
->-> q(x_0)는 원본 데이터의 분포를 의미한다. ※ 단, 원본이 아닌 근사치이다. 이를 처리하는 방법은 후술
q자체가 정방향 확산 과정을 의미하는 것이 '아니'며
q(x_1:T|x_0)가 정방향 확산 과정이다. 이를 diffusion process, forward pass 또는 approximate posterior라고한다

p_θ(x_0:T)는 결합 분포이며, 역과정으로 지칭한다.
-> 결합 분포에 대해서 자세하게 알아보자 주소는
https://hyunhp.tistory.com/175
https://ko.wikipedia.org/wiki/%EA%B2%B0%ED%95%A9%EB%B6%84%ED%8F%AC
결론적으로 결합분포란 확률 변수가 여러 개일 때 이들을 함께 고려하는 확률 분포라고 한다.
확률 분포의 일종이므로 결합 확률 분포라고도 하는듯
->-> 이산일 때와 연속일 때가 조금 다른듯 하다
이산의 경우 P(X = x and Y = y)로 표기한다 ※ 전통의 기호 &로 and를 대체 하는 경우도 많더라
P(X = x and Y = y) = P(Y = y|X = x)P(X = x) = P(X = X|Y = y)P(Y = y)

그리고 역과정 즉 p_θ(x_0:T)는
Gaussian transition을 배운 마르코프 체인으로 정의된다.
가우시안 전이(Gaussian transition)는 p(x_T) = N(x_T;0, I)로 시작한다.
-> p(x_T)는 x_T의 (이미지이니 픽셀의 값에 대한) 분포이며, 평균이 0이고 공분산 행렬이 단위 벡터인 '다변량 표준 정규 분포'이다
->-> 정방향 프로세스는 정규분포의 선형 결합에 관한 법칙에 따라 N(0, I)가 유지되도록 설계된다
p_θ(x_0:T) := p(x_T) * Π_product(t=1, T, p_θ(x_t-1|x_t))
p(x_T) = N(x_T;0, I)
p_θ(x_t-1|x_t) := N(x_t-1;μ_θ(x_t, t), Σ(x_t, t))
-> 세 번째 식의 좌변은 주어진 상태 x_t에서 이전 상태 x_t-1의 확률 분포를 나타낸다.
우변은 평균 μ_θ(x_t, t)과 공분산 행렬 Σ(x_t, t)를 갖는 정규 분포이며, θ를 모수로 사용한다
첫 째식은 전체 역과정에 관한 식이며, 세 번째 식을 반복한 것과 p(x_t)를 곱한것이다
->-> 이 식들의 나온 이론적 배경은 SDEs나 히든마르코프같은 심연의 지식을 포함한다. 나중에 알아보자???
하여튼 역과정 식이 저러하다는 배경만 알고 있자
결론: 역과정 식은 저리 생겼다

- 정과정 -
Diffusion과 다른 잠재 변수 모델과의 차이점은 근사 사후확률 q(x_1:t|x_0)에 있으며
이를 forward pass 또는 확산 과정이라고한다
(마르코프 체인 과정을 따르며 가우시안 잡음의 분산은 Β_1, Β_2, ..., Β_T에 의해 결정된다)
이는 마르코프 체인에 고정되어잇으며, 점진적으로 가우시안 잡음을 분산 스케줄 Β_1, Β_2, ..., Β_T에 따라 데이터에 추가한다

구체적인 수식은 다음과 같다
q(x_1:t|x_0) = Π_product(t=1, T, x_t|x_t-1)
q(x_t|x_t-1) := N(x_t; sqrt(1-Β_t)*x_t-1, Β_tI)
x_0 ~ q(x_0)
-> x_0는 확률분포 q(x_0)에서 나왔다. ※ 초기 이미지를 정규화 해서 q(x_0)를 다변량 표준 정규 분포로 두고 시작하는게 맞던가???
->-> x_t-1가 주어졌을 때 x_t의 조건부 확률에 관한 식에서 x_t에 관한 식을 세울 수 있다
x_t = sqrt(1-Β_t) * x_t-1 + ε ※ ε ~ N(0, Β_tI)
식의 좌우의 평균은 모두 0인 값이고, 분산은 각각 1-B_t과 B_t이므로 x_t는 다변량 표준 정규 분포를 따른다
즉 저 조건부 확률은 x_t-1가 다변량 표준 정규 분포를 따를 때 x_t도 그러하도록 스케일링한다.

훈련은 음의 로그 가능에 대한 일반적인 변분 경계를 최적화하는 방향으로 수행되었다.
그리고 그 식은 다음과 같다 ※ GPT피셜, 이는 ELBO와 젠슨 부등식의 응용이 맞다고 한다. 식이 비슷해서 넣어봄
E[-log p_θ(x_0)] <= E_q[-log p_θ(x_0:T) / q(x1:T|x_0)]
-> (GPT 피셜) 좌변은 x_0에 대한 모델의 로그 우도의 기댓값이며
우변은 변분 분포 q(x_1:T|x_0)와 결합 분포 p_θ(x_0:T) 사이의 차이를 나타내는 항이다.
※ 우변은 KL 발산의 변형이다

또한 우변은 다음과 같이 변형할 수 있다
E_q[-log p_θ(x_0:T) / q(x1:T|x_0)]
= E_q[-log p(x_T) - Σsum(t=1, T, log(p_0(x_t-1|x_t) / q(x_t|x_t-1)) )]
-> 이는 역과정 식에서 시작부분 p(x_T)분포만 빼고, 각 과정을 sum(로그곱은 합이고, 각 식은 곱으로 되어있으므로)한것과같다
그리고 저 식을 L이라고 정의하더라 ※ (GPT 피셜) 사후 분포/증거 하한에 관련된 함수라고 하더라






















이걸 참고해봅시다
https://wikidocs.net/230583