Diffusion Models Beat GANs on Image Synthesis
https://arxiv.org/pdf/2105.05233

때려치고 번역문 가져옴
https://hsejun07.tistory.com/248

초록
현행 SOTA 생성 모델보다 확산 모델이 더 우월한 이미지 견본에 도달할 수 있다

우린 이걸(우월한 이미지 견본) 일련의 절제를 통한 더 나은 구조를 찾는것으로
무조건적인(unconditional) 이미지 합성에 이것(SOTA?)을 달성하였다 -> 뭔소리야

우리는 조건부 이미지 합성을 위해
견본의 질을 분류자 지침
즉 분류기의 기울기를 사용하여
다양성과 견고함을 맞교환하는 간단하고
계산 효율적인 방법을 사용하여
견본의 질을 더욱 향상한다.
-> 뭔가 trade off인걸 잘 조절한듯?

※ FID: Frechet Inception Distance, 생성 영상의 집합과 실제 생성하고자 하는 클래스 데이터 분포간의 거리
가까울 수록 좋은 값, https://m.blog.naver.com/chrhdhkd/222013835684

그 아래는 대충 크기별 IMAGENET 데이터셋에서 FID점수 몇 점, 몇 점 나왔다는 내용과
깃허브 링크

----------------------------------------------------------------------------
1. 소개
몇 년 동안 생성 모델은 사람같은 자연어, 무한한 고품질 이미지 합성과
매우 다양한 사람말 그리고 음악 등을 생성할 수 있는 능력을 얻었다

이 모델들은 텍스트 프롬프트에서 이미지를 생성하거나 표현에서 유용한 특성을 학습하는등 다양하게 사용될 수 있다

이 모델들이 이미 현실적인 이미지와 사운드를 만들어내는데 유용할지라도,
현행 SOTA 모델을 너머의 많은 발전의 방들이 많이 있습니다.(발전할 여지가 많다는 뜻인듯)
그리고 더 나은 생성 모델은 그래픽 디자인, 게임, 음악 작업과 무수한 분야에 광범위한 충격이었을지도 모른다.

생성형 적대 신경망들은 지금 대부분의 이미지 생성 작업의 SOTA를 꽉 쥐고있고
이는 FID, IS, Precision 같은 견본 품질 척도로 계측되었다

그러나 저 척도들 가운데 어느 것들은 다양성을 전부 포착하지는 못한다
그리고 그건 생성형 적대 신경망들이 가능도 기반의 SOTA 모델들보다
다양성(= diversity)을 적게 잡아냈음을 보여왔다

더욱이 생성형 적대 신경망은 대부분 학습하기 어렵고
신경써서 선별된 hyperparamter와 regularizers(정규화하는장치, 정칙자)없이는 자주 붕괴(colllapsing, 기울기 얘기인듯?)해버린다.

생성형 적대 신경망(GAN)이 SOTA를 쥐고있을 때에는
GAN의 약점으로 인해 규모조절(scale)하고 새로운 영역(domains)들에 적용하기 어려웠습니다.

그 결과, 가능도 기반 모델로 GAN과 유사한(원문은 GAN-like)
품질을 달성하는데 많은 작업이 수행되었다.

이 모델들이 GAN보다 많은 더 많은 다양성을 잡아내고 일반적으로 더 쉽게 scale하고 학습하는 동안
시각적 견본 품질 측면에서 여전히 부족했었다

더욱이, VAE(변형 자기 부호화기)를 제외하고는
이 모델들로 샘플링하는것은 GAN보다 벽시계 시간으로 봐도(??? 많이 느리다는 뜻인가?) 느렸습니다

확산 모델은 가능도 기반 모델 종류이며 최근에 고품질 이미지를 생산하는걸 보여줬다
분포 범위나 변화없는 학습 객체, 쉬운 확장성 같은 바람직한 속성을 제공하면서 말이다 -> ???

이 모델들은 신호로부터 잡음을 점진적으로 생성하여 견본을 생성한다
그리고 그것들의 학습 객체는 재가중치화된 변형하한으로 표현될 수 있다 -> ???

(대충 LSUN이나 IMAGENET 등의 생성 데이터셋에서 GAN보다 밀렸으나
고성능 컴퓨터로 성능 개선을 했다는뜻)

upsampling하여 IMAGENET 256x256도 견본뜨기 가능
그러나 FID점수가 여전히 BIG-GAN보다는 별로라고함

GAN모델과의 점수차에 대한 가설 2개 세움
1. GAN모델이 문자그대로 빡세게 탐구되고 세련화되었음
2. GAN은 고품질 견본을 생산함으로 다양성과 견고함을 상충할 수 있음

그러나 전체 분포를 감당하지는 다루지는 못함 -> 이 부분이 확산 모델에 이득을 가져와줄것을 노렸음

먼저는 모델 구조를 개선하고 그 다음엔 다양성과 안정성의 상충의 계획을 고안한다
(대충 GAN을 뛰어넘을거라는뜻)

논문 구성은 섹션 2에서 확산 설명
섹션 3에서 FID를 향상시키는 개선 사항
섹션 4에서는 활용방법
섹션 5에서는

무조건 이미지 합성에는 구조 개선을 통해
조건 이미지 합성에 분류기 지침을 사용함으로
25회 만큼 적게 정방향 (확산 과정)을
BigGAN과 비교할만하게 FID를 유지하는것을 발견하였다 <- 이게 핵심인듯?

??? '조건부'라는 뜻은 조건부 확률을 사용하는 뜻 or 라벨링이 되어있다는 뜻으로 추측

----------------------------------------------------------------------------
2. 배경
확산 모델 요약 해줌, 수학적 디테일은 첨부 B를 읽어볼것...이라고함

높은 단계에서, 확산 모델은 점진적 잡음 과정을 역행하여
분포로부터 견본(샘플)을 떠온다

특히 샘플링은 잡음 x_T로부터 시작된다
점진적으로 적은 잡음을 가진 샘플 x_T-1, x_T-2...로 만들어진다
마지막 견본 x_0에 도달할 때까지

신호대 잡음비가 시간단계(timestep) t에 의해 결정된다면
각 단계에서 t는 특정 잡음 레벨(단계)에 상응하고,
x_t는 x_0와 잡음 ε의 혼합으로 생각될 수 있다

이 논문의 남은 부분을 위해
우리는 잡음 ε가 대각 가우시안 분포(다변량 표준 정규분포를 의미하는듯, 공분산 행렬의 대각성분만 1로 존재함)에서
나왔(drawn)다고 가정한다
(그리고 그 것이 자연적 이미지와 다양한 파생의 간편화된것에 잘 동작한다)

확산 모델은 x_t에서 약간 더 잡음이 제거된 x_t-1을 생성하는 방법을 학습한다
이 모델을 잡음낀 견본 x_t에서 잡음 성분을 예측할 수 있는 함수
엡실론_세타(x_t, t)로 매개변수화 하라

이 모델을 훈련하기 위해서는
미니 배치 안의 각 샘플(견본)은
무작위로 그려진 데이터 견본 x_0과 timestep t, 잡음 ε으로 부터 만들어져야하며
이는 함께 잡음낀 x_t를 생성한다

학습 개체는 ||(함수값 즉 예측잡음) - (실제잡음)||^2 (※ L2노름을 의미하는듯)이며
다시말해 예측 잡음과 실제 간의 MSE다

잡음 예측기로부터 잡음의 견본을 어떻게 뜨는가는 즉각적으로 명백하지는 못하다

확산 샘플링은 x_T에서 시작해서x_t에서 x_t-1을예측하는것을 반복적으로 진행하는한다
그것을 상기하면 그것은 합리적인 추정 아래에서
x_t가 주어졌을 때 x_t-1의 분포 pθ(xt−1|xt)를 대각 가우시안 N (xt−1; µθ(xt, t), Σθ(xt, t))로
모델링 할 수 있다
※ p_theta 옆 N(; 어쩌고, 저쩌고)는 확산 과정을 나타내는 함수, p_theta는 분포를 나타냄

가우스 분포에서의 분산 (원문에서는 Sigma_theta(x_t, t)으로 적혀있음)은
알려진 상수로 고정될 수 있고,
신경망의 head(멀티 '헤드' 어텐션의 헤드인지 단순히 머리를 나타내는건지는 모름)로 나눠져 학습가능하며
확산 과정 T가 충분히 큰 수일경우, 양쪽 접근이 모두 고품질의 견본을 만들어낼 수 있습니다

????
잡음제거 확산 모델을 VAE로 해석하여 도출할수 있는
실제 variational lower bound L_vlb보다,
평균 제곱 오차 objective L_simple이 더 잘 작동한다는것을 관찰하라?

이 목적으로 학습하고 해당 샘플링 절차를 사용하는것
= 여러 노이즈 레벨로 학습된 잡음 제거 모델에서 샘플을 추출하여 고품질 이미지를 생성하는
song and ermon의 denoising score matching model

두 모델을 모두 diffusion model로 속기한다

-----
2.1 Diffusion 모델의 개선사항

???
lower bound가 뭐냐?

+++할일
분산을 상수로 고정하는게 255p에나오는 a_t와, 재매개변수화 트릭이 '맞는지'확인해보자
아무래도 '진짜 개선사항'은 3챕과 4챕에 있는모양

가중합계는 대체 뭔소린가?

non-Markovian은 대체 또 뭔가

노이즈가 0이라는 소리는 무작위 잡음얘기인가?

50회 미만은 대체 뭔소리인가

???
일단 beta_t와 ~beta_t, L_simple + λ L_vlb를 사용해
epsilon_theta(x_t, t)와 Sigma_theta(x_t, t)를 동시에 학습하는게 뭔소리인지
출력 v를 보간하는게 또 뭔소리인지는 ddpm 논문과 함께 보면서 추후에 알아보자

일단 두 종류의 함수는 모두 x_t에서 x_t-1를 만드는 과정에서 나온것으로보아
신호비와 잡음비 혹은 해당 단계에서 신호와 잡음으로 추측하며,
분산 Sigma_theta(x_t, t)를 상수로 고정하는건 학습 때 재매개변수화 트릭을 통해 사용될 α를 무작위로 뽑아 해당 학습에서 고정으로 사용하는것으로 추측한다

비마르코프 체인이 뭔지는 모르겠다만 '이 노이즈'는 DDPM에 추가되는 무작위 잡음을 얘기하는거고
그걸0으로 만들면 결정론적인 매핑이라고 한다, 그리고 그게 50미만의 샘플링에서는 적절하다고 여겨지는듯

-----
2.2
평가 척도에 관한 얘기이다
공간 특징을 사용하는 sFID라는게 있다고한다
FID는 Inception-V3 잠재공간에서 두 이미지 분포 사이의 거리에 대한 대칭 측정을 제공한다고 한다
추측하기로는 이미지를 인셉션 합성곱 신경망에 넣고 나온 특징 벡터를 코사인 내적이라도 하는 모양

그 아래에는 늘 그렇지만 한계점을 표시