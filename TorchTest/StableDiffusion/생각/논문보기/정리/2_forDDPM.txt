논문의 내용이다
https://arxiv.org/pdf/2006.11239
https://velog.io/@philiplee_235/Denoising-Diffusion-Probabilistic-Models

둘을 참고 했다

----- 서론 -----

DPM은 매개변수화 된 마르코프 체인이다.
`일정 시간 이후의 데이터에 대응되는 샘플`을 생성하는 `변분 추론`을 이용하여 학습되었다.
-> 이게 대체 뭔 소린가, `일정 시간`은 diffusion time을 / 변분 추론은 MLE 파생 방식이니 따로 알아보자
->-> 변분 추론이 뭔지는 알아봤는데 사용방법이 서로 다 다르니 생성모델별로 그 방법에 대해서나 알아봅시다

여튼 체인간의 전이? 변이?는 확산 과정을 역행하도록 학습한다고한다
-> 실제로도 정방향은 재매개변수화트릭으로 딸깍하는데, 역방향은 반복문 돌리더라, 그거 학습과정에 포함됨
->-> 확산 과정은 신호가 사라지지 않을 때까지 데이터에 잡음을 추가하는 '마르코프' 과정이라고 한다, 왜 마르코프?
->->-> 이 과정이 마르코프 체인인 이유는 2가지로 요약된다.
1. x_t는 오로지 x_t-1와 노이즈 ε에만 의존한다.
2. 노이즈 ε은 독립적이며 그 평균이 정규 분포를 따른다(행렬 곱으로 하던 그건 아닌모양, 정규분포는 확률밀도함수와 관련있다).
즉, 과거 상태와 무관하게 상태 전이가 현재 상태에만 기반한 확률적 전이이다

만약 확산 과정에서 작은 가우시안 잡음이 지속적으로 더해진다면,(forward)
표본뜨기가 일어나는 chain transition(사슬 전이, 아마 reverse process를 가리키는듯)를
조건부 가우스 잡음이 추가되는 과정으로 볼 수 있으며,
인공 신경망 매개화를 부분적으로 적용할 수 있다.
->역방향에서 조건부 가우스 잡음을 추가하는 과정은 AI를 때려박을 수 있다... 뭐 그런 얘기같다

확산모델은 똑바로 정의되고 효율적인 학습이 되지만 퀄리티가 어쩌고저쩌고... 생략

확산 모델에 특정 매개화를 적용하는 것은 다음 두 경우와 동치임을 보인다고 한다.
case1: 훈련 중 여러 잡음 단계와 denoising score matching
case2: 샘플링 중 annealed Langevin dynamics
-> 뭘 매개변수화 하는지랑, dsm과 ald???가 뭔지부터 알아야한다
->->https://wikidocs.net/230559 ald은 여기에 있다, score-based 모델과 연관이 있는 모양이다
->->-> DSM과 score-based models는 확률 분포에서 나온 스코어 함수를 학습하는 것을 목표로 하며,
스코어 함수는 log p(x)의 미분이다. 즉 s(x) = ∇_x(log p(x)) / 둘 다 자세한건 모델을 통해 알아보던지 다로 알아보던지하자
구체적인 방식은 4.2에 있다고 한다

그 외 내용들
DM은 양질의 sample을 생성하지만 다른 모델들과 비교해서log-likelyhood
이미지의 미세한 부분까지 묘사할 수 있다더라. 논문에서 손실 압축이라는 개념으로 보다 정교하게 분석한다고함
DM의 샘플링 과정은 점진적 디코딩의 한 종류이다(그리고 순차적으로 일어나는 자기회귀와 유사하며 자기회귀에서 일반적으로 일어나는 현상의 일반화 라고한다)
점진적 디코딩은 불완전한 이미지에서 이미지의 일부를 증분적으로 디코딩 하는 기능이라고 한다



----- 배경 -----
- 역과정 -
확산 모델은 잠재 변수를 가지는 모델이며 그 형태는 다음과 같다고 한다.
p_θ(x_0) = sympy.integrate(p_θ(x_0:T) dx_1:T)
x_1, x_2,...,x_T는 data x_0~q(x_0)와 같은 차원을 지니는 잠재 변수이다.
-> 아마 latent space의 벡터 길이, 측 차원을 얘기하는 모양이다
->-> q(x_0)는 원본 데이터의 분포를 의미한다. ※ 단, 원본이 아닌 근사치이다. 이를 처리하는 방법은 후술
q자체가 정방향 확산 과정을 의미하는 것이 '아니'며
q(x_1:T|x_0)가 정방향 확산 과정이다. 이를 diffusion process, forward pass 또는 approximate posterior라고한다

p_θ(x_0:T)는 결합 분포이며, 역과정으로 지칭한다.
-> 결합 분포에 대해서 자세하게 알아보자 주소는
https://hyunhp.tistory.com/175
https://ko.wikipedia.org/wiki/%EA%B2%B0%ED%95%A9%EB%B6%84%ED%8F%AC
결론적으로 결합분포란 확률 변수가 여러 개일 때 이들을 함께 고려하는 확률 분포라고 한다.
확률 분포의 일종이므로 결합 확률 분포라고도 하는듯
->-> 이산일 때와 연속일 때가 조금 다른듯 하다
이산의 경우 P(X = x and Y = y)로 표기한다 ※ 전통의 기호 &로 and를 대체 하는 경우도 많더라
P(X = x and Y = y) = P(Y = y|X = x)P(X = x) = P(X = X|Y = y)P(Y = y)

그리고 역과정 즉 p_θ(x_0:T)는
Gaussian transition을 배운 마르코프 체인으로 정의된다.
가우시안 전이(Gaussian transition)는 p(x_T) = N(x_T;0, I)로 시작한다.
-> p(x_T)는 x_T의 (이미지이니 픽셀의 값에 대한) 분포이며, 평균이 0이고 공분산 행렬이 단위 벡터인 '다변량 표준 정규 분포'이다
->-> 정방향 프로세스는 정규분포의 선형 결합에 관한 법칙에 따라 N(0, I)가 유지되도록 설계된다
p_θ(x_0:T) := p(x_T) * Π_product(t=1, T, p_θ(x_t-1|x_t))
p(x_T) = N(x_T;0, I)
p_θ(x_t-1|x_t) := N(x_t-1;μ_θ(x_t, t), Σ(x_t, t))
-> 세 번째 식의 좌변은 주어진 상태 x_t에서 이전 상태 x_t-1의 확률 분포를 나타낸다.
우변은 평균 μ_θ(x_t, t)과 공분산 행렬 Σ(x_t, t)를 갖는 정규 분포이며, θ를 모수로 사용한다
첫 째식은 전체 역과정에 관한 식이며, 세 번째 식을 반복한 것과 p(x_t)를 곱한것이다
->-> 이 식들의 나온 이론적 배경은 SDEs나 히든마르코프같은 심연의 지식을 포함한다. 나중에 알아보자???
하여튼 역과정 식이 저러하다는 배경만 알고 있자
결론: 역과정 식은 저리 생겼다

- 정과정 -
Diffusion과 다른 잠재 변수 모델과의 차이점은 근사 사후확률 q(x_1:t|x_0)에 있으며
이를 forward pass 또는 확산 과정이라고한다
(마르코프 체인 과정을 따르며 가우시안 잡음의 분산은 Β_1, Β_2, ..., Β_T에 의해 결정된다)
이는 마르코프 체인에 고정되어잇으며, 점진적으로 가우시안 잡음을 분산 스케줄 Β_1, Β_2, ..., Β_T에 따라 데이터에 추가한다

구체적인 수식은 다음과 같다
q(x_1:t|x_0) = Π_product(t=1, T, x_t|x_t-1)
q(x_t|x_t-1) := N(x_t; sqrt(1-Β_t)*x_t-1, Β_tI)
x_0 ~ q(x_0)
-> x_0는 확률분포 q(x_0)에서 나왔다. ※ 초기 이미지를 정규화 해서 q(x_0)를 다변량 표준 정규 분포로 두고 시작하는게 맞던가???
->-> x_t-1가 주어졌을 때 x_t의 조건부 확률에 관한 식에서 x_t에 관한 식을 세울 수 있다
x_t = sqrt(1-Β_t) * x_t-1 + ε ※ ε ~ N(0, Β_tI)
식의 좌우의 평균은 모두 0인 값이고, 분산은 각각 1-B_t과 B_t이므로 x_t는 다변량 표준 정규 분포를 따른다
즉 저 조건부 확률은 x_t-1가 다변량 표준 정규 분포를 따를 때 x_t도 그러하도록 스케일링한다.

훈련은 음의 로그 가능도에 대한 일반적인 변분 경계를 최적화하는 방향으로 수행되었다.
그리고 그 식은 다음과 같다 ※ GPT피셜, 이는 ELBO와 젠슨 부등식의 응용이 맞다고 한다. 식이 비슷해서 넣어봄
E[-log p_θ(x_0)] <= E_q[-log p_θ(x_0:T) / q(x1:T|x_0)]
-> (GPT 피셜) 좌변은 x_0에 대한 모델의 로그 우도의 기댓값이며
우변은 변분 분포 q(x_1:T|x_0)와 결합 분포 p_θ(x_0:T) 사이의 차이를 나타내는 항이다.
※ 우변은 KL 발산의 변형이다

또한 우변은 다음과 같이 변형할 수 있다
E_q[-log p_θ(x_0:T) / q(x1:T|x_0)]
= E_q[-log p(x_T) - Σ_sum(t=1, T, log(p_θ(x_t-1|x_t) / q(x_t|x_t-1)) )]
-> 이는 역과정 식에서 시작부분 p(x_T)분포만 빼고, 각 과정을 sum(로그곱은 합이고, 각 식은 곱으로 되어있으므로)한것과같다
그리고 저 식을 L이라고 정의하더라 ※ (GPT 피셜) 사후 분포/증거 하한에 관련된 함수라고 하더라

정방향 과정의 분산인 B_t는 재매개변수화로 학습하거나 하이퍼파라미터 상수로 둘 수 있다.
그리고 역방향 과정의 표현력들은 p_θ(x_t-1|x_t)에 담긴 가우스 조건의 선택이 담긴 부분으로 보장되었습니다.
-> 역방향 과정에서 선택하는 조건들이 이미지 생성 능력을 좌지우지하는 모양이다???
왜냐하면 두 과정은 B_T가 작을 때 같은 함수 형태를 가지고 있기 때문입니다.
정방향 과정의 주목할만한 성질은 닫힌 형태의 임의 시간단계 t에
표본 x_t에 바로 접근할 수 있다는 겁니다.
-> admit이 ~에 접근할 수단이 되다로 사용된 모양이다.
이는 수식 α_t:=1-Β_t, a_t_bar:=Π_product(s=1, t, a_s)을 사용하여, 수식
q(x_t|x_0) = N(x_t; sqrt(a_t_bar)x_0, (1-a_t_bar)I)
-> 정규분포에 가법성에 따라 단계를 스킵하여 정방향 확산을 하나로 처리하는 수식, 원리는 논문에없다
그리고 x_t는 sqrt(a_t_bar)x_0 + sqrt(1-a_t_bar)*ε ※ ε ~ N(0, I)
로 나타낼 수 있다.

그러므로 효과적인 학습은 L의 무작위 조건들을 확률적 경사하강(SGD)으로 최적화하는 것으로 이루어진다.
-> 이거 오차역전파 최적화에 쓴다는 얘기가 아닐 가능성이 있다. 변분 추론 중에 이거 사용하는
방법이 있더라. https://ratsgo.github.io/generative%20model/2017/12/19/vi/
L을 다시 작성하여 분산을 감소시키면 추가 개선이 이루어진다. L의 식은 다음과 같다
-> 유도 과정이 길다보니 논문에도 부록에 있더라
위 식에서 L부터 시작한다
E_q[-log p_θ(x_0:T) / q(x1:T|x_0)]
= E_q[-log p(x_T) - Σ_sum(t=1, T, log(p_θ(x_t-1|x_t) / q(x_t|x_t-1)) )]
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t) / q(x_t|x_t-1))) - log(p_θ(x_0|x_1)/q(x_1|x_0))] ※ 역방향 마지막 과정/정방향 첫 과정에 해당하는 t=1의 사례를 따로 뺐다
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0) * q(x_t-1|x_0)/q(x_1|x_0))) - log(p_θ(x_0|x_1)/q(x_1|x_0))] ※ (Gemini) 이는 신묘한 마르코프 트릭을 사용하였다, 아래에 후첨 (20)
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0))) - Σ_sum(t=2, T, log(q(x_t-1|x_0)/q(x_t|x_0)) )  - log(p_θ(x_0|x_1)) + log(q(x_1|x_0))] ※ 20과 21번식의 중간단계, 소거를 위해 찢어놨다
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(q(x_t-1|x_0)/q(x_t|x_0)) ) + log(q(x_1|x_0)) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0))) - log(p_θ(x_0|x_1))]
여기서 앞의 로그 세 항은
log 1/p(x_T) * [q(x_2|x_0)/q(x_1|x_0) * q(x_3|x_0)/q(x_2|x_0) ... q(x_T|x_0)] * q(x_1|x_0)
= log 1/p(x_T) * [q(x_T|x_0) / q(x_1|x_0] * q(x_1|x_0)
= log 1/p(x_T) * q(x_T|x_0)
= log q(x_T|x_0)/p(x_T)
= -log p(x_T)/q(x_T|x_0)
이를 기반으로 L의 유도 과정을 이어나가면
= E_q[-log p(x_T) - Σ_sum(t=2, T, log(q(x_t-1|x_0)/q(x_t|x_0)) ) + log(q(x_1|x_0)) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0))) - log(p_θ(x_0|x_1))]
= E_q[-log p(x_T)/q(x_T|x_0) - Σ_sum(t=2, T, log(p_θ(x_t-1|x_t)/q(x_t-1|x_t, x_0))) - log(p_θ(x_0|x_1))]
여기서 쿨백-라이블러 발산을 적용하면 chap2의 식을 얻어낼 수 있다
DKL(q||p) = Σ_sum(x, q(x)*log(q(x)/p(x))
그럼 최종적으로
E_q[D_KL(q(x_T|x_0)||p(x_T)) + Σ_sum(t=2, T, D_KL(q(x_t-1|x_t, x_0||p_θ(x_t-1|x_t)))) - log(p_θ(x_0|x_1))]의 값을 얻게 된다 ※ q함수가 곱해지지 않고 KL발산으로 치환되는 이유는 아래의 후첨 설명 필요
여기서 첫 째항은 L_T 둘 째 항은 L_t-1, 마지막 항은 L0이다.
->여타 변분 추론에서 그런것 처럼 얘도 ELBO를 최대로 하는 모양

최종 E_q식은 KL 발산을 사용하여 p_θ(x_t-1|x_t)를 정방향 과정의 사후확률과 직접 비교하는데, 이는 x_0가 주어졌을 때 가능하다
-> 정방향 확산 과정에서 사용하던 분포 q에서 위치를 뒤집어 사후 확률로 만든다
x_t와 x_0를 조건으로 하여 x_t-1를 만들어내는 q분포와(q의 사후확률) 원래 역방향 확산 과정 p(x_t-1|x_t)를 KL발산으로 비교해야하고, 이건 x_0값이 존재해야 해석가능하다고 한다.
->-> 여기서 x_t에서 x_0로 갔다가 x_t-1로 가는 이론적 기반이 존재하게 된걸지도????
q(x_t-1|x_t, x_0) = N(x_t-1; ~μ(x_t, x_0), ~B_t * I)
where ~μ_t(x_t, x_0) = (sqrt(α_t-1_bar) * β)/(1-α_t_bar) * x_0 + (sqrt(a_t)*(1-a_t-1_bar)/(1-a_t_bar))*x_t and ~β_t = (1-a_t-1_bar)/(1-a_t_bar) * β_t
-> 이 식, x_0과 x_t의 가중 평균에관한 내용인 모양이다.
->-> 알파 베타 값은 잡음/신호비이며 linear나 cosine등 확산 과정에 따라 다를 수 있다.
->->-> IDDPM 코드 같은 경우는 긴 배열에다가 자르고 붙이던 그 녀석들 같은데? 이를 근사하여 사용하는 경우가 있으니 잘봐야할듯

따라서 최종 E_q식은 모든 KL 발산이 정규 분포 간의 비교이므로,
고분산 몬테카를로 추정치(이건 또 뭐야)대신 닫힌 형태의 식을 사용하여 라오-블렉웰 방식으로 계산될 수 있다.
-> 이게 뭔소린지는 차차 알아가보자

----- 본론: 확산 모델과 잡음제거 자기부호화기 -----

확산 모델은 잠재 변수 모델의 제한된 클래스로 나타날 것이나, 구현의 자유도를 높인다.
정방향 과정의 분산들 β_t와 역방향 과정의 모델 구조와 가우스 분포 파라미터화를 선택해야합니다.
-> 학습할 때 쓸 하이퍼파라미터 얘긴가?
우리의 선택을 안내하기 위해, 확산 모델과 잡음 제거 점수 매칭(3.2) 간의 새로운 명확한 연결을 확립하여,
확산 모델을 위한 단순화된 가중치 변분 경계 목적 함수를 도출했습니다.
-> DF와 DSM기법을 연결해 변분추론의 경계를 가를 가중치의 목적함수를 도출했다?
궁극적으로 우리 모델 설계는 단순성과 실험 결과(4장)에 의해 정당화 됩니다.
우리의 논의는 식 (5)의 항을 기준으로 분류됩니다.
-> ELBO 식이(에 부호 바뀐게) 식(5)였다

3.1 정방향 과정과 L_T
정방향 과정의 분산 β_t는 재매개변수화로 학습가능하다는걸 무시하고 그걸 상수로 고정해두었다 (4 Section에 자세한 내용이 있음)
그러므로 우리의 구현에서는 근사 사후확률 q는 학습가능한 파라미터들이 없다.
따라서 L_T는 학습과정에서 상수로 두고 무시될 것이다.
-> 뒤 파트를 보면 x_T가 항상 가우스 분포를 따르므로 q(x_T|x_0)와 p(x_T)가 거의(KL발산값이 10^-5정도의 차이) 유사하다고 한다

3.2 역방향 과정과 L_1:T-1
이제 p_θ(x_t-1|x_t) = N(x_t-1; μ_θ(x_t, t), Σ_θ(x_t, t)) for 1<t<T
안에 담긴 우리의 선택을 논의해보자
-> 저거 역방향 과정 한 스탭이다
먼저, 우리는 Σ_θ(x_t, t)를 훈련되지 않은 시간 의존 상수인 σ_t^2*I로 설정합니다.
실험적으로 σ_t^2 = β_t / σ_t^2 = ~β_t = (1-a_t-1_bar)/(1-a_t_bar) * β_t 는 비슷한 결과를 나타냈습니다.
-> 분산을 그냥 β_t로 쓰건 (이전 비율/지금 비율)을 곱해서 쓰건 별 차이 없던 모양
->-> 사실 t값에 따라 다르겠지만 별 차이 없이 곱하는 수가 1에 가까이 나오긴 할 듯???
->->-> 저 식 자체는 x_t가 주어졌을 때 x_t-1의 분포에 관한 식
첫 선택은 x_0가 다변량 표준 정규 분포를 따를 때 최적이며 두 번째 선택은 x_0가 한 점으로 결정론적으로 설정될 때 최적입니다.
이들은 좌표별 단위 분산을 갖는 데이터(분산이 I라는 소리인듯?)에 대한 역과정 엔트로피의 상한과 하한에 해당하는 두 극단적인 선택입니다
-> 이게 뭔소리고

두 번째로, μ_θ(x_t, t)를 표현하기 위해 (다음 L_t에 대한 분석에 자극받은) 특정 매개변수화를 제안합니다.
p_θ(x_t-1|x_t) = N(x_t-1; μ_θ(x_t, t), σ_t^2 * I) 식에서 L_t-1을 다음과 같이 쓸 수 있다.
L_t-1 = E_q[(1/2σ_t^2) * ||~μ_θ(x_t, x_0) - μ_θ(x_t, t)||^2] + C
C는 θ에 의존하지 않는 상수다. ※ 유도 과정은 하단에

그래서, μ_θ의 가장 직관적인 매개변수화는, ~μ_t 즉 전진 과정에서 사후 평균을 예측하는 모델이다
-> 이게 뭔... 소리지???

그러나 우린 8번 식을 4번식으로 재매개변수화 할 수 있다.
x_t(x_0, ε) = sqrt(a_t_bar)x_0 + sqrt(1-a_t_bar)ε for ε ~ N(0, I)
-> background에서 언급한 α_t_bar나 α_t := 1 - β_t를 이용한 loss func L_t-1항의 재매개변수화!
그리고 정방향 사후확률 공식인 7번에 적용함으로 8번식을 더 확장할 수 있으며, 다음 식과 같다
L_t-1 - C
= E_x0,ε[(1/2σ^2) * ||~μ( x_t(x_0, ε), 1/sqrt(α_t_bar) * (x_t(x_0, ε) - sqrt(1-α_t_bar)*ε) ) - μ_θ(x_t(x_0, ε), t)||^2]
= E_x0,ε[(1/2σ^2) * ||1/(sqrt(α_t) * (x_t(x_0, ε) - β/sqrt(1-α_t_bar) * ε) - μ_θ(x_t(x_0, ε), t)||^2]
-> 8번식은 오차함수, 4번식은 정방향 확산과정의 스킵, 7번식은 가중평균을 구하는데 사용하는 식이다
->-> 유도 과정은 아래에다가 써보자

등식 10은 x_t가 주어졌을 때, μ_θ는 1/sqrt(α_t) * (x_t - β_t/sqrt(1-α_t_bar) * ε)를 반드시 예측해야함을 드러내었다.
-> 손실함수 모양 자체가 그렇게 생겼으니까 말이다. 해당 값과 같아야 오차가 줄어든다.

x_t가 모델의 입력으로 가능하다면, 우리는 매개변수화를 다음과 같이 선택할 수 있다
-> x_t를 μ에서 뽑아 모델의 입력으로 바로 줄 수 있다고 한다.
μ_θ(x_t, t) = ~μ_t(x_t, 1/sqrt(α_t) * (sqrt(1 - α_t_bar)*ε_θ(x_t))) = 1/sqrt(α_t) - β/sqrt(1-α_t_bar) * ε_θ(x_t, t)
ε_θ는 x_t로부터 ε를 예측하는 함수 근사자이다.
-> ε_θ는 x_t로부터 잡음을 근사하게 예측하는 함수라고 한다, 뒤의 세타가 모수이고, x_t와 timestep을 입력으로 받는 모양
x_t-1을 p_θ(x_t-1|x_t)로부터 샘플링 하기 위해서는 x_t-1 = 1/sqrt(α_t) - β/sqrt(1-α_t_bar) * ε_θ(x_t, t) + σ_tz, z~N(0,I)를 계산해야한다
-> 역과정은 x_t에서 노이즈를 더하(사실 빼는) 과정이다. 그리고 노이즈도 가우스 분포를 따음
완전한 샘플링 절차인 알고리즘 2는 랑주뱅 동역학과 유사하다. ε_θ가 데이터 밀도의 학습된 기울기이다.
->랑주뱅인가 뭔가하는 역학이 있고, 그거랑 형태가 유사하다는 모양이다. 아까 ε_θ는 잡음 근사의 예측이라고 했다
->-> 샘플링 절차의 재매개변수화???

그건그렇고 등식 11은 시간t와 상태x_t에 의존하는 평균에 관한 식이고, 저위에 식은 재매개변수화가 된것이다.
그럼 매개변수화된 등식 11을 등식 10에다 넣으면, 형태가 다음과 같이 간단해진다.
E_x_0,ε[β_t^2/2σ^2α^2(1-α_t_bar) * ||ε - ε_θ(sqrt(α_t_bar)*x_0+sqrt(1-α_t_bar)ε, t)||^2]
-> 유도는 별로 어렵지가 않다. 1/sqrt(a_t)는 공통 요소이므로 제곱시켜서 빼내고, 같은 방식으로 b_t와 sqrt(1-a_t_bar)도 각각 제곱해서 빼낸다
이때, x_t는 서로 빼져서 상쇄되므로 그 2개를 빼는데 상관이 없다.
그러면 -ε와 + ε_θ(x_t, t)만 남는데 제곱이라 부호는 반대로 하여 ε - ε_θ(x_t, t)로 표기해도 별상관이 없다. x_t의 값을 넣어주면
ε - ε_θ(sqrt(α_t_bar)x_0 + sqrt(1-α_t_bar)ε, t)가 내부에 최종적으로 남게되고
전체식은 위 E와 같아진다.
저 E식은 t로 인덱싱된 여러 잡음 스케일에서의 DSM과 유사하다고 한다
-> DSM이 뭔지는 또 나중에 알아봅시다
->-> 오차를 제곱하니 기본적으로 MSE계열 손실함수고, 앞에 붙은 숫자는 가중치이다, 즉 가중된 MSE 손실함수를 나타낸다.

등식 12는 유사-랑주뱅 역과정(11)의 변분 경계의 한 항과 동일하다고 한다.
우리는 DSM과 유사한 목표를 최적화하는것이
랑주뱅 동역학과 유사한 샘플링 체인의 유한시간을 맞추기위해 변분 추론을 사용하는것과
동등하다는 것을 알 수 있다.
-> 그 둘을 모르면 뭔 소린지 알 수가 없다

요약하자면, 우리는 역방향 과정 평균 함수 근사기 μ_θ를 ~μ_t를 예측하도록 훈련할 수 있으며 ※ μ_t는 원래 분포 q의 역과정에서 나오는 평균이다.
또는 매개변수화를 수정하여 ε를 예측하도록 훈련할 수 있습니다. ※ 이 경우는 잡음만 예측하는 모양이다, 실제 UNet이 그러지 않았던가?, x_t의 잡음을 x_0를 가지고 예측하던가? x_t의 정보는 어디갔더라...???
(x_0를 예측하는 방법은 성능이 구리다고 실험적으로 나왔다고 한다)

우리는 ϵ-예측 파라미터화가 랑주뱅 동역학과 유사하고, 확산 모델의 변분 경계를 디노이징 스코어 매칭과 유사한 목표로 단순화함을 보여주었습니다.
이는 단지 p_θ(x_t-1|x_t)의 다른 파라미터화일 뿐이므로 4장에서 두 예측을 비교 검정한다고 함

3.3 데이터 스케일링과 역과정, L_0
데이터는 0~255의 정수로 되어 있고, [-1, 1]의 실수로 선형적으로 스케일링된다.
이건 신경망역과정이 표준 정규 prior p(x_t)에서 시작되고, 일관되게 스케일된 입력값에 대해 동작하는것을 보장한다.
-> [-1, 1] 사이로 스케일링하면 일관적으로 스케일 되는 것인가???
이산 로그 가능도를 얻기위해 우리는 역과정의 마지막 항을 정규분포 N(x_0; μ_θ(x_1, 1), σ_1^2I)에서 유도된 독립 이산 디코더로 설정했습니다.
-> 이게... 뭔소리지?
->-> 다음에 나오는 식이 독립 이산 디코더인듯
p_θ(x_0|x_1) = Π_product(i=1, D, sympy.integrate(N(x; μi_θ(x_1, 1), σ_1^2), x, (δ-(xi_0), δ+(xi_0))
δ+(x) = { INF       if x=1
          x+1/255   if x<1
δ-(x) = { -INF       if x=-1
          x-1/255   if x>-1
여기서 D는 데이터의 차원을, 위 첨자 i는 하나의 좌표를 추출함을 의미함

그 아래는 추가 연구 과제나 다른 모델과의 관계를 나타내는 모양

3.4 간편화된 학습 목표
위에 정의된 역과정과 디코더를 보면 식 12와 13에서 유도된 변분 경계를 이루는 항들은
θ에 대해 명백하게 미분가능합니다 그리고 학습에 사용할 수 있습니다.
-> 뭐 모수는 2개이니 거기 대해서 미분하면 될듯? 그 꼴이 어떻게 생겼을지는 짐작이 쉽지는 않다.
그러나 샘플 품질을 개선하고 구현을 간단하게 하는데 다음 변형 변분 경계식이 낫다고 판단했습니다.
L_simple(θ) := E_t,x_0,ε[||ε-ε_θ(sqrt(α_t_bar)*x_0+sqrt(1-α_t_bar)*ε, t)||^2]
-> 이 식이 이 논문의 핵심 내용인듯, 진짜 손실함수
t는 1과 T사이 uniform한 값???
-> ε_θ(x_t, t)는 GPT 피셜로 t시점에서 x_t에 포함된 노이즈이다
모델에서 x_t와 t를 입력으로 받아 예측하는 바로 그 녀석인듯(실제론 잡음의 분산을 입력받지 않았던가? t대신)
t=1인 경우는 L_0에 대응하며 이는 이산 디코더의 정의에서 가우시안 확률 밀도 함수와 빈 너비의 곱으로 근사한것. 이때 σ_1^2와 부과 효과는 무시
-> t=1의 경우 L_0에 대응한다고 한다. t가 노이즈 레벨(= 타임스탭)이니 1이 제일 작을 때로 설정해둔 모양
t>1 경우는 식 12에서 가중치가 빠진 버전이다
-> 앞에 붙는 α나 β등의 계수로 곱하는 과정을 제외한, 스케일링이 사라진 간편화된 버전을 의미한다.
->-> 각 단계의 중요도를 동등하게 보겠다는 뜻인가?
->->-> 이건 NCSN 모델에서 손실을 가중치 없이 계산하는 방식과 유사하다고 한다
L_T는 정방향 확산의 잡음비 β_t가 고정되므로 안나온다
->이건 3.1에도 나와있음, 유명무실한 값이라 무시함
알고리즘 1은 간편화된 이 학습 목적으로 이루어지는 학습절차의 완료를 보여준다.
-> algo 1에 Lsimple가지고 학습하는 모습을 알려준다는 소리

우린 단순화된 목표에서 식 12에서의 가중치를 버리므로
이는 표준 변분 경계 [18, 22]와 비교하여
재구성의 다양한 측면을 강조하는 가중치가 적용된 변분 경계입니다.
-> 앞에 계수날림 > 모든 단계가 동일 > 재구성이 다양해짐 ????
특히 4절에서 설정한 우리의 확산 과정은 단순화된 목표가 작은 t에 해당하는 손실 항목의 가중치를 낮추도록합니다.
-> 원래 t가 작을 때 값이 크게 나왔었나봄?
이 항은 신경망의 데이터에서 아주 작은 규모의 잡음 제거를 하도록 학습하며,
가중치 감소를 하는 것은 신경망이 큰 t에서의 더 어려운 잡음제거 작업에 집중하도록 하는데 이득을 줍니다.
-> 가중치(L_t-1에 붙는 계수)가 어지간히 방해됐었나?
우리는 실험에서 이 재가중화(reweight)가 더 나은 샘플 퀄리티를 내는 것을 보일 것입니다.



----- 실험 -----

실험에 쓰인 구조나 하이퍼 파라미터 등등의 내용인듯?
T = 1000으로 선택했다고 함
정방향 과정의 β 값을 선형적으로 증가시켰다고함. 그 값은 β_1 = 0.0001, β_T = 0.02
이 상수들은 [-1, 1]로 스케일된 데이터에 비해 작게 선택되었으며,
역/정방향 과정들이 x_T에서의 신호 대 잡음 비를 최대한 작게하는 와중에도
거의 같은 함수적 형태를 띄도록 보장한다.
(L_T = D_KL(q(x_T|x_0) | N(0, I)) ≈ 10^-5)
-> 함수 형태가 유사한것에 L_T가 작은것도 관련이 있던가? 일단 L_T는 거의 무시하도록하는 항이긴하다.
역방향 프로세스를 나타내기 위해 그룹 정규화를 전체적으로 사용한 unmasked PixelCNN++과 유사한 U-Net 백본을 사용한다.
-> 마스크 처리가 없는 픽셀 합성곱 신경망++이 뭔지는 잘몰라도, U-Net백본을 거기서도 사용하는듯, 일단 당장은 안중요한 내용임
매개변수는 시간을 걸쳐 공유되며, 이는 Transformer의 사인파 위치 임베딩을 사용하여 네트워크에 지정됩니다.
16x16 특성맵 해상도에 셀프 어텐션을 사용했습니다. 자세한건 부록 B에

4.1
Inception점수, FID점수, 음의 로그 우도 3개의 지표로 비교를 한 모양이다.
클래스 조건이 있건 없건 간에 우리가 만든 모델이 우수하다는 내용

실제 변분 경계로 학습하는게 단순화된 목표로 학습하는 것보다 더 나은 코드 길이를 제공한다는 것을 발견함
후자는 샘플 품질이 뛰어나다는 것을 발견함
추가적인 내용은 부록 D에 있다는 소리
-> 성능은 L_simple

4.2 역방향 과정 매개변수화와 학습 목표 Ablation(절제, 제거 등등...)
표 2에서는 역방향 매개변수호와 훈련 목표의 샘플 품질 효과를 보여줍니다.
-> 식에서 봤던 ~μ학습 vs ε학습이다.
기본 선택지 ~μ(를 학습하는 것)는
가중치를 곱하지 않은 MSE대신 참 변분 경계를 학습할 때만
14번식(L_simple의 식이다)처럼 잘동작한다는것을 발견했습니다.
역방향 분산을 학습하는것(매개변수화된 대각행렬 Σθ(x_t)를 변분 경계에 포함시킴)도
고정된 분산에 비해 훈련이 불안정하고 샘플품질이 낮아지는 결과를 가져옵니다
->
우리의 제안대로 ε를 예측하는건 고정 분산으로 변분 경계를 예측하는 경우에는
거의 ~μ를 예측하는 것만큼 (잘) 동작하지만, 우리의 단순화된 목적(L_simple)을 학습할 때 더욱 나았습니다.
-> L_simple을 쓰란 소리다.

4.3
표 1은 우리 CIFAR10 모델의 codelengths도 보여준다.
훈련과 테스트간 차이는 최대 0.03 bits per dimension(NLL loss를 차원으로 나눈 값, 차원은 아마도 픽셀 개수를 말하는듯)이므로
다른 우도 모델 기반에서의 차이와 유사하며 이는 과적합되지 않았음을 의미





이걸 참고해봅시다
https://wikidocs.net/230583

변분 경계식 L의 유도 과정에서 (19)->(20)에는 신묘한 마르코프의 트릭이 적용되었다.
q(x_t-1|x_t) = q(x_t|x_t-1) * q(x_t-1) / q(x_t) ※ 베이즈 정리를 활용
q(x_t-1|x_t, x_0) = q(x_t|x_t-1, x_0) * q(x_t-1|x_0) / q(x_t|x_0) ※ x_0에 대한 조건을 위 식의 양변에 추가, DDPM은 초기 상태 x_0도 중요하기 때문?
이 때, q(x_t|x_t-1, x_0) ≈ q(x_t|x_t-1)이다.
DDPM 마르코프 체인을 가정하므로 현재 상태는 이전 상태에만 의존한다고 가정한다, 따라서 조건 2개중 x_0는 아무렴 상관없다
이를 정리하면
q(x_t|x_t-1, x_0) ≈ q(x_t|x_t-1) * q(x_t-1|x_0) / q(x_t|x_0)
<-> q(x_t|x_t-1) ≈ q(x_t|x_t-1, x_0) * q(x_t|x_0) / q(x_t-1|x_0)
<-> 1 / q(x_t|x_t-1) ≈ 1/q(x_t|x_t-1, x_0) * q(x_t-1|x_0)/q(x_t|x_0)
핵심은 베이즈 정리를 적용하여 순서를 바꾸고, 양변에 x_0조건을 붙인다음
마르코프 체인의 가정을 활용하여 조건 하나를 소거해버리는 것이다

논문의 (21)식에서 (22)식으로 넘어갈 때 로그의 마이너스를 뒤집어 분모<->분자끼리 뒤집고으면 KL발산과 유사한 항이 된다.
이 때 E_q는 Q분포에 대한 기대값을 구하므로, 그대로 KL발산으로 넘어가도 상관없는것같다. 다만 KL발산으로 치환하지 못하는 항도 있기는 하다.
KL발산은 정보이론에서 유래된 개념이다. 로그 비율 log(P(X)/Q(X))는 분표 P와 Q의 상대적 정보량을 측정하는데,
여기 E_p를 도입하여 E_p[log(P(X)/Q(X))]가 된다면, P분포 하에서 P와Q분포의 비율에 대한 로그 값의 평균을 구하는 것이라고 한다???????
이게 무슨 소린지 나중에 자세하게 알아보자?????

0816 추가
https://kyujinpy.tistory.com/95
여기에서는 Eq식에서 KL발산 항 2개는 밖으로 빼놓는다
저게 좀 더 자연스러운 것 같기는 하다

0819 추가
식 5에서 8로 넘어갈 때, 두 번째항 즉 L_t-1의 값 두 분포의 KL발산을 구하는 과정에서
양쪽의 분산 행렬이 모두 σ_t^2 * I = β_t * I로 같다고 가정하는(혹은 그렇게 Set하는, 3.2 section) 모양이다.
공분산 행렬이 동일한 다변량 정규 분포간의 KL 발산은 다음과 같은 형태로 단순화 된다고한다. (GPT피셜) 자세한건 다변량 정규 분포끼리의 KL발산식을 참고해보자
D_KL(N(μ_p, Σ) || N(μ_q, Σ)) = 1/2 [transpose(μ_p - μ_q) * inverse(Σ) * (μ_p - μ_q)]
이 때, 분산 행렬의 역행렬은 (1/σ_t^2) * I가 된다.
차의 전치와 차를 곱하면 두 평균 벡터의 L2 노름이 되고. 최종적으로
(1/2*σ_t^2)||μ_θ(x_t, x_0) - μ_θ(x_t, t)||^2가 된다. 이를 E_q에 넣으면 L_t-1항이 됨

식 9에서 식10으로의 변환과정
식 4에 따라, x_t(x_0, ε) = sqrt(α_t_bar)x_0 + sqrt(1-α_t_bar)ε이다
이를 x_0에 대하여 정리하면 다음과 같다 x_0 = (x_t(x_0, ε) - sqrt(1-α_t_bar)ε) / sqrt(α_t_bar)
x_0를 ~μ(x_t, x_0)에 정리하여 대입하면 다음과 같다. ※ https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ddpm/
-> 난 도저히 계산이 꼬여서 안되더라, 여기 보고함
~μ(x_t, x_0) = x_0 * (sqrt(α_t-1_bar)*β_t/(1-a_t_bar)) + x_t * (sqrt(a_t)*(1-a_t-a_bar)/(1-a_t_bar))
             = 1/sqrt(α_t_bar) * (x_t(x_0, ε) - sqrt(1-α_t_bar)*ε) * (sqrt(α_t-1_bar)*β_t/(1-a_t_bar)) + x_t * (sqrt(a_t)*(1-a_t-a_bar)/(1-a_t_bar)) ※ x_0에다 정리된 식 대입
             = x_t(x_0, ε) * ((sqrt(α_t-1_bar)/sqrt(α_t_bar) * β_t/(1-a_t_bar) + 1/sqrt(α_t) * a_t(1-a_t-1_bar)/(1-a_t_bar)) ※ x_t와 ε로 분리, x_t항에서 sqrt(a_t)항은 a_t로 변경하고 1/sqrt(a_t)로 계수를 빼줌(나중에 분리할 목적)
              + ε * (sqrt(α_t-1_bar) * β_t / ((1-a_t_bar) * sqrt(α_t_bar)) ※ 1/sqrt(α_t) = sqrt(α_t-1_bar) / sqrt(α_t_bar) = sqrt(α_t-1_bar / α_t_bar) = sqrt(Π_product(s=1, t-1, α_s) / Π_product(s=1, t, α_s)) = sqrt(1/α_t) 즉, 머리위에 줄 그어진놈들은 프로덕트의 연쇄로 인해 대부분이 약분되는것이다!!!!!!!!
             = 1/sqrt(α_t) * ( (β_t+α_t - α_t*α_t-1_bar)/(1-α_t_bar) * x_t(x_0, ε) - β_t/(1-α_t_bar) * ε) ※ α_t*α_t-1_bar = a_t_bar/β_t+α_t=1, 따라서 β_t+α_t - α_t*α_t-1_bar = β_t+α_t - a_t_bar = 1 - a_t_bar
             = 1/sqrt(α_t) * (x_t(x_0, ε) - β_t/(1-α_t_bar) * ε)

----- 요약 -----
학습할 때는 L_simple을 오차함수로 쓴다음 역전파하면(원문의 알고리즘 항에는 나블라를 사용한다) 모델 학습가능
이 식은 EBLO를 변형한것을 변형하고 변형하고 축약한 것으로, 궁극적으로 q와 p분포 사이의 거리를 계산한다.
q는 근사 후방 분포, p는 진짜 후방 분포를 나타낸다.

샘플링은 x_T~N(0,I)로 시작
x_t-1 = (1/sqrt(α_t)) * (x_t - (1-α_t/sqrt(1-α_t))*ε_θ(x_t, t)) + σ_t*z
z~N(0,I) if t > 1, else z = 0
이는 수식 10에서 나왔다.
수식 10은 L_t-1의 간소화되지 않은 형태 즉 잡음이 아닌 이미지를 예측하는 형태에서 유래되었다.
µ_θ - ~µ_t가 0이 되도록하는것이 L_t-1 오차함수의 목적이므로, 역으로 이걸 예측하는게 샘플링 과정인듯?(추측)
여기 있는 버전엔 랜덤 잡음도 들어간다

책에 쓰여있는 샘플링 버전은 x_t의 잡음을 예측하고, 이를 바탕으로 x_0를 만든 다음 x_t와 가중평균을 내는 방식이다.
랜덤잡음도 들어간다고는하는데, 실제 코드에는 안넣었고 이를 DDIM이라고하더라