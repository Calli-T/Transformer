https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ddim/
https://arxiv.org/abs/2010.02502
https://github.com/ermongroup/ddim
순서대로 리뷰/논문/깃허브 구현체

----------서론 (요약) ----------

GAN의 성능이 (여러 모델보다) 괜찮은데,
최적화와 아키텍쳐의 선택(모델구조, 하이퍼파라미터 등등을 의미하는 모양)의 선택이 구체적이여야하고
데이터 분포의 모든 distribution을 다루지 못할 수 있습니다.
-> GANs의 장단점

DDPM이나 NCSN같은 반복적인 생성모델은 GAN없이도 비슷한 수준의 샘플을 생성할 수 있음
이를 위해 오토인코더 여러 단계의 가우시안 잡음으로 손상된 샘플들을 복원하는 모델이 학습됨
백색 잡음에서 점진적으로 잡음을 제거하여 이미지로 변환하는 마르코프 체인에 의해 생성하거나
이미지를 잡음으로 점진적으로 변환하는 순방향 확산 과정을 역전시켜 생성
-> 이건 x_T에서 마르코프체인(확률분포가 다변량 표준 정규 분포에만 의존함)에 의한 여러 식들로 실제로 x_0로 변경된다.
->-> 아래쪽은 정방향을 뒤집는다는 소린데 사후분포를 우도로 바꾼다는 소린가?

이런 모델들의 치명적인 단점은 고퀄리티 이미지를 뽑는데는 너무 많은 반복이 필요하다고 한다.
DDPM이 특히 그런데, 정방향의 역과정을 근사하는걸 데이터가 일일이 다 통과해야하므로 수천의 단계가 필요함
하나 만드는데 단계 반복을 해야하므로 GANs보다 느림

여하튼 이런 DDPM과 GAN사이의 효율차이를 줄이기 위해 고안된게 DDIM이고,
동일한 목적함수(아마 L, 혹은 오차함수 얘기인듯)로 학습된다는 점에서 DDPM과 유사하다고 한다
-> 샘플링만 다르게 할 수 있다고 했었던가?

밑의 사진은 확산 모델과 비마르코프 추론 모델의 개념을 그래프 형태로 시각화 한 것이다

3장에서는 DDPM에서 사용되는 마르코프 성질을 가진 정방향 확산 과정을 비마르코프 확산 과정으로 일반화한다고 한다.
여전히 적절한 역방향 생성 마르코프 체인을 설계할 수 있다.
-> 의외로 생성이 아니라 학습만 비마르코프로 바꾼게 DDIM이다. Gemini피셜, 정확하다고 한다.
->-> Gemini의 추가설명에서는 DDPM이 노이즈 제거 단계에서 이전 단계의 정보만을 활용하는 마르코프 성질을 가지는 반면,
DDIM은 여러 이전 단계의 정보를 활용하여 더욱 정확한 예측을 수행한다고 한다.
이러한 변분 학습 목적함수들이 공통의 대리 목적 함수를 갖는 것을 보여주며, DDPM의 학습의 목적함수와 정확히 일치한다고한다.
-> 뒤에 나오는 내용인 모양
동일한 신경망을 사용해 단순히 다른 비마르코프 확산 과정(4.1절)과 해당하는 역방향생성 마르코프 체인을 선택함으로
다양한 생성 모델 중에서 자유롭게 선택할 수 있습니다.
-> ???
특히 우리는 적은 수의 단계로 시뮬레이션 될수 있는 "짧은" 생성 마르코프 체인을
이끄는 비마르코프 확산 과정을 사용할 수 있습니다.
-> 이거 4.2절에서 나오는 내용이라고 한다.

5장의 내용은 DDIM이 DDPM에 비교해 실험적인 이점을 갖는다는것을 보여주는것
샘플링 속도 가속, 샘플링 일관성, DDIM의 일관성을 사용하여 초기 잠재 변수를 조작하여 의미있는 이미지 보간 가능
-> 빠르고 일관됨

---------- 배경 ----------
주어진 데이터 분포 q(x_0)에서 추출된 샘플을 바탕으로, q(x_0)을 근사하는 동시에 샘플링이 용의한 p_θ(x_0)를
학습하는데 관심이 있다.
아래에는 p의 수식이 있는데, where에서 파이로 각 시간 단계의 조건부 확률을 곱하는 것은 마르코프 체인을 나타내며,
p_theta(x_0:T)는 x_0부터 x_T를 각각의 연속 확률 변수로 뒀을 때 이들의 결합 확률(모두 다 일어날 확률)을 의미하는 것이다
적분이 존재하는건 이게 연속이기 때문에 그렇다.
(Gemini 피셜) x_0가 이전 모든 상태를 고려하여 나올 확률을 계산하는 과정이라고 한다

x_1,...,x_T가 잠재변수(이미지가 어떤 분포에서 나왔다고 가정하는 분표를 따르는 변수)들이면
x_0도 동일하게 그러한 것을 가리킨다.
-> 생성과정 즉 잡음 낀 이미지들이 잠재변수라면, x_0도 그러할 것으로 보인다.

모수 θ는 다음 변분 하한을 최대화하는 것으로 데이터 분포 q(x_0)에 적합하도록 학습된다.
(아래에 있는 식은 DDPM의 음의 로그 가능도 식과 유사하며 부호가 반대로 바뀌었고, 최대화 해야한다)
-> 이 부분에서는 변분 경계, 오차 함수, 목적 함수, 음의 로그 우도 등등 몇 가지 키워드를 가지고 좀 더 알아내보자