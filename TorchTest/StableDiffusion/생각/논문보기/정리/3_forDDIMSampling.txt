https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ddim/
https://arxiv.org/abs/2010.02502
https://github.com/ermongroup/ddim
순서대로 리뷰/논문/깃허브 구현체

----------서론 (요약) ----------

GAN의 성능이 (여러 모델보다) 괜찮은데,
최적화와 아키텍쳐의 선택(모델구조, 하이퍼파라미터 등등을 의미하는 모양)의 선택이 구체적이여야하고
데이터 분포의 모든 distribution을 다루지 못할 수 있습니다.
-> GANs의 장단점

DDPM이나 NCSN같은 반복적인 생성모델은 GAN없이도 비슷한 수준의 샘플을 생성할 수 있음
이를 위해 오토인코더 여러 단계의 가우시안 잡음으로 손상된 샘플들을 복원하는 모델이 학습됨
백색 잡음에서 점진적으로 잡음을 제거하여 이미지로 변환하는 마르코프 체인에 의해 생성하거나
이미지를 잡음으로 점진적으로 변환하는 순방향 확산 과정을 역전시켜 생성
-> 이건 x_T에서 마르코프체인(확률분포가 다변량 표준 정규 분포에만 의존함)에 의한 여러 식들로 실제로 x_0로 변경된다.
->-> 아래쪽은 정방향을 뒤집는다는 소린데 사후분포를 우도로 바꾼다는 소린가?

이런 모델들의 치명적인 단점은 고퀄리티 이미지를 뽑는데는 너무 많은 반복이 필요하다고 한다.
DDPM이 특히 그런데, 정방향의 역과정을 근사하는걸 데이터가 일일이 다 통과해야하므로 수천의 단계가 필요함
하나 만드는데 단계 반복을 해야하므로 GANs보다 느림

여하튼 이런 DDPM과 GAN사이의 효율차이를 줄이기 위해 고안된게 DDIM이고,
동일한 목적함수(아마 L, 혹은 오차함수 얘기인듯)로 학습된다는 점에서 DDPM과 유사하다고 한다
-> 샘플링만 다르게 할 수 있다고 했었던가?

밑의 사진은 확산 모델과 비마르코프 추론 모델의 개념을 그래프 형태로 시각화 한 것이다

3장에서는 DDPM에서 사용되는 마르코프 성질을 가진 정방향 확산 과정을 비마르코프 확산 과정으로 일반화한다고 한다.
여전히 적절한 역방향 생성 마르코프 체인을 설계할 수 있다.
-> 잘못된 내용일 가능성이 있어서 설명 적어놓은거 폐기, 맨 아래에 추가
이러한 변분 학습 목적함수들이 공통의 대리 목적 함수를 갖는 것을 보여주며, DDPM의 학습의 목적함수와 정확히 일치한다고한다.
-> 뒤에 나오는 내용인 모양
동일한 신경망을 사용해 단순히 다른 비마르코프 확산 과정(4.1절)과 해당하는 역방향생성 마르코프 체인을 선택함으로
다양한 생성 모델 중에서 자유롭게 선택할 수 있습니다.
-> ???
특히 우리는 적은 수의 단계로 시뮬레이션 될수 있는 "짧은" 생성 마르코프 체인을
이끄는 비마르코프 확산 과정을 사용할 수 있습니다.
-> 이거 4.2절에서 나오는 내용이라고 한다.

5장의 내용은 DDIM이 DDPM에 비교해 실험적인 이점을 갖는다는것을 보여주는것
샘플링 속도 가속, 샘플링 일관성, DDIM의 일관성을 사용하여 초기 잠재 변수를 조작하여 의미있는 이미지 보간 가능
-> 빠르고 일관됨

---------- 배경 ----------
주어진 데이터 분포 q(x_0)에서 추출된 샘플을 바탕으로, q(x_0)을 근사하는 동시에 샘플링이 용의한 p_θ(x_0)를
학습하는데 관심이 있다.
아래에는 p의 수식이 있는데, where에서 파이로 각 시간 단계의 조건부 확률을 곱하는 것은 마르코프 체인을 나타내며,
p_theta(x_0:T)는 x_0부터 x_T를 각각의 연속 확률 변수로 뒀을 때 이들의 결합 확률(모두 다 일어날 확률)을 의미하는 것이다
적분이 존재하는건 이게 연속이기 때문에 그렇다.
(Gemini 피셜) x_0가 이전 모든 상태를 고려하여 나올 확률을 계산하는 과정이라고 한다

x_1,...,x_T가 잠재변수(이미지가 어떤 분포에서 나왔다고 가정하는 분표를 따르는 변수)들이면
x_0도 동일하게 그러한 것을 가리킨다.
-> 생성과정 즉 잡음 낀 이미지들이 잠재변수라면, x_0도 그러할 것으로 보인다.
모수 θ는 다음 변분 하한을 최대화하는 것으로 데이터 분포 q(x_0)에 적합하도록 학습된다.
(아래에 있는 식은 DDPM의 음의 로그 가능도 식과 유사하며 부호가 반대로 바뀌었고, 최대화 해야한다)
-> 이 부분에서는 변분 경계, 오차 함수, 목적 함수, 음의 로그 우도 등등 몇 가지 키워드를 가지고 좀 더 알아내보자
->-> 해당 식에서 q(x_1:T|x_0)는 잠재 변수들에 대한 몇몇 추론 분포이다.
VAE같이 다른 특정 잠재 모델들과는 다르게 DDPM은 학습 되지 않는, 즉 고정된 추론 절차
-> 0902 추가) 뭐가 고정된걸까? 감마 스케일?
q(x_1:T|x_0)를 사용하며, 잠재 변수의 차원이 상대적으로 높다.
-> 뭐 그렇다고 한다
DDPM 논문에서는 가우시안 전이를 따르는 마르코프 체인을 고려했으며,
가우시안 전이는 감소하는 순서 α_1:T ∈ (0, 1]^T로 매개변수화 되었다
-> 알파 값에 따라 각 단계의 (마르코프 전이를 조절할) 정규 분포가 달라지며, 이는 시간 인덱스 T가 증가함에 따라 줄어든다
q(x_1:T|x_0):= Π_product(t=1, T, q(x_t|x_t-1)),
where q(x_t|x_t-1) := N(sqrt(α_t/α_t-1)x_t-1, (1-(α_t/α_t-1))I)
-> DDPM 논문의 2번식이다, α_t/α_t-1인건 전단계의 조건식 때문에 β가 ~β로 바뀌게 되어서 그런듯하다
->-> 사실 α_t/α_t-1는 1로 간주하긴 하더라

공분산행렬의 대각 성분이 양수인 곳에서 저 식이 유효하다.
이는 자기회귀적 샘플링 절차(x_0에서 x_T까지)때문에 순방향 과정이라고 합니다.
잠재 변수 모델에서 p(x_0:T)는 x_T에서 x_0까지 샘플링하는 마르포크 체인으로, 다루기 어려운 역과정
q(x_t-1|x_t)를 근사하기 때문에 생성과정이라고 합니다.
직관적으로, 순방향 과정은 관측값 x_0에 점진적으로 노이즈를 추가하며,
한편으로 생성 과정은 노이즈 관측값을 점진적으로 제거합니다.

정방향 과정의 특별한 것은 ~ (수식)
-> 재매개변수화 트릭으로 x_0에서 x_t로 바로 가는 정방향 과정을 각 과정의 결합 확률을 적분한 것으로 나타냈으며,
이를 다시한번 정규 분포의 값으로 표현하였다.
그래서 우린 x_t를 x_0와 잡음 변수 ε의 선형 결합으로 표현할 수 있다.
x_t = sqrt(α_t)x_0 + sqrt(1-α_t)ε, where ε ~ N(0, I)

α_T를 0에 충분히 가깝게 설정하면, q(x_T|x_0)는 모든 x_0에서 표준 정규 분포로 수렴합니다.
-> 신호비를 0에 가깝게 가면 x_T가 결국 표준 정규 분포를 따르게 된다는 의미인듯하다.
->-> 평균이 0으로, 공분산의 계수가 1에 가까워질테니 당연한소리
따라서, p_θ(x_T):=N(0,I)로 두는것은 자연스럽습니다.
-> x_T에 대한 근사분포 p를 다변량 표준 정규 분포로 둔다는 소리다.

모든 조건이 평균 함수와 고정된 분산으로 학습된 가우시안으로 만들어진다면,
등식 2의 목적(함수)는 다음과 같이 단순화 될 수 있습니다.
L_γ(ε_θ):= Σ_sum(t=1, T, γ_t*E_x_0, ε_t[||ε_θ(t)(sqrt(α_t)x_0 + sqrt(1-α_t)ε) - ε_t||^2]), where x_0~q(x_0), ε_t~N(0,I)
이 때 ε_θ:= {ε_θ(t)}는 T에 관한 함수의 집합이며,
이들 각각은 t로 인덱스 되고, X->X로 가는 함수이고 ※ X는 잠재변수들의 표본공간, 정의역과 치역을 나타낸듯함
학습가능한 매개변수 θ(t)로 되어있다.
γ := [γ_1,...,γ_T]는 목적(함수)에 있는 양의 계수인 공분산 벡터들이며 ※ ???
목적(함수)는 α_1:T에 의존합니다.
-> 일단 Loss_gamma(epsilon_theta)는 잡음에 관한 함수인듯하다
->-> 0902추가) γ란 각 시간 단계 t에서 발생하는 목적(오차)함수에 설정되어 곱해지는 계수이며, '하이퍼파라미터'이다
임의대로 설정할 수 있으며 아래에 나올 L_1은 모든 감마값이 1인 목적함수이다. 이 설정은 DDPM의 기본 노이즈 스케줄을 따르는 경우와 동일하다.
입실론이 잡음이며, 세타가 붙은놈은 매개변수로 나온놈(모델이 예측한 잡음), t가붙은놈은 그냥 시간단계를 따르는 잡음인듯
감마는 각 단계의 공분산 함수에 붙은 계수이거나, 아니면 뭔지 모르겠음, 1까지 가는거 보니 맞는것같긴한데
x_0와 t에 따른 수식
즉 예측 잡음과 실제 잡음의 차이의 제곱
즉 MSE에 대한 기댓값을 나타내며,
여기에 감마를 왜 곱하는 것인지는 모르겠다, 일단 여럿 있으니 각 단계의 오차를 모두 합한 값인듯함
-
DDPM논문에서는 γ를 1로 설정할 때, 목적(함수)는 훈련된 모델의 성능을 최대화 화도록 최적화한다.
-> 0902추가)
->->이건 NCSN에서의 목적 함수와 같으며, 점수 매칭 기반이다.
-
학습된 모델에서, x_0는 최초 샘플링 x_T에서 샘플되며, x_T는 사후확률분포 p_θ(x_T)에 의한 것이고,
x_t-1를 생성 과정에서 반복적으로 샘플링합니다.
-> x_T를 분포 p에서 (아마도 무작위)로 뽑은 다음, x_t-1를 뽑는 과정을 반복해서 x_0를 만들어낸다
->-> 샘플링 과정을 묘사함

정방향 과정에서 T의 길이는 DDPM에서 중요한 하이퍼파라미터이다.
변수적 관점에서는, 큰 T는 역방향 과정을 표준 정규 분포에 가깝게 하도록 합니다.
따라서 가우스 조건부 분포로 모델된 생성 과정은 좋은 근사가 됩니다.
이는 Ho et al.의 논문에서 T=1000과 같은 큰 값을 선택하는데 동기가 되었습니다.
-> 정방향 과정에서 T가 크면 N(0, I)에 가까워 지긴 하더라
그러나 샘플 x_0를 만들기 위해
모든 T반복을 평행적으로 하지 않고 순차적으로 수행해야만 하는 것은
DDPM에서 샘플링하는것이 다른 딥러닝 생성 모델들보다 훨씬 느리게 만들었습니다.
-> 역과정을 T만큼 반복하면 그만큼 느리기야 하겠지
이건 컴퓨팅 자원이 제한되면서 지연시간이 중요한 작업에는 실용적이지 못함을 의미합니다.



---------- 비마르코프 정방향 과정을 위한 변분 추론 ----------

생성 모델이 추론 과정의 역을 근사하므로,
우린 생성 모델의 반복 요구량을 줄이기 위해 추론 과정을 다시 생각해볼 필요가 있습니다.
우리의 핵심 관측은 DDPM 목적함수 L_γ가 q(x_t|x_0)의 주변 분포에만 의존하고 있으며, ※ 이 용어는 x_0에만 조건을 받을 경우 결합 확률에서와 같이 이 용어들을 약간 오용하고 있습니다... 라고 미주에 쓰여있다.
결합 분포 q(x_1:T|x_0)에 직접적으로 의존하지 않는다는 것입니다.
-> DDPM논문의 4번식에서 단계들을 압축한 폼이 있다는것을 보여준 바가 있다
여러 추론 분포들이(결합) 같은 주변 분포로 되어있으며,
우리는 대안적 추론 과정들이 비마르코프라는 사실을 탐구했으며, 이는 새로운 생성 과정으로 이어집니다.
->-> 목적함수 즉 오차함수는 q(x_t|x_0)의 주변 분포에만 의존한다고 한다. 여러 추론 분포가 그러하다고함
이 비마르코비안 추론 과정은 DDPM와 동일한 대체 목적 함수로 이어집니다.
부록 A에서 우리는 비마르코비안 관점이 가우시안 경우를 넘어서도 적용되는 것을 보여줍니다.
->-> 이건 좀 나중에 보자???

3.1 비마르코비안 정방향 과정들
실수 벡터 σ ∈ R^T_≥0로 색인된 추론 분포들 Q로 이루어진 family에 대해 고려해보자.
->0부터 T까지 각 단계에서 분포의 모수들이 시그마이며, 그건 실수 벡터로 이루어져있는 모양이다.
q_σ(x_1:T|x_0) := q_σ(x_T|x_0) * Π_product(t=2, T, q_σ(x_t-1|x_t, x_0)),
where q_σ(x_T|x_0) = N(sqrt(α_T)x_0, sqrt(1-α_T)*I) and all t > 1,
q_σ(x_t-1|x_t, x_0) = N(sqrt(α_t-1)*x_0 + sqrt(1-α_(t-1)-σ_t^2)*(x_t-α_t*x_0)/sqrt(1-α_t), σ_t^2I)
-> 각각 x_0부터 x_T까지의 각 단계별 모든 조건을 충족하는 결합 확률 분포/
위 식에서 각 단계마다 곱해지는 q분포의 비마르코프 조건부 확률에 대한 정의)/
x_t와 x_0를 가지고 보간하여 x_t-1를 만드는 과정을 의미하는 모양이다.
-
이 평균 함수는 q_σ(x_t|x_0) = N(sqrt(α_T)x_0, sqrt(1-α_T)*I)가 모든 t에 대해서 보장되도록 선택되었다. ※ 보조정리 1과 부록 B를 보라
-> 이론적 배경은 나중에 보자???
->-> 저거 예측된 x_t와 α값을 모델에 넣어 예측한 잡음과 x_0에 관한 분포 아니냐?
이는 결합 추론 분포가 바라던 대로 주변 분포에 들어맞도록 정의되었다.
정방향 과정은 베이즈 정리에서 나온다.
q_σ(x_t|x_t-1, x_0) = q_σ(x_t-1|x_t, x_0) * q_σ(x_t|x_0) / q_σ(x_t-1|x_0) ※ 증명은 하단에
그리고 또한 가우스 분포에서 나온다.(우리가 이 사실을 이 논문에 상기용으로 적어놓지 않았더라도)
-> 각 단계가 전부 정규 분포긴 하다
확산 과정의 식 3과 달리, ※ q(x_t|x_0)는 결합확률의 분포이며 이는 다시 정규분포를 따른다는 사실을 적어둠
정방향 과정은 더 이상 마르코프 과정이 아니며, x_t가 x_t-1와 x_0에 의존한다.
->-> 베이즈 정리의 조건 쪽에 조건이 2개있긴하더라
σ의 규모는 정방향 과정이 얼마나 확률적인지를 조절한다.
σ가 0이되면 어떤 t에서 x_t와 x_0가 x_t-1를 알려지고, 고정시킨다는 특수한 경우에 다다른다.
-> 무작위 잡음 추가 안하면 샘플 과정이 고정되기는(결정론적이기는) 할듯

3.2 생성 과정과 통합된 변분 추론 목적(함수)
다음으로, 우리는 각 p_θ^(t)(x_t|x_t-1)가 q_σ(x_t-1|x_t, x_0)의 지식을 활용하는
학습가능한 생성 과정 p_θ(x_0:T)를 정의합니다.
-> 근사된 분포의 잡음 단계 t로의 정방향 과정이, 위에서 정의한 역과정의 정보를 사용하는 모양이다
실제로 x_0를 예측한다음 잡음과 보간하여 x_t-1를 만들어내지 않았던가? 그런 의미인가
->-> 여기서 역과정이 비마르코비안 과정이므로 DDIM의 추론이 비마르코프 과정인건가???
-
직관적으로 잡음섞인 관측 x_t가 주어졌다면, 우리는 먼저 대응하는 x_0를 만든다.
그리고 이를 정의된 역방향 조건 분포 q_σ(x_t-1|x_t, x_0)에 넣어
x_t-1를 만드는데 사용한다
-> 3.1의 정의를 말하는듯하다. 논문에서는 (7)번식을 얘기하는듯

x_t는 x_0 ~ q(x_0)와 ε_t ~ N(0, I)를 가지고 등식 4번을 사용하여 만들 수 있다.
-> DDPM논문에서 등식 (4)번을 봐도 명확하다
-
ε_θ(t)(x_t) 모델은 x_t에서 x_0에 대한 정보 없이 ε_t의 예측을 시도한다.
등식 4번을 다시 작성하여 잡음이 제거된 관측을 예측할 수 잇으며, 이는 x_t로 예측한 x_0이다.
(해당 등식은 다음과 같음)
f_θ(t)(x_t) := (x_t - sqrt(1-α_t)ε_θ(t)(x_t))/sqrt(α_t)
-> 그냥 4번식을 정리한 것이며, 잡음의 표기가 단계 t에서 모수 θ에 대해 예측한 잡음으로 바뀐모양
-
우리는 생성 과정을 고정된 사후 분포 p_θ(x_T) = N(0, I)를 가지고 정의할 수 있으며
-> 늘 나오는 얘기지만, 그렇게 되도록 잡음을 추가했으니까
그리고 그 생성과정이라는 것은
p_θ(t)(x_t-1|x_t) = { N(f_θ(1)(x_1), σ_1^2I) if t = 1
                    { q_σ(x_t-1|x_t, f_θ(t)(x_t)) otherwise,
-> f_θ어쩌고 하는건 x_t로 예측한 x_0이며,
이를 x_t와 함께 3.1에서 정의한 역과정에서 넣는다.
t = 1의 경우 정확하게 어떤 예외 상황을 집어넣은 건지는 아직 모르겠???음
-
위 식은 등식 7로 정의 되었으며, x_0가 f_θ(t)(x_t)로 대체된 것이다. 라고 논문에도 쓰여있음
-
우리는 t=1인 경우, 생성 과정이 어디서나 동작하도록 약간의 (공분산이 σ_1^2I인) 가우시안 잡음을 추가한다
-> 이건 논문 리뷰를 봤는데, DDPM과 달리 x_1에서 x_0를 생성할 때도 가우스 잡음을 추가한다고한다.
실제로 t>2인 뭔가가 있었던것같은데 뭔지는 잘 기억이 안남???

우리는 아래 (ε_θ에대한 함수인)변분 추론 목적 함수를 통해 θ를 최적화한다
J_σ(ε_θ) := E_x_0:T~q_σ(x0:T)[log q_σ(x_1:T|x_0) - log p_θ(x_0:T)]
E_x_0:T~q_σ(x0:T)[log q_σ(x_T|x_0) + Σ_sum(t=2, T, log q_σ(x_t-1|x_t,x_0)) - Σ_sum(t=1, T, log p_θ(t)(x_t-1|x_t) - log p_θ(x_T))]
이는 q를 등식 6번에, p를 등식 1번에 대해 푼것이다
-> 로그를 써서 곱을 합으로 바꾼듯
->-> 원문은 factorize, 인수분해라고 하더라
-
J_σ의 정의는 서로다른 모델이 모든 σ의 선택에 대해 학습되어야함을 나타내고,
이는 서로 다른 변분 목적 함수(와 서로 다른 생성 과정)에 대응되기 때문이다.
-> 이게 무슨소리지?
->-> 보통 VAE같은건 I에 대해 학습하지 않던가
-
그러나 J_σ는 특정 γ값들에만 L_γ와 동등하며 이는 우리가 아래에 보일 것입니다.
-> 5번식에 Loss gamma가 있으며, 이는 잡음과 잡음 예측의 MSE이며 γ란 1~T 까지 목적 함수의 공분산 벡터이다.(= 각 단계의 잡음비이다)

정리 1. 모든 σ > 0에 대하여, J_σ = L_γ + C를 만족하는 γ ∈ R>0^T와 C ∈ R가 존재한다.
-> 증명은 https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ddim/에 두고 생략한다
변분 목적 함수 L_γ는 특별한 성질을 가지는데,
만약 모델 ε_θ(t)의 파라미터 θ가 서로다른 t에서 공유되지 않는다면
ε_θ의 최적해는 가중치 γ에 의존하지 않습니다.
합의 각 항을 개별적으로 최대화하는것으로 전역 최적점에 도달하기 때문이다.
-> 뭐의 합인데??? 오차함수?
->-> L1부터 LT까지 각 오차들의 함을 의미하는 듯하다
{ 추측임 나중에 폐기될 수 있음
->->-> 정황상 x_T부터 x_0까지 변환하는 과정에서 발생하는 여러 L_t들을 바탕으로 모수를 한꺼번에 갱신하는거 말고,
각 과정에서 오차역전파로 모수를 갱신 즉 모수를 서로 공유를 안하면 가중치 γ에 대한 의존이 없는 모양이다
그런 것으로 추측한다
결론은 역전파 한 스텝 땡길 때마다 오차역전파 하란 소린가?
->->->-> (GPT 4 mini피셜), 맞다, 근데 LLM들이 헛소리를 하는건지는 잘 모르겠다
}
L_γ의 이 특징은 두 가지를 의미한다.
먼저는 DDPM의 변분 하한에 대한 대리 목적 함수로 L_1을 사용하는 것을 정당화 한다.
다른 하나는 J_σ가 어떤 L_γ와 정리 1에서 동등하며 최적의 J_σ는 L_1과 같다.
그러므로, 파라미터들이 모델 ε_θ에서 서로 다른 t에 공유되는 것이 아니라면,
DDPM 논문에서 사용된 L_1 목적 함수는 변분 목적함수 J_σ에 대한 대리 목적 함수로 사용될 수 있다.
-> 이게 무슨 소리지??? https://jang-inspiration.com/ddim 이것도 한 번 보자

---------- 일반화된 생성 과정에서 샘플링하기 ----------
L_1을 목적함수로 사용하면 DPM과 DDPM에서 고려된 마르코프 추론 과정에 대한 생성 과정 뿐만 아니라,
σ로 매개변수화된 많은 비마르코프 정방향 과정에 대한 생성 과정도 학습하게 됩니다.
-> 이게 뭔소리야, 결국 DDPM의 오차함수를 쓰란 소린가?
->-> 마르코프 말고 비마르코프 생성 과정도 L_1(= DDPM의 목적함수)로 학습할 수 있다? 그래서 DDIM에는 원래 모델 사용가능하다는 소린가?
따라서 우리는 기본적으로 사전학습된 DDPM을 새로운 목적함수에 대한 해로 사용할 수 있으며,
σ를 바꿈으로 필요에 따른(직역은 '우리의 요구들에 종속된') 샘플들을 만드는데에
더 나은 생성 과정을 발견하는데 집중할 수 있습니다.
-> 학습 모델 쓰고 샘플링만 DDIM 쓴다는 소린가? 표준 편차를 변경하는건 대체 뭐야
->-> 표준 편차를 조절해서 그냥 고양이만 생성하는 모델에 귀여운 고양이, 특정 품종의 고양이 등등을 만들 수 있다는 뜻인가












{
    P(A|B,C)=P(B|A,C)P(A|C)/P(B|C)

    검증
    P(A,B,C) = P(A|B, C) * P(B,C)
    P(A,B,C) = P(B|A,C) * P(A,C) = P(B|A, C) * P(A|C) * P(C)

    끝의 두 식을 연립하면
    P(A|B, C) * P(B,C) = P(B|A, C) * P(A|C) * P(C)
    <->
    P(A|B, C) = ( P(B|A, C) * P(A|C) * P(C) ) / P(B,C)
    = ( P(B|A, C) * P(A|C) * P(C) ) / (P(B|C) * P(C))
    = P(B|A, C) * P(A|C) / P(B|C)
}


폐기된 내용
{
-> 의외로 생성이 아니라 학습만 비마르코프로 바꾼게 DDIM이다. Gemini피셜, 정확하다고 한다.
->-> Gemini의 추가설명에서는 DDPM이 노이즈 제거 단계에서 이전 단계의 정보만을 활용하는 마르코프 성질을 가지는 반면,
DDIM은 여러 이전 단계의 정보를 활용하여 더욱 정확한 예측을 수행한다고 한다.

-> 공분산행렬에 붙는 계수를 1까지 올리란 소리같다
->-> 인제보니 감마는 잡음비 최대값 같은데?
}