https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ddim/
https://arxiv.org/abs/2010.02502
https://github.com/ermongroup/ddim
순서대로 리뷰/논문/깃허브 구현체

----------서론 (요약) ----------

GAN의 성능이 (여러 모델보다) 괜찮은데,
최적화와 아키텍쳐의 선택(모델구조, 하이퍼파라미터 등등을 의미하는 모양)의 선택이 구체적이여야하고
데이터 분포의 모든 distribution을 다루지 못할 수 있습니다.
-> GANs의 장단점

DDPM이나 NCSN같은 반복적인 생성모델은 GAN없이도 비슷한 수준의 샘플을 생성할 수 있음
이를 위해 오토인코더 여러 단계의 가우시안 잡음으로 손상된 샘플들을 복원하는 모델이 학습됨
백색 잡음에서 점진적으로 잡음을 제거하여 이미지로 변환하는 마르코프 체인에 의해 생성하거나
이미지를 잡음으로 점진적으로 변환하는 순방향 확산 과정을 역전시켜 생성
-> 이건 x_T에서 마르코프체인(확률분포가 다변량 표준 정규 분포에만 의존함)에 의한 여러 식들로 실제로 x_0로 변경된다.
->-> 아래쪽은 정방향을 뒤집는다는 소린데 사후분포를 우도로 바꾼다는 소린가?

이런 모델들의 치명적인 단점은 고퀄리티 이미지를 뽑는데는 너무 많은 반복이 필요하다고 한다.
DDPM이 특히 그런데, 정방향의 역과정을 근사하는걸 데이터가 일일이 다 통과해야하므로 수천의 단계가 필요함
하나 만드는데 단계 반복을 해야하므로 GANs보다 느림

여하튼 이런 DDPM과 GAN사이의 효율차이를 줄이기 위해 고안된게 DDIM이고,
동일한 목적함수(아마 L, 혹은 오차함수 얘기인듯)로 학습된다는 점에서 DDPM과 유사하다고 한다
-> 샘플링만 다르게 할 수 있다고 했었던가?

밑의 사진은 확산 모델과 비마르코프 추론 모델의 개념을 그래프 형태로 시각화 한 것이다

3장에서는 DDPM에서 사용되는 마르코프 성질을 가진 정방향 확산 과정을 비마르코프 확산 과정으로 일반화한다고 한다.
여전히 적절한 역방향 생성 마르코프 체인을 설계할 수 있다.
-> 의외로 생성이 아니라 학습만 비마르코프로 바꾼게 DDIM이다. Gemini피셜, 정확하다고 한다.
->-> Gemini의 추가설명에서는 DDPM이 노이즈 제거 단계에서 이전 단계의 정보만을 활용하는 마르코프 성질을 가지는 반면,
DDIM은 여러 이전 단계의 정보를 활용하여 더욱 정확한 예측을 수행한다고 한다.
이러한 변분 학습 목적함수들이 공통의 대리 목적 함수를 갖는 것을 보여주며, DDPM의 학습의 목적함수와 정확히 일치한다고한다.
-> 뒤에 나오는 내용인 모양
동일한 신경망을 사용해 단순히 다른 비마르코프 확산 과정(4.1절)과 해당하는 역방향생성 마르코프 체인을 선택함으로
다양한 생성 모델 중에서 자유롭게 선택할 수 있습니다.
-> ???
특히 우리는 적은 수의 단계로 시뮬레이션 될수 있는 "짧은" 생성 마르코프 체인을
이끄는 비마르코프 확산 과정을 사용할 수 있습니다.
-> 이거 4.2절에서 나오는 내용이라고 한다.

5장의 내용은 DDIM이 DDPM에 비교해 실험적인 이점을 갖는다는것을 보여주는것
샘플링 속도 가속, 샘플링 일관성, DDIM의 일관성을 사용하여 초기 잠재 변수를 조작하여 의미있는 이미지 보간 가능
-> 빠르고 일관됨

---------- 배경 ----------
주어진 데이터 분포 q(x_0)에서 추출된 샘플을 바탕으로, q(x_0)을 근사하는 동시에 샘플링이 용의한 p_θ(x_0)를
학습하는데 관심이 있다.
아래에는 p의 수식이 있는데, where에서 파이로 각 시간 단계의 조건부 확률을 곱하는 것은 마르코프 체인을 나타내며,
p_theta(x_0:T)는 x_0부터 x_T를 각각의 연속 확률 변수로 뒀을 때 이들의 결합 확률(모두 다 일어날 확률)을 의미하는 것이다
적분이 존재하는건 이게 연속이기 때문에 그렇다.
(Gemini 피셜) x_0가 이전 모든 상태를 고려하여 나올 확률을 계산하는 과정이라고 한다

x_1,...,x_T가 잠재변수(이미지가 어떤 분포에서 나왔다고 가정하는 분표를 따르는 변수)들이면
x_0도 동일하게 그러한 것을 가리킨다.
-> 생성과정 즉 잡음 낀 이미지들이 잠재변수라면, x_0도 그러할 것으로 보인다.
모수 θ는 다음 변분 하한을 최대화하는 것으로 데이터 분포 q(x_0)에 적합하도록 학습된다.
(아래에 있는 식은 DDPM의 음의 로그 가능도 식과 유사하며 부호가 반대로 바뀌었고, 최대화 해야한다)
-> 이 부분에서는 변분 경계, 오차 함수, 목적 함수, 음의 로그 우도 등등 몇 가지 키워드를 가지고 좀 더 알아내보자
->-> 해당 식에서 q(x_1:T|x_0)는 잠재 변수들에 대한 몇몇 추론 분포이다.
VAE같이 다른 특정 잠재 모델들과는 다르게 DDPM은 학습 되지 않는, 즉 고정된 추론 절차
q(x_1:T|x_0)를 사용하며, 잠재 변수의 차원이 상대적으로 높다.
-> 뭐 그렇다고 한다
DDPM 논문에서는 가우시안 전이를 따르는 마르코프 체인을 고려했으며,
가우시안 전이는 감소하는 순서 α_1:T ∈ (0, 1]^T로 매개변수화 되었다
-> 알파 값에 따라 각 단계의 (마르코프 전이를 조절할) 정규 분포가 달라지며, 이는 시간 인덱스 T가 증가함에 따라 줄어든다
q(x_1:T|x_0):= Π_product(t=1, T, q(x_t|x_t-1)),
where q(x_t|x_t-1) := N(sqrt(α_t/α_t-1)x_t-1, (1-(α_t/α_t-1))I)
-> DDPM 논문의 2번식이다, α_t/α_t-1인건 전단계의 조건식 때문에 β가 ~β로 바뀌게 되어서 그런듯하다
->-> 사실 α_t/α_t-1는 1로 간주하긴 하더라

공분산행렬의 대각 성분이 양수인 곳에서 저 식이 유효하다.
이는 자기회귀적 샘플링 절차(x_0에서 x_T까지)때문에 순방향 과정이라고 합니다.
잠재 변수 모델에서 p(x_0:T)는 x_T에서 x_0까지 샘플링하는 마르포크 체인으로, 다루기 어려운 역과정
q(x_t-1|x_t)를 근사하기 때문에 생성과정이라고 합니다.
직관적으로, 순방향 과정은 관측값 x_0에 점진적으로 노이즈를 추가하며,
한편으로 생성 과정은 노이즈 관측값을 점진적으로 제거합니다.

정방향 과정의 특별한 것은 ~ (수식)
-> 재매개변수화 트릭으로 x_0에서 x_t로 바로 가는 정방향 과정을 각 과정의 결합 확률을 적분한 것으로 나타냈으며,
이를 다시한번 정규 분포의 값으로 표현하였다.
그래서 우린 x_t를 x_0와 잡음 변수 ε의 선형 결합으로 표현할 수 있다.
x_t = sqrt(α_t)x_0 + sqrt(1-α_t)ε, where ε ~ N(0, I)

α_T를 0에 충분히 가깝게 설정하면, q(x_T|x_0)는 모든 x_0에서 표준 정규 분포로 수렴합니다.
-> 신호비를 0에 가깝게 가면 x_T가 결국 표준 정규 분포를 따르게 된다는 의미인듯하다.
->-> 평균이 0으로, 공분산의 계수가 1에 가까워질테니 당연한소리
따라서, p_θ(x_T):=N(0,I)로 두는것은 자연스럽습니다.
-> x_T에 대한 근사분포 p를 다변량 표준 정규 분포로 둔다는 소리다.

모든 조건이 평균 함수와 고정된 분산으로 학습된 가우시안으로 만들어진다면,
등식 2의 목적(함수)는 다음과 같이 단순화 될 수 있습니다.
L_γ(ε_θ):= Σ_sum(t=1, T, γ_t*E_x_0, ε_t[||ε_θ(t)(sqrt(α_t)x_0 + sqrt(1-α_t)ε) - ε_t||^2]), where x_0~q(x_0), ε_t~N(0,I)
이 때 ε_θ:= {ε_θ(t)}는 T에 관한 함수의 집합이며,
이들 각각은 t로 인덱스 되고, X->X로 가는 함수이고 ※ X는 잠재변수들의 표본공간, 정의역과 치역을 나타낸듯함
학습가능한 매개변수 θ(t)로 되어있다.
γ := [γ_1,...,γ_T]는 목적(함수)에 있는 양의 공분산 벡터들이며 ※ ???
목적(함수)는 α_1:T에 의존합니다.
-> 일단 Loss_gamma(epsilon_theta)는 잡음에 관한 함수인듯하다
입실론이 잡음이며, 세타가 붙은놈은 매개변수로 나온놈(모델이 예측한 잡음), t가붙은놈은 그냥 시간단계를 따르는 잡음인듯
감마는 각 단계의 공분산 함수에 붙은 계수이거나, 아니면 뭔지 모르겠음, 1까지 가는거 보니 맞는것같긴한데
x_0와 t에 따른 수식
즉 예측 잡음과 실제 잡음의 차이의 제곱
즉 MSE에 대한 기댓값을 나타내며,
여기에 감마를 왜 곱하는 것인지는 모르겠다, 일단 여럿 있으니 각 단계의 오차를 모두 합한 값인듯함
-
DDPM논문에서는 γ를 1로 설정할 때, 목적(함수)는 훈련된 모델의 성능을 최대화 화도록 최적화한다.
-> 공분산행렬에 붙는 계수를 1까지 올리란 소리같다
->-> 인제보니 감마는 잡음비 최대값 같은데?
-이건 NCSN에서의 목적 함수와 같으며, 점수 매칭 기반이다.
-
학습된 모델에서, x_0는 최초 샘플링 x_T에서 샘플되며, x_T는 사후확률분포 p_θ(x_T)에 의한 것이고,
x_t-1를 생성 과정에서 반복적으로 샘플링합니다.
-> x_T를 분포 p에서 (아마도 무작위)로 뽑은 다음, x_t-1를 뽑는 과정을 반복해서 x_0를 만들어낸다
->-> 샘플링 과정을 묘사함

정방향 과정에서 T의 길이는 DDPM에서 중요한 하이퍼파라미터이다.
변수적 관점에서는, 큰 T는 역방향 과정을 표준 정규 분포에 가깝게 하도록 합니다.
따라서 가우스 조건부 분포로 모델된 생성 과정은 좋은 근사가 됩니다.
이는 Ho et al.의 논문에서 T=1000과 같은 큰 값을 선택하는데 동기가 되었습니다.
-> 정방향 과정에서 T가 크면 N(0, I)에 가까워 지긴 하더라
그러나 샘플 x_0를 만들기 위해
모든 T반복을 평행적으로 하지 않고 순차적으로 수행해야만 하는 것은
DDPM에서 샘플링하는것이 다른 딥러닝 생성 모델들보다 훨씬 느리게 만들었습니다.
-> 역과정을 T만큼 반복하면 그만큼 느리기야 하겠지
이건 컴퓨팅 자원이 제한되면서 지연시간이 중요한 작업에는 실용적이지 못함을 의미합니다.



---------- 비마르코프 정방향 과정을 위한 변분 추론 ----------

생성 모델이 추론 과정의 역을 근사하므로,
우린 생성 모델의 반복 요구량을 줄이기 위해 추론 과정을 다시 생각해볼 필요가 있습니다.
우리의 핵심 관측은 DDPM 목적함수 L_γ가 q(x_t|x_0)의 주변 분포에만 의존하고 있으며, ※ 이 용어는 x_0에만 조건을 받을 경우 결합 확률에서와 같이 이 용어들을 약간 오용하고 있습니다... 라고 미주에 쓰여있다.
결합 분포 q(x_1:T|x_0)에 직접적으로 의존하지 않는다는 것입니다.
-> DDPM논문의 4번식에서 단계들을 압축한 폼이 있다는것을 보여준 바가 있다
여러 추론 분포들이(결합) 같은 주변 분포로 되어있으며,
우리는 대안적 추론 과정들이 비마르코프라는 사실을 탐구했으며, 이는 새로운 생성 과정으로 이어집니다.
->-> 목적함수 즉 오차함수는 q(x_t|x_0)의 주변 분포에만 의존한다고 한다. 여러 추론 분포가 그러하다고함
이 비마르코비안 추론 과정은 DDPM와 동일한 대체 목적 함수로 이어집니다.
부록 A에서 우리는 비마르코비안 관점이 가우시안 경우를 넘어서도 적용되는 것을 보여줍니다.
->-> 이건 좀 나중에 보자???

3.1 비마르코비안 정방향 과정들
실수 벡터 σ ∈ R^T_≥0로 색인된 추론 분포들 Q로 이루어진 family에 대해 고려해보자.
->0부터 T까지 각 단계에서 분포의 모수들이 시그마이며, 그건 실수 벡터로 이루어져있는 모양이다.
q_σ(x_1:T|x_0) := q_σ(x_T|x_0) * Π_product(t=2, T, q_σ(x_t-1|x_t, x_0)),
where q_σ(x_T|x_0) = N(sqrt(α_T)x_0, sqrt(1-α_T)*I) and all t > 1,
q_σ(x_t-1|x_t, x_0) = N(sqrt(α_t-1)*x_0 + sqrt(1-α_(t-1)-σ_t^2)*(x_t-α_t*x_0)/sqrt(1-α_t), σ_t^2I)
-> 각각 x_0부터 x_T까지의 각 단계별 모든 조건을 충족하는 결합 확률 분포/
위 식에서 각 단계마다 곱해지는 q분포의 비마르코프 조건부 확률에 대한 정의)/
x_t와 x_0를 가지고 보간하여 x_t-1를 만드는 과정을 의미하는 모양이다.
-
이 평균 함수는 q_σ(x_t|x_0) = N(sqrt(α_T)x_0, sqrt(1-α_T)*I)가 모든 t에 대해서 보장되도록 선택되었다. ※ 보조정리 1과 부록 B를 보라
-> 이론적 배경은 나중에 보자???
->-> 저거 예측된 x_t와 α값을 모델에 넣어 예측한 잡음과 x_0에 관한 분포 아니냐?
이는 결합 추론 분포가 바라던 대로 주변 분포에 들어맞도록 정의되었다.
정방향 과정은 베이즈 정리에서 나온다.
q_σ(x_t|x_t-1, x_0) = q_σ(x_t-1|x_t, x_0) * q_σ(x_t|x_0) / q_σ(x_t-1|x_0) ※ 증명은 하단에
그리고 또한 가우스 분포에서 나온다.(우리가 이 사실을 이 논문에 상기용으로 적어놓지 않았더라도)
-> 각 단계가 전부 정규 분포긴 하다
확산 과정의 식 3과 달리, ※ q(x_t|x_0)는 결합확률의 분포이며 이는 다시 정규분포를 따른다는 사실을 적어둠
정방향 과정은 더 이상 마르코프 과정이 아니며, x_t가 x_t-1와 x_0에 의존한다.
->-> 베이즈 정리의 조건 쪽에 조건이 2개있긴하더라
σ의 규모는 정방향 과정이 얼마나 확률적인지를 조절한다.
σ가 0이되면 어떤 t에서 x_t와 x_0가 x_t-1를 알려지고, 고정시킨다는 특수한 경우에 다다른다.
-> 무작위 잡음 추가 안하면 샘플 과정이 고정되기는(결정론적이기는) 할듯

























{
    P(A|B,C)=P(B|A,C)P(A|C)/P(B|C)

    검증
    P(A,B,C) = P(A|B, C) * P(B,C)
    P(A,B,C) = P(B|A,C) * P(A,C) = P(B|A, C) * P(A|C) * P(C)

    끝의 두 식을 연립하면
    P(A|B, C) * P(B,C) = P(B|A, C) * P(A|C) * P(C)
    <->
    P(A|B, C) = ( P(B|A, C) * P(A|C) * P(C) ) / P(B,C)
    = ( P(B|A, C) * P(A|C) * P(C) ) / (P(B|C) * P(C))
    = P(B|A, C) * P(A|C) / P(B|C)
}