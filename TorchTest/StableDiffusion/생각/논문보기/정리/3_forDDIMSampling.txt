https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/ddim/
https://arxiv.org/abs/2010.02502
https://github.com/ermongroup/ddim
순서대로 리뷰/논문/깃허브 구현체

----------서론 (요약) ----------

GAN의 성능이 (여러 모델보다) 괜찮은데,
최적화와 아키텍쳐의 선택(모델구조, 하이퍼파라미터 등등을 의미하는 모양)의 선택이 구체적이여야하고
데이터 분포의 모든 distribution을 다루지 못할 수 있습니다.
-> GANs의 장단점

DDPM이나 NCSN같은 반복적인 생성모델은 GAN없이도 비슷한 수준의 샘플을 생성할 수 있음
이를 위해 오토인코더 여러 단계의 가우시안 잡음으로 손상된 샘플들을 복원하는 모델이 학습됨
백색 잡음에서 점진적으로 잡음을 제거하여 이미지로 변환하는 마르코프 체인에 의해 생성하거나
이미지를 잡음으로 점진적으로 변환하는 순방향 확산 과정을 역전시켜 생성
-> 이건 x_T에서 마르코프체인(확률분포가 다변량 표준 정규 분포에만 의존함)에 의한 여러 식들로 실제로 x_0로 변경된다.
->-> 아래쪽은 정방향을 뒤집는다는 소린데 사후분포를 우도로 바꾼다는 소린가?

이런 모델들의 치명적인 단점은 고퀄리티 이미지를 뽑는데는 너무 많은 반복이 필요하다고 한다.
DDPM이 특히 그런데, 정방향의 역과정을 근사하는걸 데이터가 일일이 다 통과해야하므로 수천의 단계가 필요함
하나 만드는데 단계 반복을 해야하므로 GANs보다 느림

여하튼 이런 DDPM과 GAN사이의 효율차이를 줄이기 위해 고안된게 DDIM이고,
동일한 목적함수(아마 L, 혹은 오차함수 얘기인듯)로 학습된다는 점에서 DDPM과 유사하다고 한다
-> 샘플링만 다르게 할 수 있다고 했었던가?

밑의 사진은 확산 모델과 비마르코프 추론 모델의 개념을 그래프 형태로 시각화 한 것이다

3장에서는 DDPM에서 사용되는 마르코프 성질을 가진 정방향 확산 과정을 비마르코프 확산 과정으로 일반화한다고 한다.
여전히 적절한 역방향 생성 마르코프 체인을 설계할 수 있다.
-> 의외로 생성이 아니라 학습만 비마르코프로 바꾼게 DDIM이다. Gemini피셜, 정확하다고 한다.
->-> Gemini의 추가설명에서는 DDPM이 노이즈 제거 단계에서 이전 단계의 정보만을 활용하는 마르코프 성질을 가지는 반면,
DDIM은 여러 이전 단계의 정보를 활용하여 더욱 정확한 예측을 수행한다고 한다.
이러한 변분 학습 목적함수들이 공통의 대리 목적 함수를 갖는 것을 보여주며, DDPM의 학습의 목적함수와 정확히 일치한다고한다.
-> 뒤에 나오는 내용인 모양
동일한 신경망을 사용해 단순히 다른 비마르코프 확산 과정(4.1절)과 해당하는 역방향생성 마르코프 체인을 선택함으로
다양한 생성 모델 중에서 자유롭게 선택할 수 있습니다.
-> ???
특히 우리는 적은 수의 단계로 시뮬레이션 될수 있는 "짧은" 생성 마르코프 체인을
이끄는 비마르코프 확산 과정을 사용할 수 있습니다.
-> 이거 4.2절에서 나오는 내용이라고 한다.

5장의 내용은 DDIM이 DDPM에 비교해 실험적인 이점을 갖는다는것을 보여주는것
샘플링 속도 가속, 샘플링 일관성, DDIM의 일관성을 사용하여 초기 잠재 변수를 조작하여 의미있는 이미지 보간 가능
-> 빠르고 일관됨

---------- 배경 ----------
주어진 데이터 분포 q(x_0)에서 추출된 샘플을 바탕으로, q(x_0)을 근사하는 동시에 샘플링이 용의한 p_θ(x_0)를
학습하는데 관심이 있다.
아래에는 p의 수식이 있는데, where에서 파이로 각 시간 단계의 조건부 확률을 곱하는 것은 마르코프 체인을 나타내며,
p_theta(x_0:T)는 x_0부터 x_T를 각각의 연속 확률 변수로 뒀을 때 이들의 결합 확률(모두 다 일어날 확률)을 의미하는 것이다
적분이 존재하는건 이게 연속이기 때문에 그렇다.
(Gemini 피셜) x_0가 이전 모든 상태를 고려하여 나올 확률을 계산하는 과정이라고 한다

x_1,...,x_T가 잠재변수(이미지가 어떤 분포에서 나왔다고 가정하는 분표를 따르는 변수)들이면
x_0도 동일하게 그러한 것을 가리킨다.
-> 생성과정 즉 잡음 낀 이미지들이 잠재변수라면, x_0도 그러할 것으로 보인다.
모수 θ는 다음 변분 하한을 최대화하는 것으로 데이터 분포 q(x_0)에 적합하도록 학습된다.
(아래에 있는 식은 DDPM의 음의 로그 가능도 식과 유사하며 부호가 반대로 바뀌었고, 최대화 해야한다)
-> 이 부분에서는 변분 경계, 오차 함수, 목적 함수, 음의 로그 우도 등등 몇 가지 키워드를 가지고 좀 더 알아내보자
->-> 해당 식에서 q(x_1:T|x_0)는 잠재 변수들에 대한 몇몇 추론 분포이다.
VAE같이 다른 특정 잠재 모델들과는 다르게 DDPM은 학습 되지 않는, 즉 고정된 추론 절차
q(x_1:T|x_0)를 사용하며, 잠재 변수의 차원이 상대적으로 높다.
-> 뭐 그렇다고 한다
DDPM 논문에서는 가우시안 전이를 따르는 마르코프 체인을 고려했으며,
가우시안 전이는 감소하는 순서 α_1:T ∈ (0, 1]^T로 매개변수화 되었다
-> 알파 값에 따라 각 단계의 (마르코프 전이를 조절할) 정규 분포가 달라지며, 이는 시간 인덱스 T가 증가함에 따라 줄어든다
q(x_1:T|x_0):= Π_product(t=1, T, q(x_t|x_t-1)),
where q(x_t|x_t-1) := N(sqrt(α_t/α_t-1)x_t-1, (1-(α_t/α_t-1))I)
-> DDPM 논문의 2번식이다, α_t/α_t-1인건 전단계의 조건식 때문에 β가 ~β로 바뀌게 되어서 그런듯하다
->-> 사실 α_t/α_t-1는 1로 간주하긴 하더라

공분산행렬의 대각 성분이 양수인 곳에서 저 식이 유효하다.
이는 자기회귀적 샘플링 절차(x_0에서 x_T까지)때문에 순방향 과정이라고 합니다.
잠재 변수 모델에서 p(x_0:T)는 x_T에서 x_0까지 샘플링하는 마르포크 체인으로, 다루기 어려운 역과정
q(x_t-1|x_t)를 근사하기 때문에 생성과정이라고 합니다.
직관적으로, 순방향 과정은 관측값 x_0에 점진적으로 노이즈를 추가하며,
한편으로 생성 과정은 노이즈 관측값을 점진적으로 제거합니다.

정방향 과정의 특별한 것은 ~ (수식)
-> 재매개변수화 트릭으로 x_0에서 x_t로 바로 가는 정방향 과정을 각 과정의 결합 확률을 적분한 것으로 나타냈으며,
이를 다시한번 정규 분포의 값으로 표현하였다.
그래서 우린 x_t를 x_0와 잡음 변수 ε의 선형 결합으로 표현할 수 있다.
x_t = sqrt(α_t)x_0 + sqrt(1-α_t)ε, where ε ~ N(0, I)

α_T를 0에 충분히 가깝게 설정하면, q(x_T|x_0)는 모든 x_0에서 표준 정규 분포로 수렴합니다.
-> 신호비를 0에 가깝게 가면 x_T가 결국 표준 정규 분포를 따르게 된다는 의미인듯하다.
->-> 평균이 0으로, 공분산의 계수가 1에 가까워질테니 당연한소리
따라서, p_θ(x_T):=N(0,I)로 두는것은 자연스럽습니다.
-> x_T에 대한 근사분포 p를 다변량 표준 정규 분포로 둔다는 소리다.

모든 조건이 평균 함수와 고정된 분산으로 학습된 가우시안으로 만들어진다면,
등식 2의 목적(함수)는 다음과 같이 단순화 될 수 있습니다.
L_γ(ε_θ):= Σ_sum(t=1, T, γ_t*E_x_0, ε_t[||ε_θ(t)(sqrt(α_t)x_0 + sqrt(1-α_t)ε) - ε_t||^2]), where x_0~q(x_0), ε_t~N(0,I)
이 때 ε_θ:= {ε_θ(t)}는 T에 관한 함수의 집합이며,
이들 각각은 t로 인덱스 되고, X->X로 가는 함수이고 ※ X는 잠재변수들의 표본공간, 정의역과 치역을 나타낸듯함
학습가능한 매개변수 θ(t)로 되어있다.
γ := [γ_1,...,γ_T]는 목적(함수)에 있는 양의 공분산 벡터들이며 ※ ???
목적(함수)는 α_1:T에 의존합니다.
-> 일단 Loss_gamma(epsilon_theta)는 잡음에 관한 함수인듯하다
입실론이 잡음이며, 세타가 붙은놈은 매개변수로 나온놈(모델이 예측한 잡음), t가붙은놈은 그냥 시간단계를 따르는 잡음인듯
감마는 각 단계의 공분산 함수에 붙은 계수이거나, 아니면 뭔지 모르겠음, 1까지 가는거 보니 맞는것같긴한데
x_0와 t에 따른 수식
즉 예측 잡음과 실제 잡음의 차이의 제곱
즉 MSE에 대한 기댓값을 나타내며,
여기에 감마를 왜 곱하는 것인지는 모르겠다, 일단 여럿 있으니 각 단계의 오차를 모두 합한 값인듯함
-
DDPM논문에서는 γ를 1로 설정할 때, 목적(함수)는 훈련된 모델의 성능을 최대화 화도록 최적화한다.
-> 공분산행렬에 붙는 계수를 1까지 올리란 소리같다
->-> 인제보니 감마는 잡음비 최대값 같은데?
-이건 NCSN에서의 목적 함수와 같으며, 점수 매칭 기반이다.
-
학습된 모델에서, x_0는 최초 샘플링 x_T에서 샘플되며, x_T는 사후확률분포 p_θ(x_T)에 의한 것이고,
x_t-1를 생성 과정에서 반복적으로 샘플링합니다.
-> x_T를 분포 p에서 (아마도 무작위)로 뽑은 다음, x_t-1를 뽑는 과정을 반복해서 x_0를 만들어낸다
->-> 샘플링 과정을 묘사함