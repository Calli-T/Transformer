일단 classfier_train/sample.py말고 일반 image_train이나 image_sample.py은 IDDPM이나 GUIDED나 코드에 차이가 아예없다
(호출하는 함수나 클래스 세부는 다를 수 있다)


{ # image_train.py
    IDDPM 코드랑 다른점

    1. 매개변수 파싱
    각각의 script_util.py의 model_and_diffusion_defaults()에서 차이가 생겼다. 일일히 서술할 수는 없고 느낌표로 표기
    확산 관련 내용은 diffusion_defaults()라는 새로운 함수로 빠졌음, 함수 내에서 호출해서 갱신

    2. dist_util.py에 분산 처리를 지원하는 것은 똑같으나, 환경 변수(쿠다 장치 수)를 가져오는 한 줄이 추가되었다
    또한 load_state_dict가 상당히 변경되었으나, 나중에 확인 할 내용으로 둔다.

    3. 로그 따기
    뭐 별 차이가 없어보임, 패스(코드가 아예 같을 가능성도 존재함)

    4. 모델과 확산 클래스 생성
    sigma_small과 learn_sigma는 (이미지? 잡음? 어느쪽인지는 모르겠다 아마 이미지의 분포) 분산을 학습할 것인가에 관한 내용인듯하다
    그리고 sigma_small이 create_model_and_diffusion함수의 params에서 사라져있으며, diff 생성함수에는 default값으로 되어있다
    ※ learn_sigma는 예제에서 주로 True로 되어있다, 그러나 default값이 False
    ※※ sigma_small 값은 동작에서 learn_sigma True의 경우에는 그냥 묻힌다

    { # create_model
        channel_mult, num_head_channels, resblock_updown, use_fp16, use_new_attention_order 이 5개의 매개변수는 원래 없던것이다
        그리고 모두 create_model_and_diffusion 모델의 매개변수이다.

        channel_mult: IDDPM에도 있는 것이나 커스텀이 가능해졌다. 아무것도 안주면 기본 값을 주게 설정되었으며 그 내용은 IDDPM의 것과 같다.
                        기본적으로 UNet Model에서 채널 개수 배율(과 단계수)을 올리는데 사용하는 값들이다
        num_head_channels: UNet에서 Attention channel의 개수를 지정할 때 사용하는 매개변수이다. class AttentionBlock에서 사용되며,
                        채널 수 / 해드 당 채널 수로 멀티헤드의 헤드 개수도 지정해준다(헤드수가 -1로 들어올 때만 그러하다).
                        코드에 변경이 있으므로 나중에 다시 확인해 볼 필요가 있다. 아마 헤드 당 채널을 표현할 듯?
        use_new_attention_order: Attention의 legacy와 일반의 구분인듯하다, 주석에는 qkv를 head보다 먼저 split하면 일반을 사용한다
                        추후 내용 확인 필요
        use_fp16: 뭔지 안봐도 뻔하다
        resblock_updown: 주석에, 잔차 블럭을 업/다운 샘플링에 사용하느냐를 적어놨다. 그냥 잔차블럭도 여러군데 사용되서 그냥 사용하는놈도 있고
                        up/down으로 사용되는 놈도 있으니 구분 필요, class ResBlock의 매개변수에 적어놨다. default 값 False
    }

    { # create_guassian_diffusion
        매개변수는 차이가없다. (함수 호출시 sigma_small의 값만 안주는거 말고는 차이가 없다)
        마찬가지로 해당 함수가 건네주는 SpacedDiffusion의 매개변수도 차이가 없다

        그러나 SDiff class가 상속하는 GaussianDiffusion class의 경우 존재한다
        {   # class Gaussian Diffusion

            { # condition_mean
                주석: x에 대한 조건부 로그 가능도의 기울기를 계산하는 조건 함수가 주어진 상태에서 이전 단계의 평균을 구합니다
                특히 조건 함수는 grad(log(p(y|x)))를 계산하고 y에 대한 조건을 지정하려 합니다. -> 이게 뭔 소리지 ???
                이건 솔-딕스타인의 조건 전략을 사용합니다

                내용은 cond_fn에 매개변수로 x와, 스케일된 타임스탭과(실제로는 오버라이딩된 함수가 동작), 모델 인자를 넣고 gradient를 뽑아서,
                mean에 var와 gradient를 곱한것을 더해 새로운 mean을 만들고, 이를 반환한다
                -> 이게 대체 뭐하는 짓이지
                ???

                일단 샘플링에 사용된다. cond_fn가 존재한다면 분포의 평균을 함수에 맞게 살짝 바꾸어 처리한다 p_sample에서 사용
            }
            { # condition_score
                주석: 모델의 점수 함수가 조건 함수에 의해 조건화 되면, p_mean_variance 출력이 무엇인지를 계산합니다. -> ???
                condition_mean 함수와는 달리 이건 Song과 그외 다수의 조건화 전략을 사용합니다.

                alpha_bar를 alphas_cumprod와, 추출용함수등을 사용해 가져온 다음
                ???
                ???
                ???
                ???
                ???
                DDIM에 쓰인다, DDPM에는 안쓰이더라, 나중에 이론적 기반을 다시 살펴보러 갈 때 알아보자
                ddim_sample에서 사용
            }
            전체적으로 호출-> 호출 하는 과정에서 cond_fn이 존재할 경우 샘플링 함수에서 다르게 처리하게 만드는거 말고는 IDDPM과 동일
        }
    }

    5. 데이터로더
    별 차이는 없으나 이미지 증강을 위한 자르기, 뒤집기 옵션이 추가로 생겼고 이에 맞춰 코드가 일부 추가되거나 변경되었다

    6. TrainLoop
    fp16_util의 함수가 MixedPrecisionTrainer로 대거 통합됨, 모델의 패러미터나 저장이나 최적화 함수등 원래 하던 것들의 대부분이
    해당 클래스의 인스턴스 self.mp_trainer로 선언되어 TrainLoop class의 필드로 들어가고, 이를 통해 처리된다
    나중에 코드의 차이점을 자세히 살펴볼 필요가 있겠음 ???
}

{   # classifier_train.py
    주석: (ImageNet의?) 잡음낀 이미지에 대한 분류기를 학습합니다.
    classifier_sample에서 모델을 2개 꺼내쓰는것으로 보아, 확산 과정에서 잡음낀이미지를 분류하는 모델을 따로두어
    해당 모델이 확산 모델을 돕도록 하는 것으로 추측
    이 추측이 맞다면,

    코드 상에서 image_train.py과의 차이는 다음과 같다

    1. 매개변수 파싱
    model_and_diffusion_defaults()를 쓰지 않고 classifier_and_diffusion_defaults()를 사용한다.
    {   # classifier_and_diffusion_defaults()
        일반 매개변수 리셋 함수랑 같은데, classifier_defaults 함수의 내용이 몇몇 추가된다. (분류기 모델 학습용 코드인듯???, 모델도 2개 동시에 사용함)
        매개변수 내용은 여러가지가 있으나 모델 학습할 때 내용을 살펴보고 후술함
    }

    2. 병렬처리와 3. 로그 따기는 별 차이가 없어보인다

    4. 모델과 확산 클래스 선언
    create_model_and_diffusion()를 쓰지않고, create_classifier_and_diffusion() 사용한다
    script_util.py의 create_classifier_and_diffusion부터 다시 시작

    { # create_classifier_and_diffusion
        해당 함수는 분류 모델과 확산 클래스를 반환한다
        매개변수 파싱과정에서 classifier_defaults()으로 가져온 매개변수들이 분류기를 제작하는 create_classifier로 들어가 분류기를 제작한다

        {   # create_classifier
            반쪽짜리 UNet(EncoderUNet)을 반환하는 함수이다

            채널에 곱해지는 수와 그 층의 개수는 이미지 크기에 따라 결정되도록 코드가 짜여있고,
            다운 샘플링 매개변수는 여기에서 split 해서 넣는다

            {   # class EncoderUNetModel
                말그대로 인코딩 기능만 존재하는 UNet이다

                ※ 일반 UNet과 공유하는 특이한 사항이 하나 발견되었는데, resblock_updown이라는 매개변수가 존재한다
                해당 매개변수는 False일 경우에만 추가 다운/업 샘플링을 진행하는데, True의 경우에는 아무일도 일어나지 않는다,
                그러나, ResBlock 자체에 다운/업 샘플링 기능을 내장하고 있고 매개변수 up과 down으로 이를 제어한다.

                일반 UNet의 Input-middle블럭까지 존재한다.
                그러나 끝에 풀링->flatten 하여 out_channels 만큼의 특징 벡터를 뽑아내는 층이 따로 존재하며,
                그 방법도 adaptive, attention, spatial, spatial_v2로 다양하게 존재한다
                이는 일반 UNet과 공유하지 않는 계층이며, 클래스 default 값은 adaptive이다,
                그러나 매개변수 파싱에서의 값으로 덮어지며 이쪽은 기본값이 attention이다.
                즉, 어텐션 메커니즘을 적용한 풀링으로 추측할 수 있다.
            }
        }

        반환은 classifier와 diffusion을 반환한다.
    }


}
