학습의 경우

실행은 image_train.py로 시작함
플래그 3개를 파싱함
MODEL_FLAGS에는 이미지 크기(h, w 같은 숫자인듯), 채널수, 잔차블럭수
DIFFUSION_FLAGS에는 확산 단계수, 확산 스케줄 (코사인 등등)
TRAIN_FLAGS에는 학습률, 배치크기

1.
image_train.py는 시작하자마자
create_argparser()의 반환(argparse.ArgumentParser클래스)의 parse_args()함수를 사용함

create_argparser 함수는 argparse.ArgumentParser 클래스를 반환함
해당 함수내의 이름이 default인 dict와
-> script_util.py의 model_and_diffusion_defaults 함수에서 반환하는 dict를 합쳐 update
-> 매개변수들 key와 기본 value를 가진 dict구성됨
-> argparse.ArgumentParser() 클래스를 선언하고
-> 위 기본값(default dict)를 가지고 parser를 구성
-> 구성된 parser로 실행시 입력한 값 파싱(parse_args(), 해당 반환값은 Namespace 객체임)
-> 이후 반환하여 코드의 각 부분마다 필요한곳 인자만 뽑아서 사용(기본적으로 args에 다들어있음)

요약: 실행 개개변수를 파싱합니다

2. setup_dist()
dist_util.py의 dist가 정확히 뭘 의미하는지는 모르겠으나
distributed는 부가적 코드를 의미한다고 한다.
코드 중에 torch.distributed가 존재함, 이건 병렬 처리에 관한 패키지인듯?
그러고보니 mpi4py 패키지와 관련이 있을 수 있겠다

??? 나중에 알아볼것
https://blog.naver.com/PostView.naver?blogId=sw4r&logNo=222314867436
https://better-tomorrow.tistory.com/entry/Pytorch-Multi-GPU-%EC%A0%95%EB%A6%AC-%EC%A4%91
요약: 병렬 처리용 부가코드

3. logger.configure()/logger.log
로그 따는 기능, 따로 코드로 만들어 놨다
openai-연-월-일-시-분-초-다음엔뭐더라? 폴더에 로그 따놓는게 이 기능인듯
요약: 로그 따는 기능, 안 중요한것 같으니 다음에

4. model, diffusion = create_model_and_diffusion/create_named_schedule_sampler
후술, 가장 중요하게 볼 부분
모델과 샘플러제작

script_util.py의 함수 create_model_and_diffusion은
말그대로 모델과 확산을 뽑는 것으로 보인다.
여기서 확산을 뽑는것의 정확한 실체는 뭔지 나중에 알아보고 model부터 알아보자

4-1-1 내부적으로 create_model 함수를 사용하며 이는 같은 파일에 존재한다.
매개변수로 이미지 크기, 채널수, 잔차블록수
체크포인트 사용, (어텐션) 헤드 수, 드롭아웃은 기존에 아는 정보이나
learn_sigma, class_cond, attention_resolutions
num_heads_upsample, use_scale_shift_norm은 모르는 정보이니 이 매개변수들의 흐름을 따라가볼것
요약: 매개변수는 저런것이 있고 모르는건 따라가보자

함수 코드를 쭉 따라가 보자면,
먼저 채널 사이즈에 맞춰 channel_mult 튜플을 달리 설정한다
256/64/32 사이즈만 인정하며 나머지는 에러로 처리함, 이것도 어디에 쓰이는지 알아보자

attention_ds 리스트는 attention_resolutions 매개변수를 ','로 split 해서 나온다
그렇게 split 한 값을 res라 하며 integer 값으로 바꾼 다음
image_size를 res로 나눠 몫을 구한다, 그리고 그 몫을 attention_ds에 append한다
ds는 추측컨대 downsample인듯 이 매개변수들은 결국 unet.py의 UNetModel 클래스에서 사용되는데,
거기 attention_resolutions(어텐션 해상도)에 해당 리스트를 넣는다
거기 매개변수 설명도 적혀있는데, 4가 리스트에 들어가 있으면 4x downsamling을 진행한다는 뜻
다운 샘플링은 매개변수 크기를 줄이는 모양?
실행 매개변수에 --attention_resolutions 16과 같이 사용할 수 있다!
요약: 어텐션 다운샘플링은 어텐션 해상도 매개변수의 정보를 바탕으로 생성, 여러 값 가능

그걸 기반으로 unet.py의 UNetModel class를 선언하고, 이를 return하는게 create_model함수
결국 'model'이라는건 unet model이다
들어가는 매개변수는 입력채널 (3고정 RGB), 모델채널은 num_channels
잔차블럭수는 그대로, 어텐션 방법은 아까 만든 ds 리스트를 튜플로 변환해서
드롭아웃은 그대로, 체크포인트사용여부도 그대로, 헤드 수도 그대로,
들어가는 매개변수중에 모르거나 좀 더 알아볼 필요가 있는 것은
out_channels, channel_mult, num_classes, num_heads_upsample, use_scale_shift_norm이다
요약: 모델 선언은 unet.py의 UNetModel 클래스에다 매개변수를 끌어다 모아서 가져온다.

4-1-2
unet은 말그대로 unet이며, 해당 파일의 모든 클래스는 UNetModel 클래스로 사용된다
class의 내용을 확인해보자
class TimestepBlock: abc패키지의 @abstractmethod를 사용하는 추상메서드이다.
nn.Module을 상속받고 TimestepEmbedSequential에 상속되는 것으로만 역할끝

class TimestepEmbedSequential: TimestepBlock과 nn.Sequential을 다중상
일종의 단위 구성요소로 사용하는 것으로 추측함
isinstance 메서드로 for layer in self의 각 계층이 TimestepBlock일 경우
계층에 forward할 때 x에 emb를 같이넣어주고, 아닐경우 x = layer(x)임
주석은 다음과 같음 "시간 단계 임베딩을 하위 항목에
전달하는 순차 모듈이며 추가 입력으로 지원됩니다"
emb의 경우 이를 상속하는 하위 클래스들이 구현하는 대로 오버라이딩 되는듯
요약: 임베딩이 추상메서드로 구현된 nn.Sequential임

class Upsample: UNet의 업샘플링 블럭, nn.Module을 상속받는다
매개변수로 채널수, 합성곱 사용여부, 차원(기본값 2)가 있다
__init__에서는 넣어준 매개변수를 모두 필드 변수로 가져오고,
use_conv true의 경우합성곱 층도 하나 만들어서 메서드로 만듬
forward()의 흐름은 다음과 같다 먼저 NCHW의 2번째 값
x.shape[1]을 필드변수 channels와 같도록 강제한다. 그 다음 차원에 맞게 2배크기로 보간한다
합성곱 층을 사용한다고 되어있으면 합성곱에 넣고, 아니면 그냥 반환한다.
요약: 합성곱은 옵션으로 사용하는 업샘플링 계층

class Downsample: UNet의 다운샘플링 블럭, nn.Module을 상속받는다
채널/합성곱 층 사용여부/차원을 매개변수로 사용, 업샘플링이랑 대체로 비슷함
stride는 2/3차원 여부에 따라 2 혹은 (1,2,2)을 사용한다
합성곱 층을 쓰면 매개변수들 그대로 합성곱 층을 사용하고, 아닌 경우
평균 풀링을 차원과 stride에 맞게 진행한다.
채널 강제 여부는 같으나 forward에서 합성곱/풀링 둘 중하나를 써서 반환하는거 말고는 하는거 없음
요약: 합성곱과 풀링 중 택1해서 사용하는 다운샘플링 계층

class ResBlock: 잔차블록인듯, TimestepBlock을 상속받는다
매개변수로 채널수, 임베딩 채널수(타임스탭임배딩 채널), 드롭아웃, 아웃채널, 합성곱 사용 여부
차원(입력 데이터), 체크포인트 사용 여부를 받아온다 초기화 함수에서 아웃채널
합성곱 사용여부, 스케일 시프트 노름, 체크포인트사용, 데이터 차원은 기본값 존재
_forward에서는 입력과 임베딩을 각각 다른 층에 넣고, 길이가 다르다면 임베딩쪽을 반대쪽과 맞춰준다
use_scale_shift_norm가 True일 경우, out_layer를 [0]과(out_norm) 나머지(out_rest)로 나눈다음
임베딩 아웃 값을 torch.chunk하여 scale과 shift로 나눈다.(1차원으로 2동강낸다, emb_layer에서 이미 채널 2배로 출력되게 설계됨)
그런다음 out_norm(= out_layer의 정규화 계층)에 넣는다 ※ .nn으로 된건 torch docs에서 찾아볼 수가없고, nn.py에 직접구현해둔것!!!!!!!! ※※ 구현자체는 nn.GroupNorm을 float32로 구현한것
넣은 값을 스케일링하고, 시프팅 한 다음 out_rest에 넣는다. 그러면 h나옴
use_scale_shift_norm가 False인 경우
입력의 출력과 임베딩 입력의 출력을 더하고 out_layer전체에 집어놓고 h만듬
어떻게 됐건 간에 skip_connection에 x을 넣고 h를 더한다.
in_layers는 정규화->SiLU->합성곱 순서이며
emb_layers는 SiLU->선형층(use_scale_shift_norm True의 경우 out channel의 2배, 아니면 그냥 1배)
out_layers는 정규화->SiLU->dropout->zero_module안에 합성곱넣어서 동작 순서이다
※ zero_module도 nn.py에 구현되어 있다. 모든 파라미터를 0으로 초기화할 때 쓰는 클래스인듯
skip_connection은 out_channels==channels인지, 합성곱을 쓰는지, 등등에 따라 달라지는 계층이다
입출력 채널이 같을 때 skip_connection의 nn.Identity는 그냥 값을 그대로 보내준다는 뜻
아마 잔차 연결할 때 채널이 다른경우 맞춰주는 용도로 알고있다
forward는 _forward를 사용하는데, 인자로 x, emb를 사용하지만 반환에 checkpoint 함수를 사용한다
checkpoint 함수는 nn.py에 구현되어있으며 함수/함수에 들어갈인자/모델의 매개변수(저장용)/플래그를 입력받는다
flag는 use_checkpoint값이며 true의 경우에만 checkpoint 저장을 해주는 모양, true이면
같은 nn.py의 class CheckpointFunction를 사용해 처리하는걸로 추정함
요약: 잔차블럭이다

class AttentionBlock: 트랜스포머 어텐션인듯? nn.Module을 상속받는다
매개변수로 채널수, 헤드개수, 체크포인트 사용여부를 받고 __init__에서 self필드로 전환
__init__에서 추가적으로 nn.py의 normalization사용, 이건 그룹정규화다/https://luvbb.tistory.com/43
self.qkv는 합성곱의 일종인듯하며, 차원은 1차원이고 입력채널의 3배만큼의 출력을 만든다(아마 각각 q, k, v인듯?), kernel_size는 1
이 때 사용되는 conv_nd가 nn.py에 작성되어있으며, 합성곱이긴하나 kernel_size가 1인걸로 보아 사실상 선형 임베딩을 대체하(거나 그 자체가 되)는 모양
self.attention은 class QKVattention의 인스턴스이며 후술
self.proj_out은형식상으로는 입출력 개수가 channel이고, kernel 크기가 1이며 0값으로 초기화되는 1차원 합성곱신경망이나, 뭘 의미하는건지는 알아볼 필요가 있음
forward가 내부적으로 _forward와 checkpoint를 이용하는건 ResBlock과 같음
_forward는 함수 시작과 동시에 x.shape를 b, c, *a로 나눈다(NCHW를 각각 N,C,HW로 나누는듯? *는 unpacking연산자)
x를 (b, c, -1)로 reshape하는 것으로 보아 배치와 채널을 그대로 하고 나머지를 1차원으로 바꾼다 즉 (N,C,H,W) -> (N,C,H*W)
그런다음 x를 그룹정규화 하고 qkv 계층에 넣는것으로 qkv 텐서를 만들어낸다
그런다음 qkv로 'attention'하고 이를 다시 reshape하고, 이를 proj_out에 넣은다음 h를 만들어 x값과 더하고, 이를 다시 원래대로 reshape한다
h.reshape(b, -1, h.shape[-1])은 대체 뭔지 모르겠다???
+0627추가: attention의 결과물은 [N, C, T]이므로 h.shape[-1] = T  = H*W, (b, -1, T)의 경우 결과적으로 (b, c, T)크기 텐서로 변환
추측) proj_out은 transformer encoder block 끝의 ffn의 역할일지도?


class QKVAttention: 모르긴 몰라도 어텐션 메커니즘을 여기서 사용하는듯, nn.Module을 상속받음
qkv가 out_channel이 channel*3이므로, 위에서 HW를 1차원으로 줄였으니 qkv가 [N, (C*3), H*W]의 크기의 텐서이다
q, k, v를 다시 [N, C, H*W] 크기의 텐서로 3등분해준다
scale이란 값이 나오는데 Attention(Q, K, V) = softmax((Q K^T) / sqrt(d_k)) * V에서 sqrt(d_k)를 의미하는 값이다
weight과 einsum은 아인슈타인 표기법이다. A와 B 행렬의 곱을 C라 하자면 Cij = (AB)ij = (from k=1 to n)a_ik * b_kj = a_i1*b_1j + a_i2*b_2j + ⋯ + a_in*b_nj
이는 th.einsum('ij,jk->ik', a,b)으로 간단히 표기할 수 있다. QKV 텐서는 픽셀수 H*W=T와, 채널 C에 대해 [T, C/num_head] 크기의 행렬을 가지고 있다
따라서 Q K^T는 [T, T] 크기의 행렬이다. 한편, 코드로 만들어진 QKV텐서는 각각 [N, C, T] 크기이다
Q, K를 각각 [b, c, t]와 [b, c, s]로 본다면 배치의 행렬은 각각 t행 c열/s행 c열이 될것이고, 전치하니 뒤는 c행 s열이 될것이다, 이를 간단히 하면 'tc,cs->ts', q, k이며,
b개 만큼의 배치이므로 'bct,bcs->bts', q, k이다
요약1: 어텐션 스코어맵을 구하는 과정에서 아인슈타인 표기법을 사용한 텐서곱을 진행하였다
이후 bts를 softmax하고 비슷한 방식으로 V 텐서와 곱하면, 그것이 바로 어텐션이다
요약2: attention(Q, K, V) = softmax((Q K^T) / sqrt(d_k)) * V, 즉 forward함수는 self-attention 메커니즘 그 자체를 충실하게 구현했다
[N, C, T]크기의 어텐션 결과물을 돌려준다
+count_flops는 작업 수를 계산하는 thop 패키지용 카운터, 어텐션 연산에 들어있다...고 주석에 적혀있다.


class UNetModel: nn.Module을 상속함, 잡음 추측 모델
매개변수로는 들어오는 채널수, 모델 채널수, 나가는 채널수, 잔차블럭수, 드롭아웃 비율, 차원, 클래스 개수
체크포인트 사용여부, 헤드수, 스케일_시프트_놈_사용은 뭔지 알겠고
attention resolutions, channel_mult, conv_resample, num_head_upsample은 코드 따라 알아볼것
일단 __init__시작하자마자 num_head_upsample 수부터 정하는데 -1이 default이고 디폴트는 num_head값이랑 똑같이 맞춘다
그 외에 다른 매개변수는 self의 필드로 전환함
time_ewbed_dim은 모델 채널의 4배수로 설정, time_embed 계층은 nn.Sequential로 선형->SiLU->선형층을 거쳐 time_embed_dim 크기의 out을 출력 -> 뭐하는 건지나 알아보자
num_classes가 None이 아닐 경우, self.label_emb이 nn.Embedding으로  num_classes행 time_embed_dim열 크기로 선언됨  ※ nn.Embedding은 lookup table이다 -> 대체 뭐에다 쓰는건지, 룩업테이블은 어떻게 만드는지 알아보자
input_blocks는 nn.ModuleList로 선언되고 선언 될 때는 TimestepEmbedSequential에 합성곱 신경망이 하나 있으며
해당 합성곱 신경망은 in_channel을 받아 model_channel개수의 채널을 출력하며, 커널 크기는 3이다
input_block_chans는 model_channels가 담긴 list로 선언한다, ds는 1로 선언한다
{ //첫 번째 계층의 구성, 코드의 for level, mult...부터 ds *= 2 까지
아래 for문으로 channel_mult에 대해 반복 작업을 하는데
또다시 for로 num_res_block만큼 layers에 잔차블럭을 만들어준다.
이 때 channel_mult의 배율만큼 출력 채널이 늘어난다 ※ 입력채널은 ch 변수로 규정, 앞선 출력 채널이 입력 채널이 되는 방식
※ds는 downsampling 배율로 추정 ds값 (초기 1)이 attention_resolutions안에 존재한다면, layers에 어텐션 블럭을 하나 추가해준다
self.input_blocks에 layers의 모든 계층을 TimestepEmbedSequential에 넣고 append한다
또, input_block_chans에 채널 수(ch)를 append함
요약1: 이중 for문안에서 내부 for문은 잔차블럭의 수만큼 (잔차블럭과 어텐션블럭으로 이루어진)블럭을 만들어 내고, 채널도 기록해둔다
channel_mult의 반복 즉 외부 for문은 내부 for문의 하나와 if문 하나로 되어있다. 내부 for문은 위의 내용
channel_mult는 enumerate로 래핑되어 반복되는데, index에 해당하는 부분은 level로 되어있다.
if문의 조건은 이 level을 사용한다. (level의 값이 channel_mult의 길이보다 1보다 작은 값과 같지않다면)
즉, 마지막 단계가 아니라면 다운샘플링 계층을 만들어 input_blocks에 넣고, input_block_chans에 채널을 기록한다, 추가로 ds를 2배한다
요약2: 여러 매개변수로 unet의 한 부분을 만들어낸다. 변수명은 입력블럭이라고 한다. channel_mult는 채널에 곱해지는 배율이다.
추측: attention_resolutions는 어텐션 해상도를 의미하는 모양, ds가 왜 해당 값에 포함이 될 때만 어텐션 블럭을 추가해주는지도 알아봐야한다
}
{ // 두 번째 계층의 구성, UNet의 밑바닥에 해당한다
요약3: 잔차->어텐션->잔차블럭으로됨, use_scale_shift_norm은 여기서도 사용
}
{ // 세 번째 구성, 첫 번째 구성의 역순, 업스케일링 과정이다
첫 번째와 유사하게 channel_mult를 enumerate로 래핑하여 for로 반복하나, 이번에는 list로바꾸고 [::-1]로 인덱싱하여 역순으로 사용한다
잔차 블럭수 보다 하나 많게 돌아가도록 내부 for문을 구성한다
내부 for문의 내용은, 먼저 layers를 list로 선언하고 내부에 잔차 블럭을 하나 선언한다.
이 때, 잔차 블럭은 기록한 채널수를 pop하여, skip connection을 구성한다
※ 첫 번째 구성과 유사하게 ch값이 후설정되나, 역순이며 첫 값은 첫번째 구성에서 넘어온 값 그대로 사용할 수 있다.
ds값과 attention_resolutions를 사용하여 첫 번째 구성과 유사하게 upsample 계층을 구성한다.
이 때 num_head_upsample을 사용한다 ※ down과 up sampling의 멀티헤드 어텐션의 헤드 수가 다른 경우가 존재하는 모양???, 어차피 어텐션 스코어 맵은 같게 나올테니
첫 번째 level이 아니고(첫 번째 구성에서는 마지막 level을 배제했다, 역순이니 반대로), i가 num_res_block과 같은 경우(= 내부 반복문의 마지막 반복의 경우)
업샘플링 레이어를 추가해준다. 추가하고 ds 값도 절반으로 줄어듬
layers를 만들면 그걸로 output_blocks에 TimestepEmbedSequential 계층을 append한다
요약4: UNet의 상승 구간이다. 어째선지 각 레벨에 계층이 한 개 더 많다
의문점: 대체 왜 num_res_block보다 1회 더 반복을 많이하는가??? ch를 제대로 pop할 수 있는가?
input_blocks_chans는 선언할 때 1개의 값을 가진 list다. 첫 번째 구성의 내부 for문에서 input_blocks_chans를 num_res_blocks 만큼 추가하고,
마지막 level을 제외한 모든 경우에서 ch를 하나씩 더하여 len(channel_mult) * (num_res_block + 1) 만큼의 input_blocks_chans list를 가지게된다.
세번째 구성에서는 외부 for 내에 내부 for 말고 아무것도 없고, 내부 for가 num_res_block + 1만큼 돌아가니 결국 len(channel_mult) * (num_res_block + 1)만큼 pop한다.
정확하게 일치함 -> 해결완료, 어디가 어디에서 연결 되는지나 알아보자
}
{ // 네 번째 구성, self.out = nn .Sequential부터
요약5: 그룹 정규화 -> SiLU -> Conv(영으로 초기화됨, model_channel만큼의 입력채널, out_channels만큼의 출력채널, 커널 3)
}
밑에 convert함수 2종류는 아마도 부동소수점 자리수선택에 관한 문제인듯
@property는 데코함수, https://www.daleseo.com/python-property/
내용은 input_blocks의 params들의 데이터 타입을 알려주는듯, 이는 forward()에서 사용된다
{
forward의 세 매개 변수 x, timesteps, y는
각각 입력텐서, 타임스텝의 1차원 배치(아마도, 타임스텝이 배치 개수만큼있는듯), 레이블(배치 N개만큼 존재, 클래스 조건이 있다면)이다
출력텐서와 입력 텐서는 주석을 보면 모양이 같은데, 아마 입력의 차원에 따라 [N, C, ...]에서 뒷 부분이 달라지는듯함
assert문은 클래스 개수가 존재할 때만 배치의 레이블에 해당하는 y값은 줄것을 강제하고있다
emb는 임베딩이다. 확산 스케줄과 라벨 정보(클래스 정보가 존재할 경우)를 담고있다.
timestep embedding은 nn.py에 구현되어있는데, 주석에 정현파 타임스텝 임베딩이라고 적혀있다. 이는 스칼라 값(잡음의 분산)에 대한 연속적인 값을 가진벡터로의 임베딩이다.
정현파(=사인파, 코사인파) 임베딩은 다음과 같이 인코딩 된다. r(x) = (sin(2πe^0fx), ⋯ , sin(2πe^(L-1)fx), cos(2πe^0fx), ⋯ , cos(2πe^(L-1)fx))
이 때, 잡음 길이의 절반이 L이며, 코드에 적혀진 주파수는 f = -ln(10000)/(dim-1) 이다
※ arange 함수의 경우 start,end,step을 매개로 받으며 [start,end)에서 (end-start)/step이 1단계마다 올라가는 값의 크기이다
※※ 임베딩 차원의 길이가 홀수인 경우 추가 공정있음
timestep_embedding에서 [배치수, 모델 채널수]의 임베딩을 반환받으면
이를 (__init__에서 선언된)self.time_embed 계층에 넣는다 [배치수, 모델 채널수 *4]의 emb가 된다
만약, 클래스(레이블)이 주어진 경우라면 self.label_emb의 룩업 계층을 통하여 위의 emb와 더해준다
요약1: 정현파 임베딩을 만들어주고, 필요한 경우 클래스 임베딩을 더해준다
self.inner_dtype과 input_blocks을 사용하여 입력 x를 h로 캐스팅한다
※ 자세한 설명은 공식 문서 https://pytorch.org/docs/stable/generated/torch.Tensor.type.html 에
계층에 h과 emb을 같이 넣어서 h로 반환하며, skip connection을 위해 h를 hs의 list에 append해둔다->UNet의 하강구간
h를 다시 middle block에 emb와 넣고 결과값 h를 뽑아낸다 ->UNet의 아랫구간
output_block의 각 계층을 하나씩 꺼내오며, hs의 값도 하나씩 pop하며 concat하여 skip_connection을 구현한다, cat함수의 차원은 1(NCHW에서  C를 의미)
※input_blocks와 output_blocks는 모두 ModuleList
concat과 emb를 입력으로 하여 output_block의 계층에 넣고 h값을 뽑아낸다
h를 다시금 x.dtype으로 뽑아내고,
self.out계층을 마지막으로 h를 넣고 그 값을 return으로 반환하면 UNetModel의 forward 함수가 완성된다
}
번외1: self.get_feature_vectors 함수는 forward와 비슷하긴 한데, down/middle/up의 결과를 list에 모아서 반환한다
번외2: class SuperResModel <- 얘는 뭐하는 건지는 나중에 알아보자, 아마도 화질 개선인듯
※ script_util의 create_model말고 sr_create_model이 super resolution 관련 모델 선언인듯?

4-2
create_gaussian_diffusion은 script_util.py의 create_model_and_diffusion함수에서 create_model과 같이 사용하는 함수이다
※ 매개변수도 같은 방식으로 조달한다 매개변수의 시작에 빈 asterisk가 존재하는데, 이는 bare asterisk라는 방식으로 asterisk 뒤의 매개변수는
기명으로만 접근가능하게하는, 개발자 편의성을 위한 문법적요소이다. https://this-programmer.tistory.com/503
매개변수들은 후술
여기서 필요한 함수나 클래스는 대부분 gaussian_diffusion.py의 폴더에서 가져온다
betas는 gd에서 noise_schedule과, steps를 주고 β 즉 잡음비(= 분산)를 가져오는것이다. (신호비는 재매개변수화트릭으로, cumprod로 만든다)
아래의 if-elif-else 구문은 매개변수 use_kl, rescale_learned_sigmas를 가지고 LossType class를 설정하는것
-> 변분 하한등의 수학적 이론과, rescale_leanned_sigmas가 대체 뭔지나 알아보자, 내용 자체는 오차 함수인것같다
{ // timestep을 가지고 재처리를 하는모양, timestep 매개변수가 None이라면 [steps]를 사용한다
timestep_respacing 매개변수는 steps와 함께 respace.py에 space_timesteps함수의 매개변수로 들어가게된다
해당 함수의 주석에서
원래 확산 과정에서 사용되던 timesteps의 list를 만듭니다. 원래 확산 과정에서 동일한 크기의 부분에서 사용할 시간 단계수가 저장됩니다.
예를들어, 300회의시간 단계가 있고 section 수가 [10, 15, 20]이면 처음 100회는 10회, 두 번째 100회 스텝은 15회, 세번 째는 20회
스텝이 됩니다. stride가 ddim으로 시작하는 문자열일 경우, DDIM논문에서의 고정 striding이 사용되고 단 하나의 섹션만이 혀용됩니다.
-> section_counts가 list일 경우, timesteps를 len(section_counts)로 나눈 다음 각각의 num_timesteps를 list의 값으로 대체하고
"ddim"으로 시작하는 문자열은 뭔가 특별한 작업을 해준다. 이게 정확하게 뭘 의미하는건???지는모르겠다 -> IDDPM 논문을 다시 읽어보자, 이게 대체 뭔...???
section_counts가 string일경우 다음의 코드가 실해오딘다. desired_count는 "ddim"뒤가 몇 글자인지 세어주는 변수이다.
1부터 timesteps-1까지를 for문으로 순회하며 if문을 하나 돌린다. 해당 if문의 내용은 이러하다.
만약 0부터 num_timesteps까지를 i씩 순회(range(0, num_timesteps, i))할 때의 길이가 desired_count라면, 해당 range로 만든 set을 반환한다.
※ 정수로 안나오면 raise로 오류처리함
??? 이상적인 횟수를 정해놓고, 거기 맞춰서 set을 반환하는모양, 이건 SpacedDiffusion(GaussianDiffusion을 상속함)에서 사용되는데, 그 때 알아보자
만약 section_count문자이나 ddim으로 시작하지 않고 콤마로 구분된 문자열이라면, 이를 잘라 int list로 만들어준다
어쨌가어나 section_counts로 정수 list가 됐다면 len으로 timesteps를 자르고(몫과 나머지는 각기 다른 변수명이다)
list 내부의 값으로 한 번에 몇 단계씩 건너뛸지 (값이 10이면 0, 10, 20...)를 index를 쭉 지정해주고, 이걸 한 list에 담아서 반환한다
요약: 확산 단계를 재가공하는 함수이다. 0부터 최종단계까지 몇 단계식 건너뛸지에 관한 내용이 반환된다.
}
use_timesteps, 아까 만든 betas(= 잡음비)가 첫 두 매개변수이며 model_mean_type이 다음 매개변수인데
이는 create_gaussian_diffusion의 predict_xstart의 값에 따라 EPSILON과 START_X 둘로 나뉜다.(해당 클래스 자체는 gd에 존재)



+ model.to(dist_util.dev()) 이 코드는 디바이스 선택과 MPI 설정 등 작업

5. TrainLoop