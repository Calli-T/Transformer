학습의 경우

실행은 image_train.py로 시작함
플래그 3개를 파싱함
MODEL_FLAGS에는 이미지 크기(h, w 같은 숫자인듯), 채널수, 잔차블럭수
DIFFUSION_FLAGS에는 확산 단계수, 확산 스케줄 (코사인 등등)
TRAIN_FLAGS에는 학습률, 배치크기

1.
image_train.py는 시작하자마자
create_argparser()의 반환(argparse.ArgumentParser클래스)의 parse_args()함수를 사용함

create_argparser 함수는 argparse.ArgumentParser 클래스를 반환함
해당 함수내의 이름이 default인 dict와
-> script_util.py의 model_and_diffusion_defaults 함수에서 반환하는 dict를 합쳐 update
-> 매개변수들 key와 기본 value를 가진 dict구성됨
-> argparse.ArgumentParser() 클래스를 선언하고
-> 위 기본값(default dict)를 가지고 parser를 구성
-> 구성된 parser로 실행시 입력한 값 파싱(parse_args(), 해당 반환값은 Namespace 객체임)
-> 이후 반환하여 코드의 각 부분마다 필요한곳 인자만 뽑아서 사용(기본적으로 args에 다들어있음)

요약: 실행 개개변수를 파싱합니다

2. setup_dist()
dist_util.py의 dist가 정확히 뭘 의미하는지는 모르겠으나
distributed는 부가적 코드를 의미한다고 한다.
코드 중에 torch.distributed가 존재함, 이건 병렬 처리에 관한 패키지인듯?
그러고보니 mpi4py 패키지와 관련이 있을 수 있겠다

??? 나중에 알아볼것
https://blog.naver.com/PostView.naver?blogId=sw4r&logNo=222314867436
https://better-tomorrow.tistory.com/entry/Pytorch-Multi-GPU-%EC%A0%95%EB%A6%AC-%EC%A4%91
요약: 병렬 처리용 부가코드

3. logger.configure()/logger.log
로그 따는 기능, 따로 코드로 만들어 놨다
openai-연-월-일-시-분-초-다음엔뭐더라? 폴더에 로그 따놓는게 이 기능인듯
요약: 로그 따는 기능, 안 중요한것 같으니 다음에

4. model, diffusion = create_model_and_diffusion/create_named_schedule_sampler
후술, 가장 중요하게 볼 부분
모델과 샘플러제작

script_util.py의 함수 create_model_and_diffusion은
말그대로 모델과 확산을 뽑는 것으로 보인다.
여기서 확산을 뽑는것의 정확한 실체는 뭔지 나중에 알아보고 model부터 알아보자

4-1-1 내부적으로 create_model 함수를 사용하며 이는 같은 파일에 존재한다.
매개변수로 이미지 크기, 채널수, 잔차블록수
체크포인트 사용, (어텐션) 헤드 수, 드롭아웃은 기존에 아는 정보이나
learn_sigma, class_cond, attention_resolutions
num_heads_upsample, use_scale_shift_norm은 모르는 정보이니 이 매개변수들의 흐름을 따라가볼것
요약: 매개변수는 저런것이 있고 모르는건 따라가보자

함수 코드를 쭉 따라가 보자면,
먼저 채널 사이즈에 맞춰 channel_mult 튜플을 달리 설정한다
256/64/32 사이즈만 인정하며 나머지는 에러로 처리함, 이것도 어디에 쓰이는지 알아보자

attention_ds 리스트는 attention_resolutions 매개변수를 ','로 split 해서 나온다
그렇게 split 한 값을 res라 하며 integer 값으로 바꾼 다음
image_size를 res로 나눠 몫을 구한다, 그리고 그 몫을 attention_ds에 append한다
ds는 추측컨대 downsample인듯 이 매개변수들은 결국 unet.py의 UNetModel 클래스에서 사용되는데,
거기 attention_resolutions(어텐션 해상도)에 해당 리스트를 넣는다
거기 매개변수 설명도 적혀있는데, 4가 리스트에 들어가 있으면 4x downsamling을 진행한다는 뜻
다운 샘플링은 매개변수 크기를 줄이는 모양?
실행 매개변수에 --attention_resolutions 16과 같이 사용할 수 있다!
요약: 어텐션 다운샘플링은 어텐션 해상도 매개변수의 정보를 바탕으로 생성, 여러 값 가능

그걸 기반으로 unet.py의 UNetModel class를 선언하고, 이를 return하는게 create_model함수
결국 'model'이라는건 unet model이다
들어가는 매개변수는 입력채널 (3고정 RGB), 모델채널은 num_channels
잔차블럭수는 그대로, 어텐션 방법은 아까 만든 ds 리스트를 튜플로 변환해서
드롭아웃은 그대로, 체크포인트사용여부도 그대로, 헤드 수도 그대로,
들어가는 매개변수중에 모르거나 좀 더 알아볼 필요가 있는 것은
out_channels, channel_mult, num_classes, num_heads_upsample, use_scale_shift_norm이다
요약: 모델 선언은 unet.py의 UNetModel 클래스에다 매개변수를 끌어다 모아서 가져온다.

4-1-2
unet은 말그대로 unet이며, 해당 파일의 모든 클래스는 UNetModel 클래스로 사용된다
class의 내용을 확인해보자
class TimestepBlock: abc패키지의 @abstractmethod를 사용하는 추상메서드이다.
nn.Module을 상속받고 TimestepEmbedSequential에 상속되는 것으로만 역할끝

class TimestepEmbedSequential: TimestepBlock과 nn.Sequential을 다중상
일종의 단위 구성요소로 사용하는 것으로 추측함
isinstance 메서드로 for layer in self의 각 계층이 TimestepBlock일 경우
계층에 forward할 때 x에 emb를 같이넣어주고, 아닐경우 x = layer(x)임
주석은 다음과 같음 "시간 단계 임베딩을 하위 항목에
전달하는 순차 모듈이며 추가 입력으로 지원됩니다"
emb의 경우 이를 상속하는 하위 클래스들이 구현하는 대로 오버라이딩 되는듯
요약: 임베딩이 추상메서드로 구현된 nn.Sequential임

class Upsample: UNet의 업샘플링 블럭, nn.Module을 상속받는다
매개변수로 채널수, 합성곱 사용여부, 차원(기본값 2)가 있다
__init__에서는 넣어준 매개변수를 모두 필드 변수로 가져오고,
use_conv true의 경우합성곱 층도 하나 만들어서 메서드로 만듬
forward()의 흐름은 다음과 같다 먼저 NCHW의 2번째 값
x.shape[1]을 필드변수 channels와 같도록 강제한다. 그 다음 차원에 맞게 2배크기로 보간한다
합성곱 층을 사용한다고 되어있으면 합성곱에 넣고, 아니면 그냥 반환한다.
요약: 합성곱은 옵션으로 사용하는 업샘플링 계층

class Downsample: UNet의 다운샘플링 블럭, nn.Module을 상속받는다
채널/합성곱 층 사용여부/차원을 매개변수로 사용, 업샘플링이랑 대체로 비슷함
stride는 2/3차원 여부에 따라 2 혹은 (1,2,2)을 사용한다
합성곱 층을 쓰면 매개변수들 그대로 합성곱 층을 사용하고, 아닌 경우
평균 풀링을 차원과 stride에 맞게 진행한다.
채널 강제 여부는 같으나 forward에서 합성곱/풀링 둘 중하나를 써서 반환하는거 말고는 하는거 없음
요약: 합성곱과 풀링 중 택1해서 사용하는 다운샘플링 계층

class ResBlock: 잔차블록인듯, TimestepBlock을 상속받는다
매개변수로 채널수, 임베딩 채널수(타임스탭임배딩 채널), 드롭아웃, 아웃채널, 합성곱 사용 여부
차원(입력 데이터), 체크포인트 사용 여부를 받아온다 초기화 함수에서 아웃채널
합성곱 사용여부, 스케일 시프트 노름, 체크포인트사용, 데이터 차원은 기본값 존재
_forward에서는 입력과 임베딩을 각각 다른 층에 넣고, 길이가 다르다면 임베딩쪽을 반대쪽과 맞춰준다
use_scale_shift_norm가 True일 경우, out_layer를 [0]과(out_norm) 나머지(out_rest)로 나눈다음
임베딩 아웃 값을 torch.chunk하여 scale과 shift로 나눈다.(1차원으로 2동강낸다, emb_layer에서 이미 채널 2배로 출력되게 설계됨)
그런다음 out_norm(= out_layer의 정규화 계층)에 넣는다 ※ .nn으로 된건 torch docs에서 찾아볼 수가없고, nn.py에 직접구현해둔것!!!!!!!! ※※ 구현자체는 nn.GroupNorm을 float32로 구현한것
넣은 값을 스케일링하고, 시프팅 한 다음 out_rest에 넣는다. 그러면 h나옴
use_scale_shift_norm가 False인 경우
입력의 출력과 임베딩 입력의 출력을 더하고 out_layer전체에 집어놓고 h만듬
어떻게 됐건 간에 skip_connection에 x을 넣고 h를 더한다.
in_layers는 정규화->SiLU->합성곱 순서이며
emb_layers는 SiLU->선형층(use_scale_shift_norm True의 경우 out channel의 2배, 아니면 그냥 1배)
out_layers는 정규화->SiLU->dropout->zero_module안에 합성곱넣어서 동작 순서이다
※ zero_module도 nn.py에 구현되어 있다. 모든 파라미터를 0으로 초기화할 때 쓰는 클래스인듯
skip_connection은 out_channels==channels인지, 합성곱을 쓰는지, 등등에 따라 달라지는 계층이다
입출력 채널이 같을 때 skip_connection의 nn.Identity는 그냥 값을 그대로 보내준다는 뜻
아마 잔차 연결할 때 채널이 다른경우 맞춰주는 용도로 알고있다
forward는 _forward를 사용하는데, 인자로 x, emb를 사용하지만 반환에 checkpoint 함수를 사용한다
checkpoint 함수는 nn.py에 구현되어있으며 함수/함수에 들어갈인자/모델의 매개변수(저장용)/플래그를 입력받는다
flag는 use_checkpoint값이며 true의 경우에만 checkpoint 저장을 해주는 모양, true이면
같은 nn.py의 class CheckpointFunction를 사용해 처리하는걸로 추정함
요약: 잔차블럭이다

class AttentionBlock: 트랜스포머 어텐션인듯? nn.Module을 상속받는다
매개변수로 채널수, 헤드개수, 체크포인트 사용여부를 받고 __init__에서 self필드로 전환
__init__에서 추가적으로 nn.py의 normalization사용, 이건 그룹정규화다/https://luvbb.tistory.com/43
self.qkv는 합성곱의 일종인듯하며, 차원은 1차원이고 입력채널의 3배만큼의 출력을 만든다(아마 각각 q, k, v인듯?), kernel_size는 1
이 때 사용되는 conv_nd가 nn.py에 작성되어있으며, 합성곱이긴하나 kernel_size가 1인걸로 보아 사실상 선형 임베딩을 대체하(거나 그 자체가 되)는 모양
self.attention은 class QKVattention의 인스턴스이며 후술
self.proj_out은형식상으로는 입출력 개수가 channel이고, kernel 크기가 1이며 0값으로 초기화되는 1차원 합성곱신경망이나, 뭘 의미하는건지는 알아볼 필요가 있음
forward가 내부적으로 _forward와 checkpoint를 이용하는건 ResBlock과 같음
_forward는 함수 시작과 동시에 x.shape를 b, c, *a로 나눈다(NCHW를 각각 N,C,HW로 나누는듯? *는 unpacking연산자)
x를 (b, c, -1)로 reshape하는 것으로 보아 배치와 채널을 그대로 하고 나머지를 1차원으로 바꾼다 즉 (N,C,H,W) -> (N,C,H*W)
그런다음 x를 그룹정규화 하고 qkv 계층에 넣는것으로 qkv 텐서를 만들어낸다
그런다음 qkv로 'attention'하고 이를 다시 reshape하고, 이를 proj_out에 넣은다음 h를 만들어 x값과 더하고, 이를 다시 원래대로 reshape한다
h.reshape(b, -1, h.shape[-1])은 대체 뭔지 모르겠다???
+0627추가: attention의 결과물은 [N, C, T]이므로 h.shape[-1] = T  = H*W, (b, -1, T)의 경우 결과적으로 (b, c, T)크기 텐서로 변환
추측) proj_out은 transformer encoder block 끝의 ffn의 역할일지도?


class QKVAttention: 모르긴 몰라도 어텐션 메커니즘을 여기서 사용하는듯, nn.Module을 상속받음
qkv가 out_channel이 channel*3이므로, 위에서 HW를 1차원으로 줄였으니 qkv가 [N, (C*3), H*W]의 크기의 텐서이다
q, k, v를 다시 [N, C, H*W] 크기의 텐서로 3등분해준다
scale이란 값이 나오는데 Attention(Q, K, V) = softmax((Q K^T) / sqrt(d_k)) * V에서 sqrt(d_k)를 의미하는 값이다
weight과 einsum은 아인슈타인 표기법이다. A와 B 행렬의 곱을 C라 하자면 Cij = (AB)ij = (from k=1 to n)a_ik * b_kj = a_i1*b_1j + a_i2*b_2j + ⋯ + a_in*b_nj
이는 th.einsum('ij,jk->ik', a,b)으로 간단히 표기할 수 있다. QKV 텐서는 픽셀수 H*W=T와, 채널 C에 대해 [T, C/num_head] 크기의 행렬을 가지고 있다
따라서 Q K^T는 [T, T] 크기의 행렬이다. 한편, 코드로 만들어진 QKV텐서는 각각 [N, C, T] 크기이다
Q, K를 각각 [b, c, t]와 [b, c, s]로 본다면 배치의 행렬은 각각 t행 c열/s행 c열이 될것이고, 전치하니 뒤는 c행 s열이 될것이다, 이를 간단히 하면 'tc,cs->ts', q, k이며,
b개 만큼의 배치이므로 'bct,bcs->bts', q, k이다
요약1: 어텐션 스코어맵을 구하는 과정에서 아인슈타인 표기법을 사용한 텐서곱을 진행하였다
이후 bts를 softmax하고 비슷한 방식으로 V 텐서와 곱하면, 그것이 바로 어텐션이다
요약2: attention(Q, K, V) = softmax((Q K^T) / sqrt(d_k)) * V, 즉 forward함수는 self-attention 메커니즘 그 자체를 충실하게 구현했다
[N, C, T]크기의 어텐션 결과물을 돌려준다
+count_flops는 작업 수를 계산하는 thop 패키지용 카운터, 어텐션 연산에 들어있다...고 주석에 적혀있다.


class UNetModel: nn.Module을 상속함, 잡음 추측 모델
매개변수로는 들어오는 채널수, 모델 채널수, 나가는 채널수, 잔차블럭수, 드롭아웃 비율, 차원, 클래스 개수
체크포인트 사용여부, 헤드수, 스케일_시프트_놈_사용은 뭔지 알겠고
attention resolutions, channel_mult, conv_resample, num_head_upsample은 코드 따라 알아볼것
일단 __init__시작하자마자 num_head_upsample 수부터 정하는데 -1이 default이고 디폴트는 num_head값이랑 똑같이 맞춘다
그 외에 다른 매개변수는 self의 필드로 전환함
time_ewbed_dim은 모델 채널의 4배수로 설정, time_embed 계층은 nn.Sequential로 선형->SiLU->선형층을 거쳐 time_embed_dim 크기의 out을 출력 -> 뭐하는 건지나 알아보자
num_classes가 None이 아닐 경우, self.label_emb이 nn.Embedding으로  num_classes행 time_embed_dim열 크기로 선언됨  ※ nn.Embedding은 lookup table이다 -> 대체 뭐에다 쓰는건지, 룩업테이블은 어떻게 만드는지 알아보자
input_blocks는 nn.ModuleList로 선언되고 선언 될 때는 TimestepEmbedSequential에 합성곱 신경망이 하나 있으며
해당 합성곱 신경망은 in_channel을 받아 model_channel개수의 채널을 출력하며, 커널 크기는 3이다
input_block_chans는 model_channels가 담긴 list로 선언한다, ds는 1로 선언한다
{ //첫 번째 계층의 구성, 코드의 for level, mult...부터 ds *= 2 까지
아래 for문으로 channel_mult에 대해 반복 작업을 하는데
또다시 for로 num_res_block만큼 layers에 잔차블럭을 만들어준다.
이 때 channel_mult의 배율만큼 출력 채널이 늘어난다 ※ 입력채널은 ch 변수로 규정, 앞선 출력 채널이 입력 채널이 되는 방식
※ds는 downsampling 배율로 추정 ds값 (초기 1)이 attention_resolutions안에 존재한다면, layers에 어텐션 블럭을 하나 추가해준다
self.input_blocks에 layers의 모든 계층을 TimestepEmbedSequential에 넣고 append한다
또, input_block_chans에 채널 수(ch)를 append함
요약1: 이중 for문안에서 내부 for문은 잔차블럭의 수만큼 (잔차블럭과 어텐션블럭으로 이루어진)블럭을 만들어 내고, 채널도 기록해둔다
channel_mult의 반복 즉 외부 for문은 내부 for문의 하나와 if문 하나로 되어있다. 내부 for문은 위의 내용
channel_mult는 enumerate로 래핑되어 반복되는데, index에 해당하는 부분은 level로 되어있다.
if문의 조건은 이 level을 사용한다. (level의 값이 channel_mult의 길이보다 1보다 작은 값과 같지않다면)
즉, 마지막 단계가 아니라면 다운샘플링 계층을 만들어 input_blocks에 넣고, input_block_chans에 채널을 기록한다, 추가로 ds를 2배한다
요약2: 여러 매개변수로 unet의 한 부분을 만들어낸다. 변수명은 입력블럭이라고 한다. channel_mult는 채널에 곱해지는 배율이다.
추측: attention_resolutions는 어텐션 해상도를 의미하는 모양, ds가 왜 해당 값에 포함이 될 때만 어텐션 블럭을 추가해주는지도 알아봐야한다
}
{ // 두 번째 계층의 구성, UNet의 밑바닥에 해당한다
요약3: 잔차->어텐션->잔차블럭으로됨, use_scale_shift_norm은 여기서도 사용
}
{ // 세 번째 구성, 첫 번째 구성의 역순, 업스케일링 과정이다
첫 번째와 유사하게 channel_mult를 enumerate로 래핑하여 for로 반복하나, 이번에는 list로바꾸고 [::-1]로 인덱싱하여 역순으로 사용한다
잔차 블럭수 보다 하나 많게 돌아가도록 내부 for문을 구성한다
내부 for문의 내용은, 먼저 layers를 list로 선언하고 내부에 잔차 블럭을 하나 선언한다.
이 때, 잔차 블럭은 기록한 채널수를 pop하여, skip connection을 구성한다
※ 첫 번째 구성과 유사하게 ch값이 후설정되나, 역순이며 첫 값은 첫번째 구성에서 넘어온 값 그대로 사용할 수 있다.
ds값과 attention_resolutions를 사용하여 첫 번째 구성과 유사하게 upsample 계층을 구성한다.
이 때 num_head_upsample을 사용한다 ※ down과 up sampling의 멀티헤드 어텐션의 헤드 수가 다른 경우가 존재하는 모양???, 어차피 어텐션 스코어 맵은 같게 나올테니
첫 번째 level이 아니고(첫 번째 구성에서는 마지막 level을 배제했다, 역순이니 반대로), i가 num_res_block과 같은 경우(= 내부 반복문의 마지막 반복의 경우)
업샘플링 레이어를 추가해준다. 추가하고 ds 값도 절반으로 줄어듬
layers를 만들면 그걸로 output_blocks에 TimestepEmbedSequential 계층을 append한다
요약4: UNet의 상승 구간이다. 어째선지 각 레벨에 계층이 한 개 더 많다
의문점: 대체 왜 num_res_block보다 1회 더 반복을 많이하는가??? ch를 제대로 pop할 수 있는가?
input_blocks_chans는 선언할 때 1개의 값을 가진 list다. 첫 번째 구성의 내부 for문에서 input_blocks_chans를 num_res_blocks 만큼 추가하고,
마지막 level을 제외한 모든 경우에서 ch를 하나씩 더하여 len(channel_mult) * (num_res_block + 1) 만큼의 input_blocks_chans list를 가지게된다.
세번째 구성에서는 외부 for 내에 내부 for 말고 아무것도 없고, 내부 for가 num_res_block + 1만큼 돌아가니 결국 len(channel_mult) * (num_res_block + 1)만큼 pop한다.
정확하게 일치함 -> 해결완료, 어디가 어디에서 연결 되는지나 알아보자
}
{ // 네 번째 구성, self.out = nn .Sequential부터
요약5: 그룹 정규화 -> SiLU -> Conv(영으로 초기화됨, model_channel만큼의 입력채널, out_channels만큼의 출력채널, 커널 3)
}
밑에 convert함수 2종류는 아마도 부동소수점 자리수선택에 관한 문제인듯
@property는 데코함수, https://www.daleseo.com/python-property/
내용은 input_blocks의 params들의 데이터 타입을 알려주는듯, 이는 forward()에서 사용된다
{
forward의 세 매개 변수 x, timesteps, y는
각각 입력텐서, 타임스텝의 1차원 배치(아마도, 타임스텝이 배치 개수만큼있는듯), 레이블(배치 N개만큼 존재, 클래스 조건이 있다면)이다
출력텐서와 입력 텐서는 주석을 보면 모양이 같은데, 아마 입력의 차원에 따라 [N, C, ...]에서 뒷 부분이 달라지는듯함
assert문은 클래스 개수가 존재할 때만 배치의 레이블에 해당하는 y값은 줄것을 강제하고있다
emb는 임베딩이다. 확산 스케줄과 라벨 정보(클래스 정보가 존재할 경우)를 담고있다.
timestep embedding은 nn.py에 구현되어있는데, 주석에 정현파 타임스텝 임베딩이라고 적혀있다. 이는 스칼라 값(잡음의 분산)에 대한 연속적인 값을 가진벡터로의 임베딩이다.
정현파(=사인파, 코사인파) 임베딩은 다음과 같이 인코딩 된다. r(x) = (sin(2πe^0fx), ⋯ , sin(2πe^(L-1)fx), cos(2πe^0fx), ⋯ , cos(2πe^(L-1)fx))
이 때, 잡음 길이의 절반이 L이며, 코드에 적혀진 주파수는 f = -ln(10000)/(dim-1) 이다
※ arange 함수의 경우 start,end,step을 매개로 받으며 [start,end)에서 (end-start)/step이 1단계마다 올라가는 값의 크기이다
※※ 임베딩 차원의 길이가 홀수인 경우 추가 공정있음
timestep_embedding에서 [배치수, 모델 채널수]의 임베딩을 반환받으면
이를 (__init__에서 선언된)self.time_embed 계층에 넣는다 [배치수, 모델 채널수 *4]의 emb가 된다
만약, 클래스(레이블)이 주어진 경우라면 self.label_emb의 룩업 계층을 통하여 위의 emb와 더해준다
요약1: 정현파 임베딩을 만들어주고, 필요한 경우 클래스 임베딩을 더해준다
self.inner_dtype과 input_blocks을 사용하여 입력 x를 h로 캐스팅한다
※ 자세한 설명은 공식 문서 https://pytorch.org/docs/stable/generated/torch.Tensor.type.html 에
계층에 h과 emb을 같이 넣어서 h로 반환하며, skip connection을 위해 h를 hs의 list에 append해둔다->UNet의 하강구간
h를 다시 middle block에 emb와 넣고 결과값 h를 뽑아낸다 ->UNet의 아랫구간
output_block의 각 계층을 하나씩 꺼내오며, hs의 값도 하나씩 pop하며 concat하여 skip_connection을 구현한다, cat함수의 차원은 1(NCHW에서  C를 의미)
※input_blocks와 output_blocks는 모두 ModuleList
concat과 emb를 입력으로 하여 output_block의 계층에 넣고 h값을 뽑아낸다
h를 다시금 x.dtype으로 뽑아내고,
self.out계층을 마지막으로 h를 넣고 그 값을 return으로 반환하면 UNetModel의 forward 함수가 완성된다
}
번외1: self.get_feature_vectors 함수는 forward와 비슷하긴 한데, down/middle/up의 결과를 list에 모아서 반환한다
번외2: class SuperResModel <- 얘는 뭐하는 건지는 나중에 알아보자, 아마도 화질 개선인듯
※ script_util의 create_model말고 sr_create_model이 super resolution 관련 모델 선언인듯?

4-2
create_gaussian_diffusion은 script_util.py의 create_model_and_diffusion함수에서 create_model과 같이 사용하는 함수이다
※ 매개변수도 같은 방식으로 조달한다 매개변수의 시작에 빈 asterisk가 존재하는데, 이는 bare asterisk라는 방식으로 asterisk 뒤의 매개변수는
기명으로만 접근가능하게하는, 개발자 편의성을 위한 문법적요소이다. https://this-programmer.tistory.com/503
매개변수들은 후술
여기서 필요한 함수나 클래스는 대부분 gaussian_diffusion.py의 폴더에서 가져온다
betas는 gd에서 noise_schedule과, steps를 주고 β 즉 잡음비(= 분산)를 가져오는것이다. (신호비는 재매개변수화트릭으로, cumprod로 만든다)
아래의 if-elif-else 구문은 매개변수 use_kl, rescale_learned_sigmas를 가지고 LossType class를 설정하는것
-> 변분 하한등의 수학적 이론과, rescale_leanned_sigmas가 대체 뭔지나 알아보자, 내용 자체는 오차 함수인것같다
{ // timestep을 가지고 재처리를 하는모양, timestep 매개변수가 None이라면 [steps]를 사용한다
timestep_respacing 매개변수는 steps와 함께 respace.py에 space_timesteps함수의 매개변수로 들어가게된다
해당 함수의 주석에서
원래 확산 과정에서 사용되던 timesteps의 list를 만듭니다. 원래 확산 과정에서 동일한 크기의 부분에서 사용할 시간 단계수가 저장됩니다.
예를들어, 300회의시간 단계가 있고 section 수가 [10, 15, 20]이면 처음 100회는 10회, 두 번째 100회 스텝은 15회, 세번 째는 20회
스텝이 됩니다. stride가 ddim으로 시작하는 문자열일 경우, DDIM논문에서의 고정 striding이 사용되고 단 하나의 섹션만이 혀용됩니다.
-> section_counts가 list일 경우, timesteps를 len(section_counts)로 나눈 다음 각각의 num_timesteps를 list의 값으로 대체하고
"ddim"으로 시작하는 문자열은 뭔가 특별한 작업을 해준다. 이게 정확하게 뭘 의미하는건???지는모르겠다 -> IDDPM 논문을 다시 읽어보자, 이게 대체 뭔...???
section_counts가 string일경우 다음의 코드가 실해오딘다. desired_count는 "ddim"뒤가 몇 글자인지 세어주는 변수이다.
1부터 timesteps-1까지를 for문으로 순회하며 if문을 하나 돌린다. 해당 if문의 내용은 이러하다.
만약 0부터 num_timesteps까지를 i씩 순회(range(0, num_timesteps, i))할 때의 길이가 desired_count라면, 해당 range로 만든 set을 반환한다.
※ 정수로 안나오면 raise로 오류처리함
??? 이상적인 횟수를 정해놓고, 거기 맞춰서 set을 반환하는모양, 이건 SpacedDiffusion(GaussianDiffusion을 상속함)에서 사용되는데, 그 때 알아보자
만약 section_count문자이나 ddim으로 시작하지 않고 콤마로 구분된 문자열이라면, 이를 잘라 int list로 만들어준다
어쨌가어나 section_counts로 정수 list가 됐다면 len으로 timesteps를 자르고(몫과 나머지는 각기 다른 변수명이다)
list 내부의 값으로 한 번에 몇 단계씩 건너뛸지 (값이 10이면 0, 10, 20...)를 index를 쭉 지정해주고, 이걸 한 list에 담아서 반환한다
요약: 확산 단계를 재가공하는 함수이다. 0부터 최종단계까지 몇 단계식 건너뛸지에 관한 내용이 반환된다.
}
use_timesteps, 아까 만든 betas(= 잡음비)가 첫 두 매개변수이며 model_mean_type이 다음 매개변수인데
이는 create_gaussian_diffusion의 predict_xstart의 값에 따라 EPSILON과 START_X 둘로 나뉜다.(해당 클래스 자체는 gd에 존재)
※ 여러 class가 열거형으로 작성되었으며, ModelMeanType도 그러하다. Python의 enum.Enum을 상속하였다.
이는 문자 그대로 해석하면 모델평균의 타입이며, 이에관한 주석은 gd의 class GaussianDiffusion에 서술되어있다.
다음 매개변수는 model_var_type인데, 이는 매개변수 sigma_small에 따라 FIXED_LARGE와 FIXED_SMALL로 나뉜다.
loss_type과 rescale_timesteps는 받아온 그대로 다시 매개변수로 넣는다.
{ // Diffusion Classes
class Space_diffusion:
반환하는 클래스는 class SpaceDiffusion이다. __init__에서 use_timesteps를 제외한 매개변수를 다시금 base_diffusion(self아님!)에 넣고
for 문으로 순회한다, for문의 내용은 가져온 timesteps 집합에 대하여(space_timesteps으로 가져온!) 일치하는 index가 보이면 betas를 계산한다
alpha_cumprod자체는 base_diffusion의 __init__에서 계산되며, 이를 기반으로 새로운 β값(잡음비)을 계산해준다. 이 과정에서 self.에
timestep_map 리스트를 만들고 갱신되는 timestep의 index를 기록해두며, 갱신된 kwarg[betas]와 여타 kwarg를 가지고 부모 클래스의 __init__을
작동시킨다
space_diffusion의 함수 p_mean_variance/training_losses는 model과 매개변수, 키워드 매개변수를 모두 들고 함수 _wrap_model을 내부적으로
사용하며, _wrap_model함수는 _WrappedModel 클래스(아래에 선언됨)을 내부적으로 사용한다.
이는 오버라이딩 된 메서드로 원본 메서드와 보이는 차이는 거의 없고, model을 래핑된 모델로 사용한다.
추가로, _scale_timesteps도 오버라이딩 되었는데 별 차이는 없고, timesteps에 관계된 메서드 같으나 래핑 클래스에서 처리하므로 이를 무시하도록
메서드가 짜여있다.
class _WrappedModel: 모델을 래핑하는 클래스이다
model, 위의 __init__에서 만든 timestep_map/original_num_steps, rescale_timesteps(원래 가진 클래스의 필드임)를 매개변수로 가져간다.
_Wrap의 __init__에서는 해당 매개변수를 self.로 필드로 가져간다. __init__ 외에는 __call__이 작성되어있다.
__call__은 말그대로 Model 클래스가 호출되었을 때 사용되며, 그 내용은 다음과 같다 ???????? -> 뭔가 적혀있으나 지금 시점에서는 알 수가 없으므로
코드의 흐름을 따라가면서 알아보자
gaussian_diffusion.py파일의 함수와 클래스에 관하여,
첫 두 함수는 확산 스케줄에 관한 내용이다. 세 클래스는 각각 평균, 확산, 오차함수에 관한 내용이며, 자세한 내용은 코드를 통해서 확인해야한다. 그 외에
class GaussianDiffusion: 이름은 가우시안(정규)확산이며, train_util.py의 TrainLoop 코드의 여러 장소에서 사용되므로 학습용 클래스로 추측한다.
호출하는 순서는 image_train.py의 main() -> script.util.py의 create_model_and_diffusion() -> 같은 파일의 create_gaussian_diffusion()
-> respace.py의 SpacedDiffusion class -> 의 부모 클래스인 GaussianDiffusion class (gaussian_diffusion.py의)
{ # __init__에선 뭘하나?
__init__에서 betas와 type들은 호출한 함수에서 가져오거나, 상속한 부모 클래스에서 처리해주며,
rescale_timesteps는 실행 매개변수로 받으며(default True) 이를 기반으로 _Wrapper class에서 처리하고, GD class에서 사용된다
모든 매개변수가 self.의 필드로 들어가며, betas는 넘파이로 변환하여 사용한다. (betas(잡음비)는 [0, 1] 값의 1차원 배열로 강제된다)
self.num_timestep선언부터, 하단의 코드 7줄은 SpaceDiff의 new_betas를 만드는 것으로 추측한다.
alphas의 cumprod는 a_bar를 구현하는 과정이며, alpha_prev와 alpha_next는 이전, 다음 단계의 a_bar 값을 쉽게 가져오기 위함으로
추측한다(크기 자체는 같으며 넘파이 리스트의 끝이나 시작을 하나 떼고 1.0 or 0.0 값을 붙이는 것으로 만듬, 단계적 잡음제거된 이미지를 만들 때 사용)
???
아래 sqrt는 alphas_cumprod(=a_bar)값의 제곱근이나, one_minus, log, recip, recipm1값은 뭐에 사용하는지 모르겠다
-> 코드를 보고 확인해보자???
-> 이거 일단은 self.의 필드라서 클래스 내부에서 사용하기는함
-> 일단은 x_{t-1}에서 x_t의 확산과 others?를 계산한다고한다
-> 어느 코드의 설명에는 5줄 중 위 2줄에만 정방향 확산 과정의 q(X_t|x_0)를 써놨다
(실제 Song et al..2020의 수식에도 로그에 관한내용은 없더라)
그 아랫줄부터 posterior~~~코드는 posterior(후부라고하는데, 사후확률을 의미하는듯?)를 계산한다고 한다.
-> https://kyujinpy.tistory.com/123 여기에 lucidrains의 DDPM코드 설명이 있으며, 이것과 유사하니 참고과 될 만할것같다
-> 해당 논문 해석에 따르면, 놀랍게도, beta_t는 논문에서 '간단하게' 분산으로 쓰자고 제안된것
-> 원래 식은 beta_t에 (1-alpha_hat_prev)/(1-alpha_hat)를 곱한것....!!!
-> 즉, 잡음비 구하는 공식이 약간 다름
-> 그 외에도 로그를 사용하고, 클리핑이 들어가있는 분산/평균을 사용하는 '앙증맞은' 트릭이 존재한다.
-> 루시드레인 논문... 읽어야한다
-> 답이없으니 for_math에 있는걸 모두 읽고 이해해보자, 안그래서야 논문의 내용은 커녕 함수가 뭔지도 모르겠다
}

이후는 뭐가 뭔지 알 수 없으니 방법을 달리해서, guassian diffusion에 있는 함수들은 어디에서 쓰이는지 추적해서 역으로 사용법을 추적하자
{ # image_train.py 즉 학습 과정에서 사용하는 GD class의 함수들
image_train.py에서 diffusion instance를 만들었다면 이를 train_util.py의 TrainLoop class의 instance를 선언할 때
매개변수로 사용한다. 그리고 해당 클래스에서 run_loop()함수를 동작시킨다.
-> 해당 매개변수는 __init__에서 self의 필드로 들어가며, 이후 self.diffusion은 학습과 로깅에 사용된다.
-> 로깅은 GD class의 필드 하나만 활용하니 크게 살펴볼 것은 없으며, 학습 과정은 다음과 같다
-> TrainLoop의 run_loop() -> run_step() -> forward_backward() -> GD의 training_losses() -> q_sample() 순으로 사용된다.
}

{ # 나중에 image_sample.py 즉 샘플뜨는 과정에서 알아보자
반대로, image_sample.py에서는 GD class의 p_sample_loop()나 ddim_sample_loop()를 사용한다
호출순서는 p_sample_loop() -> p_sample_loop_progressive() -> p_sample() -> p_mean_variance()
ddim쪽도 거의 유사하나, 끝에서 p_mean_variance를 호출하고, 호출 이후 같은 함수에서 _predict_eps_from_xstart()를 사용한다
}

+ model.to(dist_util.dev()) 이 코드는 디바이스 선택과 MPI 설정 등 작업
++ load_data는 전용 데이터로더 클래스인듯, torch의 dataloader인것은 맞고 특이사항으로 결정론적 옵션을 받아 shuffle여부를 정한다
클래스라벨이 존재하는지도 따지는듯, 그리고 yield로 로더에 있는거 하나씩 꺼내주는 방식이다
+++ resample.py의 코드는 뭔지 정확하게는 모르겠으나, 옵션으로 uniform과 loss-second-moment가 있다.
아무래도 학습에 사용할 무작위 확산 시간(신호비?)을 샘플링하는 과정인듯 ???
(alpha_cumprod는 [0, 1]사이의 값을 여러번 곱한것이라 결과도 [0, 1]사이로 나온다 -> uniform함수사용, loss-s-m은 뭔지 모르겠음)

5. TrainLoop
train.util.py의 class TrainLoop가 학습이 실제로 이루어지는 클래스이다
매개변수는 모르는 것은 없으나 몇몇 특이한 사용례에 주의하면서 코드 분석해볼것
{
__init__에서는 매개변수를 self의 필드로 집어넣으나, 몇몇 특이사항이 존재한다
microbatch < 이거 batch_size랑 비교해서 얘가 더 크면 batch_size로 self에 넣는 모양, 아마 배치를 잘게 나누어쓰는듯
self.ema_rate는 string으로도 가능한 모양이다, string의 경우 , split해서 리스트로 쓰고, 아니어도 리스트 자체는 사용함
schedule_sampler는 주어진 경우가 아니면 그냥 UniformSampler쓴다, 특이하게도, 매개변수가 diffusion(GD class)이다
global_batch도 좀 특이한데, 분산처리용 torch하위 라이브러리 torch.distributed의 기능을 사용해 batch_size의 n배로 만든다
INITIAL_LOG_LOSS_SCALE는 기본값이 20, 뭔지 모르겠다??? lg_loss_scale로 사용함
model_params나 model에서 .parameters()로 가져오고 master_params는 model_params에서 가져오는데, 뭔 차이인지 모르겠다
최적화 함수는 opt이며 self.master_params가 들어간다, AdamW사용
resume_step이 학습하다가만걸 다시 학습하는 코드인듯, 관련 코드가 load어쩌고
DistributedDataParallel는 여러 GPU를 병렬하는 코드인 모양, 이게 기본 세팅이고 하나있는 gpu도 이대로 가는듯?
cuda_available하다면, 반드시 use_ddp를 하는모양, cuda가 아니면 self.ddp_model = self.model이며 cuda인경우 DDP로 래핑함
}
아래의 함수 _load_and_sync_parameters/_load_ema_parameters/_load_optimizer_state 는 각각 model load용인듯
_setup_fp16은 16bit 부동 소숫점으로의 변환인듯?

{ # run_loop()
run_step()을 호출하는 코드와 그 외 코드들이다. while로 반복하는데, 조건이 좀 특이하다
아래 코드는 일종의 더미? 데이터가 아니겠는가/openai가 제공하는 모델중에 lr_anneal_steps를 쓰는 모델이 존재하지를 않음
                    (not self.lr_anneal_steps) or (self.step + self.resume_step < self.lr_anneal_steps)가 while의 조건인데
                    조건 자체는 어닐링 스탭이 0이거나 전체 돌린 epoch가 어닐링 스탭보다 작을 경우 둘 중 하나만 만족해도 동작하는데,
                    0으로 주어 무한으로 돌리거나, 어닐링 스탭을 epoch 대용으로 사용하는 2가지 경우가 아닌가 싶다
                    하지만 어째선지, run_step()은 optimize_normal()를 호출하고 이는 _anneal_lr()를 호출한다 그리고 _anneal_lr()에서는
                    어닐링 스탭 값이 0일 때는 아무것도 안하지만 돌린 횟수(== self.step + self.resume_step)를 어닐링 스탭 값으로 나눳을 때
                    lr의 감쇠가 일어나도록 짜여있다. 즉, 최종적으로 돌릴 횟수가 어닐링 스탭보다 큰 것을 상정하는데, run_loop()의 while조건과는
                    맞지 않는 코드라 동작하지 않으니, self.step이나 self.resume_step의 흐름을 확인할 필요가 있다.

batch, cond는 next(self.data) -> 이건 yield로 만들어진 데이터로더 가져오기           (최종적으로 GD의 training_losses에서 class가 없는 경우라도 동작하도록 짜여있음)

self.run_step()을 호출하고 거기서 동작함
    { # self.run_step(self, batch, cond)
        순전파역전파 -> 하강최적화함수 -> 로그따기 순서로 진행함
        { # forward_backward
            zero_grad(model_params)시작 (※ 해당함수는 fp16_util.py에 있으나 딱히 16bit fp랑 관련있어보이지는 않음)
            해당 함수는 param.grad를 detach_하고 zero_함 (grad 기울기 자동 추적 삭제 및 텐서를 0으로 초기화)

            이후 for문으로 batch_size/micro_batch만큼 순회함(※ 배치를 마이크로 배치만큼 나눠서 처리하는게 사실인듯)
            micro, micro_cond는 데이터와 라벨을 마이크로 배치만큼 잘라서 가져오는 작업 (※last_batch는 boolean type으로, 이름 그대로의 역할임)

            t, weights를 가져온다. 이는 resample.py의 Uniform sampler를 참고하여 보면 다음과 같은 행동을 한다
            -> [0, num_timesteps) 범위안의 정수를 가지고 길이가 batch_size인 indices와 같은 길이의 1로 차있는 텐서 weights를 반환한다
            micro.shape[0] 마이크로 배치의 크기이므로 해당 크기만큼처리함

            이후 GD class의 training_losses함수를 호출하는데
            ddp로 wrapping된 모델과, 배치에서 잘라낸 마이크로 배치와, indices와, (존재한다면) 마이크로 크기로 잘라낸 라벨들을 매개변수로 한다
                { # training_losses
                    주석에 보니 단일 타임스텝에 대하여 오차를 계산한다고 한다
                    if model_kwargs가 None(= 라벨 없으면) 빈 {}로 처리한다
                    noise는 호출 때 넣어주지 않았는데, 따라서 None이며, x_start(여기선 마이크로 배치) 크기만큼의 0~1사이 노이즈를 만들어준다 ???
                    ※ 주석을 보니 이게 값이 지정된 경우면 제거하려는 특정 가우시안 노이즈라고한다...???
                    q_sample을 사용하여 x_t를 뽑아낸다, 이 때 매개변수로 배치, 인덱스, 노이즈가 주어진다
                    {   # q_sample
                        주어진 확산 단계 수만큼 데이터를 확산시킨다고 하는데, 정황상 얘가 정방향 프로세스인듯
                        q(x_t | x_0)라고 한다
                        재매개변수화 트릭이 적용된 정방향 확산 과정은x_t = sqrt(alpha_t_bar) * x_0 + sqrt(1-alpha_t_bar) * ε 이며,
                        반환값은
                        _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start
                        + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)
                        * noise 인것으로 보아
                        정방향 확산 과정이 맞는것으로 추측된다
                        { # _extract_into_tensor
                            매개변수명이 arr,timesteps,broadcast_shape이며 각각 신호비 분산값의 제곱근(아래의 경우 1-alpha의 제곱근), 인덱스, x_0의 shape인데
                            코드가 약간 세밀하게 볼 포인트가 몇 군데 존재한다
                            일단 텐서로 변환시키는거 까지는 별 문제없으나 [timesteps]로 가져오는것을 확인할 수 있다
                            이 때, timesteps로 들어온 값은 forward_backward()에서 sampler로 가져온 t(=indices)값인데, 이 값을 통하여
                            얼마만큼의 alpha_t_bar값을 가질건지를 배열에서 가져오는 모양이다(일종의 fancy indexing인듯, 텐서도 그게 되나?)
                            (broadcast_shape으로 들어온건 x_0.shape즉 [N, C, ...] 텐서인데, res와 broadcast의 shape을 보고 차원(축)을 맞춰주는 코드가 따로 있다)
                            반환 할 때는 res를 expand(https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch-tensor-expand)하여 반환한다
                            아마 이미지 채널, 픽셀 값에 각각 곱해주기 위해 차원을 늘리는 모양
                            요약 -> q(x_t|x_0)를 위한 alpha값을 텐서모양에 맞게 뽑아준다. 사실 numpy 값으로 텐서 뽑아주는 함수인듯
                        }
                    }

                    Bits per dimension(BPD): 음의 로그 가능도 / (C * H * W), 낮을 수록 정규 분포에 근접함
                    terms와 if문의 내용을 정확히는 모르겠으나, VAE와 관련 있는 내용으로 추측한다
                    일단 내용은 다음과 같다 self.loss_type이 KL이거나 rescaled_KL이라면 _vb_terms_bpd함수를 사용하여 term["loss"]를 채운다
                        {   # _vb_terms_bpd
                            변분 바운드와 비트 당 손실을 계산하여 모델 학습의 손실을 평가하는 역할(by gpt)

                            true_mean, _, true_log_variance_clipped를 어느 함수로 가져온다
                            {   # q_posterior_mean_variance
                                해당 함수는 x_0/x_t/timestep을 매개변수로 하여 x_t-1(diffusion posterior)의 평균과 분산을 구한다고 한다
                                구한다음 이를 KL 발산으로 뭔가 처리를 하는데, 이건 나중에 논문을 좀 더 자세히 들여다 보기로하고 뭘 구하는지나 알아보자

                                이 함수의 용도를 gpt에 물어본 결과,
                                후방 분포(베이지안 통계학에서 x_t와 x_0가 주어졌을 때 사후 확률)를 구하여
                                역방향 확산 과정을 지원하는데 사용한다.
                                자세한 내용은 note.txt에 서술

                                후방 분포 q(x_t|x_t, x_0)의 평균은 coef1에 x_0를, coef2에 x_t를 곱하여 더하는 것으로 계산된다
                                그 계수들은 시간단계 t로 가져오며, 해당 계수를 만드는 방법은 논문 2페이지의 (10)과 (11)번 수식에 존재한다
                                베이지안 이론에 의하여, 후방 분포와 잡음비와 평균은 다음과 같이 계산한다
                                wave(물결표)_beta_t = beta_t * (1 - alpha_bar_t-1)/(1-alpha_bar_t) ※ bar은 cumprod를 의미
                                wave_μ_t(x_t, x_0) = x_0 * (sqrt(alpha_bar_t-1) * beta_t)/(1 - a_t_bar)
                                                    + x_t * (sqrt(alpha_bar_t) * (1 - a_bar_t-1)/(1 - a_t_bar)
                                q(x_t-1|x_t, x_0) = N(x_t-1; wave_μ(x_t, x_0), wave_beta_t*I)

                                _extract~~~ 함수에 들어가는 coef들을 보면, 해당 식과 일치한다

                                posterior varience도 같은 값이긴 하나 log를 붙여 사용하는 self.posterior_log_variance_clipped같은 경우에는
                                log 0이 존재하지 않기 때문에, [1]번의 log 값을 사용해 [0]번을 갈음하여 사용한다
                            }

                            {   # p_mean_variance
                                해당 함수는 모델을 사용하여 이전 상태 x_t-1의 분포 p(x_t-1|x_t)를 예측하고, 초기 상태 x_0를 예측하는 함수라고 한다
                                위 함수와 마찬가지로 x_t-1에 대한 분포를 제작하나, 모델을 사용한다

                                B, C = x.shape~~~부터 3줄은 모델 즉 UNet의 예측 즉 예측 잡음을 가져온다
                                ???
                                -> 이 코드는 다음에 논문이라도 읽어보고 하자, 대체 뭔 소린지 원...
                            }

                            위 두 함수는 x_t-1에 대한 분포를 구하나 그 방식이 베이지안 사후 분포에 의한 것인지, 모델 패러미터에서 나온것인지 차이가 있다
                            이 차이에 대한 값을 쿨백 라이블러 발산으로 계산하는 것이 아래의 kl 두 줄이다.
                            한번 decoder_nll은 discretized_gaussian_log_likelihood 라는 함수로 가져오는데 이는 이름 그대로 이산 정규 로그 음의 가능도이다
                            위 모델의 분포 중 mean값과, variance, 그리고 원본 이미지 x_0 값을 사용하여 음의 로그 가능도 오차를 계산해준다
                            KL이나 NLL쪽 모두 이진 로그로 후처리를 해준다
                            ???
                            그러나 반환할 적에 output으로 들어가는것은 하나 뿐인데, 첫 타임 스탭에서만 NLL이 채택된다
                            ???
                            그 외에는 모델로 에측한 pred_x_0가 같이 반환된다
                            즉 오차 한 종류와 예측된 이미지 하나가 반환된다
                        }
                }
        }
    }
}