학습의 경우

실행은 image_train.py로 시작함
플래그 3개를 파싱함
MODEL_FLAGS에는 이미지 크기(h, w 같은 숫자인듯), 채널수, 잔차블럭수
DIFFUSION_FLAGS에는 확산 단계수, 확산 스케줄 (코사인 등등)
TRAIN_FLAGS에는 학습률, 배치크기

1.
image_train.py는 시작하자마자
create_argparser()의 반환(argparse.ArgumentParser클래스)의 parse_args()함수를 사용함

create_argparser 함수는 argparse.ArgumentParser 클래스를 반환함
해당 함수내의 이름이 default인 dict와
-> script_util.py의 model_and_diffusion_defaults 함수에서 반환하는 dict를 합쳐 update
-> 매개변수들 key와 기본 value를 가진 dict구성됨
-> argparse.ArgumentParser() 클래스를 선언하고
-> 위 기본값(default dict)를 가지고 parser를 구성
-> 구성된 parser로 실행시 입력한 값 파싱(parse_args(), 해당 반환값은 Namespace 객체임)
-> 이후 반환하여 코드의 각 부분마다 필요한곳 인자만 뽑아서 사용(기본적으로 args에 다들어있음)

요약: 실행 개개변수를 파싱합니다

2. setup_dist()
dist_util.py의 dist가 정확히 뭘 의미하는지는 모르겠으나
distributed는 부가적 코드를 의미한다고 한다.
코드 중에 torch.distributed가 존재함, 이건 병렬 처리에 관한 패키지인듯?
그러고보니 mpi4py 패키지와 관련이 있을 수 있겠다

??? 나중에 알아볼것
https://blog.naver.com/PostView.naver?blogId=sw4r&logNo=222314867436
https://better-tomorrow.tistory.com/entry/Pytorch-Multi-GPU-%EC%A0%95%EB%A6%AC-%EC%A4%91
요약: 병렬 처리용 부가코드

3. logger.configure()/logger.log
로그 따는 기능, 따로 코드로 만들어 놨다
openai-연-월-일-시-분-초-다음엔뭐더라? 폴더에 로그 따놓는게 이 기능인듯
요약: 로그 따는 기능, 안 중요한것 같으니 다음에

4. model, diffusion = create_model_and_diffusion/create_named_schedule_sampler
후술, 가장 중요하게 볼 부분
모델과 샘플러제작

script_util.py의 함수 create_model_and_diffusion은
말그대로 모델과 확산을 뽑는 것으로 보인다.
여기서 확산을 뽑는것의 정확한 실체는 뭔지 나중에 알아보고 model부터 알아보자

4-1-1 내부적으로 create_model 함수를 사용하며 이는 같은 파일에 존재한다.
매개변수로 이미지 크기, 채널수, 잔차블록수
체크포인트 사용, (어텐션) 헤드 수, 드롭아웃은 기존에 아는 정보이나
learn_sigma, class_cond, attention_resolutions
num_heads_upsample, use_scale_shift_norm은 모르는 정보이니 이 매개변수들의 흐름을 따라가볼것
요약: 매개변수는 저런것이 있고 모르는건 따라가보자

함수 코드를 쭉 따라가 보자면,
먼저 채널 사이즈에 맞춰 channel_mult 튜플을 달리 설정한다
256/64/32 사이즈만 인정하며 나머지는 에러로 처리함, 이것도 어디에 쓰이는지 알아보자

attention_ds 리스트는 attention_resolutions 매개변수를 ','로 split 해서 나온다
그렇게 split 한 값을 res라 하며 integer 값으로 바꾼 다음
image_size를 res로 나눠 몫을 구한다, 그리고 그 몫을 attention_ds에 append한다
ds는 추측컨대 downsample인듯 이 매개변수들은 결국 unet.py의 UNetModel 클래스에서 사용되는데,
거기 attention_resolutions(어텐션 해상도)에 해당 리스트를 넣는다
거기 매개변수 설명도 적혀있는데, 4가 리스트에 들어가 있으면 4x downsamling을 진행한다는 뜻
다운 샘플링은 매개변수 크기를 줄이는 모양?
실행 매개변수에 --attention_resolutions 16과 같이 사용할 수 있다!
요약: 어텐션 다운샘플링은 어텐션 해상도 매개변수의 정보를 바탕으로 생성, 여러 값 가능

그걸 기반으로 unet.py의 UNetModel class를 선언하고, 이를 return하는게 create_model함수
결국 'model'이라는건 unet model이다
들어가는 매개변수는 입력채널 (3고정 RGB), 모델채널은 num_channels
잔차블럭수는 그대로, 어텐션 방법은 아까 만든 ds 리스트를 튜플로 변환해서
드롭아웃은 그대로, 체크포인트사용여부도 그대로, 헤드 수도 그대로,
들어가는 매개변수중에 모르거나 좀 더 알아볼 필요가 있는 것은
out_channels, channel_mult, num_classes, num_heads_upsample, use_scale_shift_norm이다
요약: 모델 선언은 unet.py의 UNetModel 클래스에다 매개변수를 끌어다 모아서 가져온다.

4-1-2
unet은 말그대로 unet이며, 해당 파일의 모든 클래스는 UNetModel 클래스로 사용된다
class의 내용을 확인해보자
class TimestepBlock: abc패키지의 @abstractmethod를 사용하는 추상메서드이다.
nn.Module을 상속받고 TimestepEmbedSequential에 상속되는 것으로만 역할끝

class TimestepEmbedSequential: TimestepBlock과 nn.Sequential을 다중상
일종의 단위 구성요소로 사용하는 것으로 추측함
isinstance 메서드로 for layer in self의 각 계층이 TimestepBlock일 경우
계층에 forward할 때 x에 emb를 같이넣어주고, 아닐경우 x = layer(x)임
주석은 다음과 같음 "시간 단계 임베딩을 하위 항목에
전달하는 순차 모듈이며 추가 입력으로 지원됩니다"
emb의 경우 이를 상속하는 하위 클래스들이 구현하는 대로 오버라이딩 되는듯
요약: 임베딩이 추상메서드로 구현된 nn.Sequential임

class Upsample: UNet의 업샘플링 블럭, nn.Module을 상속받는다
매개변수로 채널수, 합성곱 사용여부, 차원(기본값 2)가 있다
__init__에서는 넣어준 매개변수를 모두 필드 변수로 가져오고,
use_conv true의 경우합성곱 층도 하나 만들어서 메서드로 만듬
forward()의 흐름은 다음과 같다 먼저 NCHW의 2번째 값
x.shape[1]을 필드변수 channels와 같도록 강제한다. 그 다음 차원에 맞게 2배크기로 보간한다
합성곱 층을 사용한다고 되어있으면 합성곱에 넣고, 아니면 그냥 반환한다.
요약: 합성곱은 옵션으로 사용하는 업샘플링 계층

class Downsample: UNet의 다운샘플링 블럭, nn.Module을 상속받는다
채널/합성곱 층 사용여부/차원을 매개변수로 사용, 업샘플링이랑 대체로 비슷함
stride는 2/3차원 여부에 따라 2 혹은 (1,2,2)을 사용한다
합성곱 층을 쓰면 매개변수들 그대로 합성곱 층을 사용하고, 아닌 경우
평균 풀링을 차원과 stride에 맞게 진행한다.
채널 강제 여부는 같으나 forward에서 합성곱/풀링 둘 중하나를 써서 반환하는거 말고는 하는거 없음
요약: 합성곱과 풀링 중 택1해서 사용하는 다운샘플링 계층

class ResBlock: 잔차블록인듯, TimestepBlock을 상속받는다
매개변수로 채널수, 임베딩 채널수(타임스탭임배딩 채널), 드롭아웃, 아웃채널, 합성곱 사용 여부
차원(입력 데이터), 체크포인트 사용 여부를 받아온다 초기화 함수에서 아웃채널
합성곱 사용여부, 스케일 시프트 노름, 체크포인트사용, 데이터 차원은 기본값 존재
_forward에서는 입력과 임베딩을 각각 다른 층에 넣고, 길이가 다르다면 임베딩쪽을 반대쪽과 맞춰준다
use_scale_shift_norm가 True일 경우, out_layer를 [0]과(out_norm) 나머지(out_rest)로 나눈다음
임베딩 아웃 값을 torch.chunk하여 scale과 shift로 나눈다.(1차원으로 2동강낸다, emb_layer에서 이미 채널 2배로 출력되게 설계됨)
그런다음 out_norm(= out_layer의 정규화 계층)에 넣는다 ※ .nn으로 된건 torch docs에서 찾아볼 수가없고, nn.py에 직접구현해둔것!!!!!!!! ※※ 구현자체는 nn.GroupNorm을 float32로 구현한것
넣은 값을 스케일링하고, 시프팅 한 다음 out_rest에 넣는다. 그러면 h나옴
use_scale_shift_norm가 False인 경우
입력의 출력과 임베딩 입력의 출력을 더하고 out_layer전체에 집어놓고 h만듬
어떻게 됐건 간에 skip_connection에 x을 넣고 h를 더한다.
in_layers는 정규화->SiLU->합성곱 순서이며
emb_layers는 SiLU->선형층(use_scale_shift_norm True의 경우 out channel의 2배, 아니면 그냥 1배)
out_layers는 정규화->SiLU->dropout->zero_module안에 합성곱넣어서 동작 순서이다
※ zero_module도 nn.py에 구현되어 있다. 모든 파라미터를 0으로 초기화할 때 쓰는 클래스인듯
skip_connection은 out_channels==channels인지, 합성곱을 쓰는지, 등등에 따라 달라지는 계층이다
입출력 채널이 같을 때 skip_connection의 nn.Identity는 그냥 값을 그대로 보내준다는 뜻
아마 잔차 연결할 때 채널이 다른경우 맞춰주는 용도로 알고있다
forward는 _forward를 사용하는데, 인자로 x, emb를 사용하지만 반환에 checkpoint 함수를 사용한다
checkpoint 함수는 nn.py에 구현되어있으며 함수/함수에 들어갈인자/모델의 매개변수(저장용)/플래그를 입력받는다
flag는 use_checkpoint값이며 true의 경우에만 checkpoint 저장을 해주는 모양, true이면
같은 nn.py의 class CheckpointFunction를 사용해 처리하는걸로 추정함
요약: 잔차블럭이다

class AttentionBlock: 트랜스포머 어텐션인듯? nn.Module을 상속받는다
매개변수로 채널수, 헤드개수, 체크포인트 사용여부를 받고 __init__에서 self필드로 전환
__init__에서 추가적으로 nn.py의 normalization사용, 이건 그룹정규화다/https://luvbb.tistory.com/43
self.qkv는 합성곱의 일종인듯하며, 차원은 1차원이고 입력채널의 3배만큼의 출력을 만든다(아마 각각 q, k, v인듯?), kernel_size는 1
이 때 사용되는 conv_nd가 nn.py에 작성되어있으며, 합성곱이긴하나 kernel_size가 1인걸로 보아 사실상 선형 임베딩을 대체하(거나 그 자체가 되)는 모양
self.attention은 class QKVattention의 인스턴스이며 후술
self.proj_out은형식상으로는 입출력 개수가 channel이고, kernel 크기가 1이며 0값으로 초기화되는 1차원 합성곱신경망이나, 뭘 의미하는건지는 알아볼 필요가 있음
forward가 내부적으로 _forward와 checkpoint를 이용하는건 ResBlock과 같음
_forward는 함수 시작과 동시에 x.shape를 b, c, *a로 나눈다(NCHW를 각각 N,C,HW로 나누는듯? *는 unpacking연산자)
x를 (b, c, -1)로 reshape하는 것으로 보아 배치와 채널을 그대로 하고 나머지를 1차원으로 바꾼다 즉 (N,C,H,W) -> (N,C,H*W)
그런다음 x를 그룹정규화 하고 qkv 계층에 넣는것으로 qkv 텐서를 만들어낸다
그런다음 qkv로 'attention'하고 이를 다시 reshape하고, 이를 proj_out에 넣은다음 h를 만들어 x값과 더하고, 이를 다시 원래대로 reshape한다
h.reshape(b, -1, h.shape[-1])은 대체 뭔지 모르겠다???
+0627추가: attention의 결과물은 [N, C, T]이므로 h.shape[-1] = T  = H*W, (b, -1, T)의 경우 결과적으로 (b, c, T)크기 텐서로 변환
추측) proj_out은 transformer encoder block 끝의 ffn의 역할일지도?


class QKVAttention: 모르긴 몰라도 어텐션 메커니즘을 여기서 사용하는듯, nn.Module을 상속받음
qkv가 out_channel이 channel*3이므로, 위에서 HW를 1차원으로 줄였으니 qkv가 [N, (C*3), H*W]의 크기의 텐서이다
q, k, v를 다시 [N, C, H*W] 크기의 텐서로 3등분해준다
scale이란 값이 나오는데 Attention(Q, K, V) = softmax((Q K^T) / sqrt(d_k)) * V에서 sqrt(d_k)를 의미하는 값이다
weight과 einsum은 아인슈타인 표기법이다. A와 B 행렬의 곱을 C라 하자면 Cij = (AB)ij = (from k=1 to n)a_ik * b_kj = a_i1*b_1j + a_i2*b_2j + ⋯ + a_in*b_nj
이는 th.einsum('ij,jk->ik', a,b)으로 간단히 표기할 수 있다. QKV 텐서는 픽셀수 H*W=T와, 채널 C에 대해 [T, C/num_head] 크기의 행렬을 가지고 있다
따라서 Q K^T는 [T, T] 크기의 행렬이다. 한편, 코드로 만들어진 QKV텐서는 각각 [N, C, T] 크기이다
Q, K를 각각 [b, c, t]와 [b, c, s]로 본다면 배치의 행렬은 각각 t행 c열/s행 c열이 될것이고, 전치하니 뒤는 c행 s열이 될것이다, 이를 간단히 하면 'tc,cs->ts', q, k이며,
b개 만큼의 배치이므로 'bct,bcs->bts', q, k이다
요약1: 어텐션 스코어맵을 구하는 과정에서 아인슈타인 표기법을 사용한 텐서곱을 진행하였다
이후 bts를 softmax하고 비슷한 방식으로 V 텐서와 곱하면, 그것이 바로 어텐션이다
요약2: attention(Q, K, V) = softmax((Q K^T) / sqrt(d_k)) * V, 즉 forward함수는 self-attention 메커니즘 그 자체를 충실하게 구현했다
[N, C, T]크기의 어텐션 결과물을 돌려준다
+count_flops는 작업 수를 계산하는 thop 패키지용 카운터, 어텐션 연산에 들어있다...고 주석에 적혀있다.


class UNetModel: nn.Module을 상속함, 잡음 추측 모델
매개변수로는 들어오는 채널수, 모델 채널수, 나가는 채널수, 잔차블럭수, 드롭아웃 비율, 차원, 클래스 개수
체크포인트 사용여부, 헤드수, 스케일_시프트_놈_사용은 뭔지 알겠고
attention resolutions, channel_mult, conv_resample, num_head_upsample은 코드 따라 알아볼것
일단 __init__시작하자마자 num_head_upsample 수부터 정하는데 -1이 default이고 디폴트는 num_head값이랑 똑같이 맞춘다
그 외에 다른 매개변수는 self의 필드로 전환함
time_ewbed_dim은 모델 채널의 4배수로 설정, time_embed 계층은 nn.Sequential로 선형->SiLU->선형층을 거쳐 time_embed_dim 크기의 out을 출력 -> 뭐하는 건지나 알아보자
num_classes가 None이 아닐 경우, self.label_emb이 nn.Embedding으로  num_classes행 time_embed_dim열 크기로 선언됨  ※ nn.Embedding은 lookup table이다 -> 대체 뭐에다 쓰는건지, 룩업테이블은 어떻게 만드는지 알아보자
input_blocks는 nn.ModuleList로 선언되고 선언 될 때는 TimestepEmbedSequential에 합성곱 신경망이 하나 있으며
해당 합성곱 신경망은 in_channel을 받아 model_channel개수의 채널을 출력하며, 커널 크기는 3이다
input_block_chans는 model_channels가 담긴 list로 선언한다, ds는 1로 선언한다
{ //첫 번째 계층의 구성, 코드의 for level, mult...부터 ds *= 2 까지
아래 for문으로 channel_mult에 대해 반복 작업을 하는데
또다시 for로 num_res_block만큼 layers에 잔차블럭을 만들어준다.
이 때 channel_mult의 배율만큼 출력 채널이 늘어난다 ※ 입력채널은 ch 변수로 규정, 앞선 출력 채널이 입력 채널이 되는 방식
※ds는 downsampling 배율로 추정 ds값 (초기 1)이 attention_resolutions안에 존재한다면, layers에 어텐션 블럭을 하나 추가해준다
self.input_blocks에 layers의 모든 계층을 TimestepEmbedSequential에 넣고 append한다
또, input_block_chans에 채널 수(ch)를 append함
요약1: 이중 for문안에서 내부 for문은 잔차블럭의 수만큼 (잔차블럭과 어텐션블럭으로 이루어진)블럭을 만들어 내고, 채널도 기록해둔다
channel_mult의 반복 즉 외부 for문은 내부 for문의 하나와 if문 하나로 되어있다. 내부 for문은 위의 내용
channel_mult는 enumerate로 래핑되어 반복되는데, index에 해당하는 부분은 level로 되어있다.
if문의 조건은 이 level을 사용한다. (level의 값이 channel_mult의 길이보다 1보다 작은 값과 같지않다면)
즉, 마지막 단계가 아니라면 다운샘플링 계층을 만들어 input_blocks에 넣고, input_block_chans에 채널을 기록한다, 추가로 ds를 2배한다
요약: 여러 매개변수로 unet의 한 부분을 만들어낸다. channel_mult는 채널에 곱해지는 배율이다.
추측: attention_resolutions는 어텐션 해상도를 의미하는 모양, ds가 왜 해당 값에 포함이 될 때만 어텐션 블럭을 추가해주는지도 알아봐야한다
}


+ model.to(dist_util.dev()) 이 코드는 디바이스 선택과 MPI 설정 등 작업

5. TrainLoop