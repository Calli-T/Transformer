순전파=순방향전달: 입력이 주어지면 신경망의 출력을 계산하는 프로세스
역전파: 순전파 방향과 반대로 연산 진행, 예측과 실제 값의 차이를 계산해 오차를 줄인다.
순전파 과정에서 나온 오차를 활용해서 각 계층의 가중치와 편향을 최적화한다.

순전파는 입력값과 가중치를 곱한걸 전부 다 더하고, 편향을 더한다.
오차는 위 과정의 결과랑 실제 값을 가지고 오차 함수를 사용해서 뱉는다.

역전파는 가중치 갱신이 역순이다.
모델의 학습은 오차가 작아지는 방향으로 갱신하므로 미분값이 0에 가까워져야한다.

146p
https://goofcode.github.io/back-propagation
https://hi-guten-tag.tistory.com/211
https://kimmessi.tistory.com/20
https://bskyvision.com/718
https://wikidocs.net/37406
????

시그모이드 함수의 미분과정:             https://187cm.tistory.com/30
좀 더 자세한 미분과정:                   https://cord-ai.tistory.com/47

오차역전파를 일일이 미분하는 대신 하는이유:
가중치를 갱신하기 위해서는 각 노드의 손실함수를 일일히 미분했다간
너무 많은 시간을 잡아먹기때문에 미분의 연쇄법칙을 이용해 계산양을 줄인다.

미분의 연쇄법칙: 합성 함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다

출력 목표치: t
k층 출력: a(k) = a1w1 + a2w2 +... +anwn + bias
k층 오차: δ(k) = a(k) - t
k층 가중치: W(k)
학습률: α

k층 오차성분 계산: δ(k) = δ(k+1)W(k+1)a(k)'
k층 가중치 보정: W(k) = W(k) - α * δ(k) * a(k-1)                  이번층 가중치 - 학습률 * k층 오차성분 * 이전층 출력

W(k) = W(k) - α * ∂ E / ∂ W(k)              ∂ E / ∂ W(k)는 가중치의 오차에 대한 미분

경사하강법 때 W_i+1 = W_i - α∇f(Wi)               ∇f(Wi)는 기울기, 기울기가 -이면 오른쪽으로, +면 왼쪽으로 가서 기울기 0인구간으로 간다.
∂ E / ∂ W(k) 와 ∇f(Wi)는 무슨 차이인가?
앞은 가중치를 오차에 대해 미분하였다. 뒤는 기울기이다, 애초에 뭐에 대한 기울기였나? 경사하강법의 가중치의 도함수가 기울기였다.
가중치의 오차에 대한 미분, https://hyjykelly.tistory.com/38 이것과는 무슨 차이가 있는가

https://wikidocs.net/37406에선 앞으로 넘길때 E_total즉,한 층의 오차 전체!!!를 사용한다
다시 보니 시그모이드 출력값의 Etotal에 대해 미분하는 과정에서 나머지는 상수로 다 지워진다

https://wikidocs.net/37406 이걸로 확인