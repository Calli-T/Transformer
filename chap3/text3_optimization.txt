65p

최적화: 목적 함수의 결괏값을 최적화하는 변수를 찾는 알고리즘
*목적함수: 함숫값의 결과를 최대 또는 최소로 최적화하는 함수
**학습: 인공 신경망에서 오찻값을 최소화하여 학습 데이터에 대한 가설의 정확도를 높이는 행위
손실 함수의 값이 최소가 되는 변수를 찾는다면 새로운 데이터에 대해 보다 정교한 예측가능
최적화 알고리즘은 실젯값과 예측값의 차이를 계산해 오차를 최소로 줄일 수 있는 가중치와 편향을 계산
최적의 가중치와 편향을 갖는 가설은 오찻값이 0에 가까운 함수가 된다 <-> 가설에 대한 함수의 도함수의 변화량이 0이 가깝다

정리하자면 정교한 예측을 위한 가설 즉 함수를 만들기위해
목적 함수의 결과값을 최적화하는 과정이 최적화이며
목적 함수의 최적화는 곧 손실 함수의 값의 최소화를 의미하면
손실 함수의 최소화는 오차의 최소화를 의미하고
오차의 최소화는 가중치와 편향이 최적의 값일 때 이루어지고
최적의 값은 가설 함수의 도함수의 변화량이 0에 가까울 때 이루어진다.
이게 뭔소리야
최적의 가중치 -> 오차가 적다
가중치와 오차그래프에서 최적을 벗어나면 모두 오차가 커진다 즉
가중치와 오차의 그래프에서 기울기가 0이 되는 지점에서 최적의 가중치를 갖는다
어느 지점에서 가설함수가 최적의 값을 가지는지, 최적에서 먼값인지는 알 수가 없다
따라서 그래프를 이동해야하는데 이동하는 방향은최적화 알고리즘이 따로 있다

최적화: 손실함수의 값이 최소화되는 가설 함수의 가중치를 찾아내는것

경사 하강법: 함수의 기울기가 낮은 곳으로 이동시켜 극값에 도달할 때까지 반복하는 알고리즘
함수의 기울기가 가장 낮은곳에서 (가중치의) 최적 해를 가지는 것은 알려져있다.

경사 하강법
1. 가중치의 초깃값을 잡는다.
2. 가중치의 다음번 값은 이전값에서 기울기와 곱한 값을 뺀다 (수식으로 하면 W_i+1 = W_i - a * f(W_i), 이때 f(W_i)는 기울기)
3. 기울기가 0을 갖게되는 가중치를 찾을 때까지 반복한다.

a는 보폭이다.(learning rate라고 했던것같다)

---
가설 Y_i_hat = W_i * x +b_i로 하고
손실 함수는 평균 제곱 오차를 사용하면
MSE(W, b) = 1/n * (i to n)∑(Y_i - Y_i_hat)^2

f(W_i)는 손실함수의 미분으로서,
W_i+1   =   W_i - a * ∂/∂W MSE(W, b)
        =   W_i - a * ∂/∂W (1/n * (i to n)∑ Y_i - (Y_i_hat)^2)
        =   W_i - a * ∂/∂W (i to n)∑ 1/n{Y_i - (W_i * x + b_i)^2}
        =   W_i - a * 2/n * (i to n)∑ {{Y_i - (W_i * x + b_i)} * (-x)}
        =   W_i - a * 2/n * (i to n)∑ {(Y_i - Y_i_hat) * (-x)}
        =   W_i - a * 2/n * (i to n)∑ (Y_i_hat - Y_i) * x
        =   W_i - a * 2E[(Y_i_hat - Y_i) * x]

2는 생략 가능
W_i+1   =   W_i - a * E[(Y_i_hat - Y_i) * x

편향에 대해 하려면 b에 대해 편미분 해야한다.

실전 경사하강법에 관한 디테일은
https://velog.io/@jakeseo_me/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-3-3-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98-%EB%AF%B8%EB%B6%84-%EA%B8%B0%EC%9A%B8%EA%B8%B0

학습률은 가중치를 갱신할 때 곱해지는 값 a이다. 값이 적절해야한다

71p
최적화 문제
학습률에 따라 그래프가 발산하여 아예 못찾거나, 너무 적게 움직여 오랜 시간이 걸릴 수가있다.

기울기가 0이 되는 값이 여러개라면, 최적의 값을 찾지 못하는 경우가 있다.
기울기가 0이 되는 지점은 최댓값/최솟값/극댓값/극솟값이 있다
학습률이 적절하게 크지 못하거나, 시작점의 위치에 따라 최솟값이 아닌 극솟값에서 가중치가 결정될 수 있다.
또한, 안장점이 존재하는 함수에서도 적절한 가중치를 찾을 수 없다.

안장점: 특정방향에서 보면 최댓값 or 극댓값이지만 다른 방향에서는 최솟값/극솟값이 되는 점
(3차원 그래프 상에서 표현되었다)

최적화 알고리즘은 목적 함수가 목적 함수가 최적의 값을 찾아가도록 최적화되게 하는 알고리즘 (경사하강법등)
거기에 Momentum, Adam, Adagrad등 여러 최적화 알고리즘 존재

~72p