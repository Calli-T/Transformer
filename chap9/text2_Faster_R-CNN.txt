510p
Faster R-CNN

Faster R-CNN은 영역 제안 네트워크(Region Proposal Network, RPN)를 사용해서 관심영역을 추출
선택적 탐색 보다 관심영역을 더 잘뽑는다고함

설명 읽어보니 관심영역 뽑고 난다음은 Fast R-CNN과 비슷한듯
RPN에서 영역 뽑을 때 백본모델로 feature map뽑은걸로 처리를 하는듯

영역제안 네트워크, RPN
- R-CNN과 달리 모든영역 -> feature vector로 가는게 아니라
(객체 탐지를 위한) 특징 추출과
(경계 상자 위치 추정을 위한) 영역 제안 방식을 합친구조
- GPU연산이 되서 빠르다고함
- Selective Search보다 정확하다고함

실제로 하는일
1. 이미지를 격자로 쪼개서 슬라이딩윈도우(가 지나갈 궤적???)생성
512x512를 16x16 격자로 쪼개면 1024개의 슬라이딩윈도우!
2. 슬라이딩 윈도우로 생성된 셀에 전부 앵커 박스적용
1024개의 슬라이딩윈도우 셀에 앵커박스 9개 적용시 9216개의 앵커 박스!
※ 앵커박스: 객체의 위치와 크기를 예측하기 위해 사용되는 사각형 영역/
Faster R-CNN은 1:1, 2:1, 1:2 3개비율 128,256,512 3개의크기 3x3=9개의 앵커박스사용
3. 9개의 앵커 박스에 객체가 존재하는지 존재 하지 않는지 점수를 2개의 클래스로 계산 (박스 분류)
하나의 슬라이딩윈도 -> 9개의 앵커 박스 -> 각 2개의 클래스 -> 전체 18개의 분류 클래스
슬라이딩 윈도의 크기가 H x W x C라면 1x1x18합성곱을 통해 HxWx18의 feature map
4. 앵커 박스로 객체의 위치 및 크기를 예측한것에 대해 박스 회귀를 적용해 상자의 크기와 위치를 조정
고정적인 비율과 그리드 -> 박스회귀로 경계 상자를 조정하는 작업 진행 (박스 회귀)
5. 관심 영역 선별
앵커 박스의 객체 여부와 위치와 크기를 계산한 다음
이 값을 활용하여 객체의 관심 영역을 계산한다.
    5-1) 박스 분류에서 계산된 점수를 정렬
    5-2) 정렬된 객체 점숫값이 0.7(or 특정 값) 이상인 앵커 박스를 추출
    5-3) 추출된 점수 값을 기준으로 상위 N개의 앵커 박스를 선택
    5-4) N개의 앵커 박스에만 박스 회귀를 적용
    5-5) 비최댓값 억제 연산을 적용해 중복되는 앵커박스 제거
    5-6) 남은 앵커 박스를 관심 영역으로 사용

{
박스회귀
박스 회귀 계층은 분류와 마찬가지로 1x1 Conv 연산 적용(1x1x36 ???)
박스 회귀는 36차원을 생성함 즉 4(36/9)개의 클래스가 존재,
4개의 클래스는 각각 x,y,w,h(위치와 크기)값을 출력한다
크기와 위치를 조정하기위해 네 개의 조정값(d_x, d_y, d_w, d_h)으로 회귀를 진행

앵커 박스 정보에 관한 수식
P(위첨자)i = (P(위첨자)i(아래첨자)x, P(위첨자)i(아래첨자)y, P(위첨자)i(아래첨자)w, P(위첨자)i(아래첨자)h)
※ 이건 원본 박스 표기방법
앵커 박스 조정 값에 관한 수식
d(위첨자)i = (d(아래첨자)x(P(위첨자)i), d(아래첨자)y(P(위첨자)i), d(아래첨자)w(P(위첨자)i), d(아래첨자)h(P(위첨자)i))
d-i = (d_x(P-i), d_y(P-i), d_w(P-i), d_h(P-i))
※ 각각의 d_변수()는 P에 해당하는 박스를 최대한 G에 가깝도록 이동시키는 함수

P(위첨자)i: i번째 앵커 박스의 정보/각각의 크기와 위치를 담은
d(위첨자)i: i앵커 박스의 조정값

https://better-tomorrow.tistory.com/entry/Bounding-box-regression
https://blog.firstpenguine.school/13
https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Faster-R-CNN-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0
https://better-tomorrow.tistory.com/entry/Bounding-box-regression

최종 앵커 박스 Ghat(위첨자)i에 관한 수식
Ghat-i_x = P-i_w * d_x(P-i) + P-i_x
Ghat-i_y = P-i_h * d_y(P-i) + P-i_y
Ghat-i_w = P-i_w * e^d_w(P-i)
Ghat-i_h = P-i_h * e^d_h(P-i)

※ x, y는 점이므로 이미지 크기에 상관없이 위치만 이동
※ w와 h는 크기에 비례하여 조정을 시켜주어야함
※ Gw와 Gh에 지수함수인이유는 작은 값으로 큰영향을 주어 미세조정을하고 비율을 조절하기위해
※ 조정된 앵커박스 G는 함수 d를 통과한 P이며 d(하나가 아니다!)는 학습이 가능하다!!!
}

모델 학습시 관심 영역을 선발하는법
1. 특징 맵 가장자리의 윈도에 할당된 앵커 박스는 관심 영역에서 제외
2. 실제 박스와 IOU가 0.7이상인 앵커 박스를 관심 영역으로 사용
3. 추출된 관심영역이 없는경우, 객체 점수값이 가장 큰 앵커박스를 관심영역으로 사용
4. 정답 박스와 IoU가 0.3(or 특정 값) 이하인 앵커 박스를 배경 영역으로 사용
※ IOU: (정답박스 ∩ 예측박스의 영역) / (정답박스 ∪ 예측박스의 영역)

517p
Faster R-CNN은 2단계로 구성되었다
1. 영역 제안 네트워크로 관심영역 추출하고 후보로 둠
2. 후보 영역들에 객체 탐지 네트워크로 객체를 탐지

후보 영역에 객체를 탐지할 때 관심 영역 풀링 등을 사용해
후보 영역을 고정된 크기로 변환하여 객체를 탐지한다 ->
Faster R-CNN은 7x7풀링을 사용한다. N개의 특징 맵을 모두 모으면
[N, 7, 7, 512]텐서가 된다고 한다? ->
이를 flatten하면 [N, 25088]텐서가 되며 이를 객체 탐지 네트워크에 전달한다 ->
객체 탐지 네트워크는 박스 분류와 박스 회귀 계층이 있다
클래스 C개에 대해 분류를 실시하면 [N, C]가 되고
회귀 계층 결과값이 클래스마다 생성되어 [N, C * 4]가 된다(??? x,y,w,h 말인가?)

최종 출력값은 클래스 예측 결과와 경계 상자가 된다
(!?! 언젠가 저 위에 5단계와 아래 2단계를 병합해서 이해를 해보자...)

517p
Faster R-CNN의 학습에는 정답 클래스와 박스 정보를 사용해서 비용(Loss?)을 계산한다)
복합 손실함수는 다음과 같다
L = 1/N_cls * L_cls + λ * 1/N_reg * L_reg
λ: 분류 손실 함수와 박스 손실 함수의 균형을 맞추기 위한 하이퍼파라미터이다
N_cls: 손실값을 계산할 때 사용하는 정규화 상수로, 미니 배치의 크기를 의미
N_reg: 앵커 박스의 수

일반적으로 N_cls = 2^8, N_reg는 2^8 * 3^2, λ초깃값을 10으로 사용

518p
분류 손실 함수는 잘아는 크로스 엔트로피로 구성됨
각 클래스별로 예측값의 교차엔트로피의 합을 더하는듯
단, 배경 클래스가 추가된다(관심 영역에 객체가 없을 경우에 판단됨)
학습 클래스에 반드시 존재해야함
수식은
L_cls = -Σ p*_i * log(p_i) + (1 - p*_i)*log(1-p_i)

분류에서 배경으로 나온경우 경계상자는 의미가 없으므로 0으로 설정

박스 회귀의 식은
L_reg = -Σ p*_i *  smooth_L1(t_i-t*_i)
※p*_i = 1이면 클래스가 배경이 아닌경우, 배경인경우는 0
t_i: 모델이 예측한 i번째 앵커 박스의 경계 상자 정보
t*_i: i번째 앵커 박스의 실젯값 경계 상자 정보

앵커 박스 예측값에 관한 수식
t_x = d_x(P) = (G^hat_x - P_x)/P_w
t_y = d_y(P) = (G^hat_y - P_y)/P_h
t_w = d_w(P) = log(G^hat_w/P_w)
t_h = d_h(P) = log(G^hat_h/P_h)
앵커 박스 실제값에 관한 수식
t*_x = d_x(P) = (G_x - P_x)/P_w
t*_y = d_y(P) = (G_y - P_y)/P_h
t*_w = d_w(P) = log(G_w/P_w)
t*_h = d_h(P) = log(G_h/P_h)

위 두 수식으로 계산된 값을 smooth_L1함수에 넣어 손실계산
smooth_L1는 다음과 같다
                 0.5x^2 if:|x| < 1
smooth_L1(x) = {
                 |x|-0.5 otherwise
이는 L1에 조건을 추가해 만든 함수로써, 이 함수는 입력값의 절댓값이 1보다 작은경우
L2를 적용하고절댃값이 1보다 큰경우 L1을 적용한다고한다 ???

L2 손실은 큰 값에 대해 손실이 지나치게 커지는 문제가 있고,
L1은 미분값이 계산안되는 지점이 있어 역전파가 불가능한 경우가 있다
두 손실함수를 섞어 단점을 보완???했???다고한다

