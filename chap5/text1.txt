자연어 처리의 문제점
모호성: 단어와 구가 사용 맥락에 따른 여러 의미를 갖게 되어 모호함
가변성: 사투리, 강세, 신조어, 작문 스타일로 인해 가변적
구조: 문장이나 구의 의미를 이해할 때 구문을 파악하여 의미를 해석한다.
알고리즘은 문장의 구조와 문법적 요소를 이해하여 의미를 추론하거나 분석할 수 있어야 한다.

이런 문제를 이해하고 구분할 수 있는 모델을 만드려면
말뭉치를 일정한 단위의 토큰으로 나눠야 한다

말뭉치는 말그대로 자연어 텍스트 데이터셋이고
토큰은 개별 단어나 문장 부호 등 말뭉치를 학습용으로 나눈 작은 단위이다
토큰화는 토큰으로 나누는 과정

토크나이저 구축방법의 종류
공백분할: 텍스트를 공백단위로 분리해 개별 단어로 토큰화
정규표현식: 정규 표현식으로 특정 패턴을 식별해 텍스트 분할
어휘 사전 적용: 사전에 정의된 단어 집합을 토큰으로 사용
머신러닝 활용: 데이터셋을 기반으로 토큰화하는 방법을 학습한 머신러닝을 적용

어휘 사전의 경우 사전에 정의된 단어를 활용해 토크나이저를 구축하지만
OOV(Out of Vocab)이라 하여 없는 단어가 존재할 수 있으며 이를 고려해야한다.

232p
큰 어휘 사전을 구축하면 학습 비용이 증대하고 차원의 저주에 빠질 수도 있다.
차원의 저주란? 학습 데이터의 차원이 증가할 수록 학습에 필요한 데이터가 증가하거나 모델의 성능이 저하되는 현상을 의미한다.

단어의 수가 많아질수록 벡터값이 거의 0을 가지는 희소 데이터로 표현된다
희소 데이터의 표현방법은 사전 단어 출현 빈도만 고려하므로 문장내 토큰간의 순서 관계를 잘 표현하지 못한다.

----------------------------------------
단어 토큰화: 띄워쓰기나 문장부호로 의미있는 단어를 구분하여 토큰화
글자 토큰화: 글자 단위로 토큰화
자모 토큰화: 한글 토큰화를 자소 단위로 분리, 완성형에서 조합형으로

글자 토큰화는 작은 단어 사전을 구축하고 이는
학습 때 자원을 아끼고
전체 말뭉치에서 각 단어를 더 자주 학습하는 장점이 있다고한다...

(자모 토큰화에서)개별 토큰은 의미가 없으므로 자연어 모델이 각 토큰의 의미를 조합해 결과를 도출해야한다.
토큰 조합 방식해 문장 생성이나 개체명 인식을 구현할 경우. 다의어나 동음이의어가 많은 도메인에서 구별이 어려워진다.
또한 모델 입력 시퀀스의 길이가 길어지면 연산량이 증가하는 단점이 있다.
※개체명 인식: 자연어 내에서 사전에 정의된 이름을 가진 개체를 인식하고 추출하는 자연어처리의 한 분야
----------------------------------------
형태소 토큰화: 텍스트를 의미가 있는 최소 단위로 나누는 토큰화 방식

교착어는 접사를 붙여 의미를 완성해나가는 언어이며
한국어는 교착어이다
교착어는 형태소 토큰화를 중요하게 생각한다

형태소는 또다시 스스로 의미를 지니는 자렵 형태소와 그렇지 않은 의존 형태소로 나뉜다

237p
형태소 어휘 사전: 자연어 처리에서 사용되는 단어의 집합 중에서도 각 단어의 형태소 정보를 포함하는 사전
이를 통해 문맥을 고려할 수 있기에 더욱 정확한 분석이 가능해진다.

238p
KoNLPy/NLTK/spaCy 라이브러리를 활용해 형태소 단위의 토큰화가 가능하다

241p
Okt보단 Kkma가 좀 더 자세하게 분리하는듯

242p
NLTK의 품사 태깅에는 Punkt나 Averaged Perception Tagger

245p
spaCy 라이브러리는
pip install spacy말고도
python -m spacy download en_core_web_sm

얘는 NLTK와 다르게 머신러닝 기반의 자연어 처리 라이브러리
GPU 가속을 사용하고 리소스 많이씀
24개국어 사전
GPU가속 및 다른 언어를 사용하고자 한다면 https://spacy.io/usage

사전학습된모델은 spaCy load로 불러와서 분리하는듯

spaCy는 객체 지향적으로 구현되었고, doc객체에 저장한다
doc은 다시 여러 token 객체로 되어있으며
token 객체에는 자연어 처리에 필요한 정보들이 있다.
nlp 인스턴스에 문장을 입력하면 doc객체를 반환한다
doc아래 token아래 tag_나 pos_같은 속성해 값을 확인한다.

토큰객체에는 pos_나 tag말고도 원본 text나
토큰 사이의 공백을 포함하는 텍스트 데이터, 벡터, 벡터 노름 등의 속성이 포함되어있다고한다.

248p
외래어, 띄워쓰기 오류, 오탈자등은 형태소 분석을 어렵게 한다
또한, 전문용어나 고유어가 많은 데이터를 처리할 때 약점을 보인다.
모르는 단어를 적절히 나누는 것에 취약하며
어휘사전의 크기를 크게 만들고 OOV에 대응하기 어렵게한다.
예를들어 신조어, '돈쭐내다'의 경우 '돈쭐' '돈쭐날' 등이 새로 추가되어 어휘 사전의 크기를 크게 만든다

이를 해결할 방법으로 하위 단어 토큰화가 있다
Reinforcement는 Rein/force/ment로 나눌 수 있다
처리 속도 증가와 OOV/신조어/은어/고유어 문제의 완화가 가능하다고한다... 어떻게?

250p
하위 단어 토큰화: 하나의 단어를 빈번하게 사용되는 하위 단어의 조합으로 나누어 토큰화
바이트 페어 인코딩/워드피스/유니그램 모델 등이 있다

-------------------------------------------------
바이트 페어 인코딩: 텍스트 데이터에서 가장 빈번하게 등장하는 글자 쌍의 조합을 찾아 부호화하는 압축 알고리즘
연속된 글자 쌍이 더 이상 나타나지 않거나,
정해진 어휘 사전 크기에 도달할 때까지 조합 탐지와 부화하를 반복하며
이 과정에서 자주 등장하는 단어는 하나의 토큰으로 토큰화,
덜 등장하는 단어는 여러 토큰의 조합으로 표현된다

예시
abracadabra->AracadAra->ABcadAB->CcadC
ab글자쌍이 빈도수가 높으므로 A로 압축(압축하는 문자는 딱히 뭐든 상관 x)
같은 방식으로 ra가 제일 많으므로 ra를 B로 압축
치환된 데이터에서 AB가 제일 많으므로 AB를 C로 압축

251p
말뭉치에서 하는방법
빈도사전 ('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)
어휘사전 ['low', 'lower', 'newest', 'widest']

1단계 빈도 사전내 모든 단어를 글자 단위로 나눈다

빈도사전('l', 'o', 'w', 5), ('l','o','w','e','r', 2), ('n','e','w','e','s','t', 6), ('w','i','d','e','s','t', 3)
어휘사전 ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']

2단계 가장 자주 등장한거 'e', 's'쌍 9회
빈도사전('l', 'o', 'w', 5), ('l','o','w','e', 'r', 2), ('n','e','w','es','t', 6), ('w','i','d','es','t', 3)
어휘사전 ['d', 'e', 'i', 'l', 'n', 'o', 'r', 't', 'w', 'es']

3단계 가장 자주 등장한거 'es', 't' 9회 'est로 병합'
빈도사전('l', 'o', 'w', 5), ('l','o','w','e','r', 2), ('n','e','w','est', 6), ('w','i','d','est', 3)
어휘사전 ['d', 'e', 'i', 'l', 'n', 'o', 'r', 't', 'w', 'es', 'est']

10번 반복 이후는
빈도사전 ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', est', 3)
어휘사전 ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid', widest']

이런식으로  'newer', 'wider', 'lowest'와 같이 기존 말뭉치에 등장하지 않았던 단어가 입력되지 않더라고
OOV처리가 아닌 'new e r', 'wid e r', 'low est'와 같이 하위단어로 토큰화 가능

-------------------------------------------------
Sentencepiece 라이브러리로 학습되는 토크나이저 사용
Korpora는 오픈소스 말뭉치 다운로드와 전처리용 라이브러리

257p
워드피스 토크나이저는 바이트 페어 인코딩 토크나이저와 유사하지만
빈도가 아닌 확률 기반으로 글자 쌍을 병합

모델이 새로운 하위 단어를 생성할 때
이전 하위 단어와 함께 나타날 확률을 계산해
가장 높은 확률을 가진 하위 단어를 선택
선택된 하위 단어는 이후로 더 높은 확률로 선택되고,
모델이 좀 더 정확한 하위 단어로 분리 가능
각 글자 쌍의 점수는
score = f(x, y)/(f(x) * f(y))
※ f는 빈도를 나타내는 함수
※ f(x, y)는 x와 y가 조합된 글자 쌍의 빈도를 의미

빈도사전('l', 'o', 'w', 5), ('l','o','w','e','r', 2), ('n','e','w','e','s','t', 6), ('w','i','d','e','s','t', 3)
어휘사전 ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']

여기서 e와 s는 9번, e는 17, s는 9번 등장하므로 9/(17*9) = 0.06..이 점수가 된다
i와 d의 쌍은 3번 등장하지만 i는 3번, d는 3번 등장하므로 3/(3*3) = 0.33...이 점수고 가장 높다
e와 s대신 i와 d를 병합해서

빈도사전('l', 'o', 'w', 5), ('l','o','w','e','r', 2), ('n','e','w','e','s','t', 6), ('w','id','e','s','t', 3)
어휘사전 ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'id']

다음 과정에서는 l과 o의 쌍이  3번, 각각 7번 나오므로 7/(7*7) = 0.14...로 가장높으므로 병합
이런식으로 하는듯
위 과정을 반복해서 쌍이 없거나 정해진 숫자만큼 어휘사전이 도달할 때까지 학습한다.

259p
허깅 페이스의 토크나이저스 라이브러리를 실습