GPT는 트랜스포머 디코더를 쌓은것
인코더는 입력의 특징 추출을 하고
디코더는 입력 출력 문장 간의 관계를 이해하여 출력 문장을 생성한다
따라서, 디코더를 쌓아 모델을 구성하는것이 자연어 생성에서 더 높은 성능을 발휘한다

GPT 3.5는 인간의 피드백을 이용한 강화 학습을 한다
모델이 좋은 결과를 생성해 좋은 평가를 받으면 보상을 주어 모델을 학습시킨다

GPT-4는 멀티모달 하다

390p
GPT-1은 트랜스포머 구조를 바탕으로한 단방향 언어 모델이다
단방향 언어 모델은 텍스를 생성할 때 현재 위치에서 이전 단어들에 대한 정보만을 참고하여 예측함

학습은 4.5GB 크기의 BookCorpus 데이터세트를 사용한다.
입력 문장을 임의의 위치까지 보여주고 뒤에 이어지는 문장을 모델이 자동으로 예측하게 한다.
비지도 학습이고 별도의 레이블이 없다

--- ??? 이거 뭔소리임??? ---
입력 문장을 일정한 길이의 블록으로 나누어 처리하고
각 블록 내에서 첫 번째 토큰부터 순서대로 다음 토큰을 예측하게 한다.
모델이 장기적인 문맥을 이해할 수 있게 한다.

392p
미세 조정을 위한 다운스트림 작업에서도 GPT-1은
각 작업에 따라 입출력 구조를 바꾸지 않고
언어 모델을 보조 학습에 사용할 수 있다.

※ 보조 학습: 모델이 주어진 주요학습 외에도 다른 부가적인 작업을 하기 위해 학습하는것
분류, 명제 추론, 문장 유사도, 다중 선택 등등이 주어졌음

대신 손실 함수가 추가로 필요함

보조 학습으로 언어 모델을 개선하는 동시에&
다운스트림 작업의 목표를 달성하기 위해 필요한 정보를 학습할 수 있다
...???

다운 스트림 작업에서 사용하는 특수토큰에는
문장시작인 <start>, 구분자 <delim>, 문장끝인 <extract>

393p
GPT2는 디코더가 1의 4배(48개) 길이 입력은 512->1024로 2배 늘어남
학습 데이터는 미국 웹사이트의 크롤링된 40GB사용

!또한 GPT-2는 제로-샷 학습가능
※ Zero-Shot Leanring(ZSL): 모델이 학습 과정에서 본 적 없는 데이터에서 새로운 클래스를 인식할 수 있도록 하는 기술
'GPT는 놀라워요 = GPT is amazing'과 같은 형식의 문장을 학습시킨후
'번역 한글 문장 = '을 입력하면 번역된 영문이 출력되는 형식

394p
GPT-3는 GPT-2과 모델 구조가 비슷하지만 매개 변수가 116배많다
96헤드 멀티 헤드 어텐션, 디코더 층 개수도 96개
희소 어텐션을 일반 어텐션을 섞어 사용한다(??? 희소 행렬곱같은걸 쓰나? 예전에, Sparse Matrix를 빠르게 다루는 알고리즘을 배운적있었다)
임베딩 크기가 1,600

학습 데이터셋은 45TB
크롤링, 위키백과, 서적 등에서 수집했다고 한다

토큰 길이는 2048개까지 가능

다운스트림 방식으로 학습 x
그러나 다운스트림 작업도 가능
일반화 능력이 강하나
질의응답, 수학문제 풀기, 자연어 추론 가능(기술적 한계 극복)

일반화 능력이 높음 그러나 사람과  비교하여 볼 때
인간적인 이해력이나 상식적인 추론 능력이 모자람
/생성한 텍스트가 항상 사실임을 필요로 하지 않고,
/저작권 문제가 있음

396p
GBT 3.5는 GPT-3의 구조를 따르나
매개변수를 줄이고 RLHF를 통해 인간피드백으로 피드백을한다

RLHF는 사람이 관여해서 학습함
사람이 작성한 답변과 모델이 작성한 답변 비교해서 보상으로 강화학습
사람이 랭킹을 부여해서 리워드함, 리워드도 모델 만듬
GPT가 만든걸 리워드 모델로 평가해줌

시드에 따라 여러 다른 답변 가능함 GPT도

사람이 랭킹할 때, 그 정보를 가지고 강화학습에 필요한
리워드를 책정하는 모델을 만들고
모델로 GPT의 답변을 평가하여 모델을 최적화

이 과정을 통해 자연스러워짐

397p
멀티모달하며
2^15개의 토큰 입력가능

399p
GPT-2의 사용례이다.
허깅페이스쪽 라이브러리 인듯
transformers.pipeline 클래스는 입력된 작업에 모델로 적합한 파이프라인 구축
매개변수 task는 할일, 모델은 모델(여기선 gpt2)
max_length는 길이, num_return_sequences는 생성할 개수
{ pad_token_id를 eos_token_id로 왜 쓰는가?
패딩 토큰은 모델의 자유생성 여부를 결정, 입력기반으로 자유롭게 다음
단어나 문장을 생성한다고하는데 글쎄???

https://littlefoxdiary.tistory.com/46
https://velog.io/@castle_mi/GPT
pad_token_id를 eos로 설정하면 개방형 생성이라고 한다
또한, warning을 피하기 위해서라고한다???
저 옵션을 안주면 대체로 길게 나오긴 하는데 뭔지 잘모르겠음
https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id
이게 뭔소리람...
}

405p
걍 타이핑은 생략함
문장 분류 모델 클래스로 학습하면 내부적으로 손실 계산해서 반환
train 함수로 모델 학습시 손실은 모델 출력값의 loss 속성을 사용

모델 평가는 교차 엔트로피 함수로
모델의 반환 손실값이 아닌 로짓/레이블로 계산

calc_accuracy 함수로 모델의 정확도 계산
모델의 예측 결과와 실제 레이블을 비교하여 정확도 계산하고 반환

각 에포크에서 학습 손실, 검증 손실, 검증 정확도를 계산
검증 손실이 제일 낮을걸 체크포인트에서 저장
best는 제일 잘된거

최적의 값 best로 테스트해서 평가