GPT는 트랜스포머 디코더를 쌓은것
인코더는 입력의 특징 추출을 하고
디코더는 입력 출력 문장 간의 관계를 이해하여 출력 문장을 생성한다
따라서, 디코더를 쌓아 모델을 구성하는것이 자연어 생성에서 더 높은 성능을 발휘한다

GPT 3.5는 인간의 피드백을 이용한 강화 학습을 한다
모델이 좋은 결과를 생성해 좋은 평가를 받으면 보상을 주어 모델을 학습시킨다

GPT-4는 멀티모달 하다

390p
GPT-1은 트랜스포머 구조를 바탕으로한 단방향 언어 모델이다
단방향 언어 모델은 텍스를 생성할 때 현재 위치에서 이전 단어들에 대한 정보만을 참고하여 예측함

학습은 4.5GB 크기의 BookCorpus 데이터세트를 사용한다.
입력 문장을 임의의 위치까지 보여주고 뒤에 이어지는 문장을 모델이 자동으로 예측하게 한다.
비지도 학습이고 별도의 레이블이 없다

--- ??? 이거 뭔소리임??? ---
입력 문장을 일정한 길이의 블록으로 나누어 처리하고
각 블록 내에서 첫 번째 토큰부터 순서대로 다음 토큰을 예측하게 한다.
모델이 장기적인 문맥을 이해할 수 있게 한다.

392p
미세 조정을 위한 다운스트림 작업에서도 GPT-1은
각 작업에 따라 입출력 구조를 바꾸지 않고
언어 모델을 보조 학습에 사용할 수 있다.

※ 보조 학습: 모델이 주어진 주요학습 외에도 다른 부가적인 작업을 하기 위해 학습하는것
분류, 명제 추론, 문장 유사도, 다중 선택 등등이 주어졌음

대신 손실 함수가 추가로 필요함

보조 학습으로 언어 모델을 개선하는 동시에&
다운스트림 작업의 목표를 달성하기 위해 필요한 정보를 학습할 수 있다
...???

다운 스트림 작업에서 사용하는 특수토큰에는
문장시작인 <start>, 구분자 <delim>, 문장끝인 <extract>

393p
GPT2는 디코더가 1의 4배(48개) 길이 입력은 512->1024로 2배 늘어남
학습 데이터는 미국 웹사이트의 크롤링된 40GB사용

!또한 GPT-2는 제로-샷 학습가능
※ Zero-Shot Leanring(ZSL): 모델이 학습 과정에서 본 적 없는 데이터에서 새로운 클래스를 인식할 수 있도록 하는 기술
'GPT는 놀라워요 = GPT is amazing'과 같은 형식의 문장을 학습시킨후
'번역 한글 문장 = '을 입력하면 번역된 영문이 출력되는 형식

394p
GPT-3는 GPT-2과 모델 구조가 비슷하지만 매개 변수가 116배많다
96헤드 멀티 헤드 어텐션, 디코더 층 개수도 96개
희소 어텐션을 일반 어텐션을 섞어 사용한다(??? 희소 행렬곱같은걸 쓰나? 예전에, Sparse Matrix를 빠르게 다루는 알고리즘을 배운적있었다)
임베딩 크기가 1,600

학습 데이터셋은 45TB
크롤링, 위키백과, 서적 등에서 수집했다고 한다

토큰 길이는 2048개까지 가능

다운스트림 방식으로 학습 x
그러나 다운스트림 작업도 가능
일반화 능력이 강하나
질의응답, 수학문제 풀기, 자연어 추론 가능(기술적 한계 극복)

일반화 능력이 높음 그러나 사람과  비교하여 볼 때
인간적인 이해력이나 상식적인 추론 능력이 모자람
/생성한 텍스트가 항상 사실임을 필요로 하지 않고,
/저작권 문제가 있음

396p
GBT 3.5는 GPT-3의 구조를 따르나
매개변수를 줄이고 RLHF를 통해 인간피드백으로 피드백을한다

RLHF는 사람이 관여해서 학습함
사람이 작성한 답변과 모델이 작성한 답변 비교해서 보상으로 강화학습
사람이 랭킹을 부여해서 리워드함, 리워드도 모델 만듬
GPT가 만든걸 리워드 모델로 평가해줌

시드에 따라 여러 다른 답변 가능함 GPT도

사람이 랭킹할 때, 그 정보를 가지고 강화학습에 필요한
리워드를 책정하는 모델을 만들고
모델로 GPT의 답변을 평가하여 모델을 최적화

이 과정을 통해 자연스러워짐

397p
멀티모달하며
2^15개의 토큰 입력가능

399p
GPT-2의 사용례이다.
허깅페이스쪽 라이브러리 인듯
transformers.pipeline 클래스는 입력된 작업에 모델로 적합한 파이프라인 구축
매개변수 task는 할일, 모델은 모델(여기선 gpt2)
max_length는 길이, num_return_sequences는 생성할 개수
{ pad_token_id를 eos_token_id로 왜 쓰는가?
패딩 토큰은 모델의 자유생성 여부를 결정, 입력기반으로 자유롭게 다음
단어나 문장을 생성한다고하는데 글쎄???

https://littlefoxdiary.tistory.com/46
https://velog.io/@castle_mi/GPT
pad_token_id를 eos로 설정하면 개방형 생성이라고 한다
또한, warning을 피하기 위해서라고한다???
저 옵션을 안주면 대체로 길게 나오긴 하는데 뭔지 잘모르겠음
https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id
이게 뭔소리람...
}

405p
걍 타이핑은 생략함
문장 분류 모델 클래스로 학습하면 내부적으로 손실 계산해서 반환
train 함수로 모델 학습시 손실은 모델 출력값의 loss 속성을 사용

모델 평가는 교차 엔트로피 함수로
모델의 반환 손실값이 아닌 로짓/레이블로 계산

calc_accuracy 함수로 모델의 정확도 계산
모델의 예측 결과와 실제 레이블을 비교하여 정확도 계산하고 반환

각 에포크에서 학습 손실, 검증 손실, 검증 정확도를 계산
검증 손실이 제일 낮을걸 체크포인트에서 저장
best는 제일 잘된거

최적의 값 best로 테스트해서 평가

8550개의 소규모로 잘된다고한다

-----------------------------------------------------------------------------

406p
BERT는 양방향 인코더 사용, 이전과 이후 단어 모두 참조
대규모 데이터셋으로 사전학습되어있음
트랜스포머 인코더 기반(그림에는 트랜스포머 인코더에서 즉시 선형 임베딩으로 간다)

https://happy-obok.tistory.com/23
pretrain 하는 과정에서

(Masked Language Model)
무작위 15%를 [MASK] 토큰으로 바꾸고 학습한다고한다
이 과정에서 양방향 문맥 정보를 참고한다.
15%에서 80%는 [MASK], 10%는 무작위 단어로 바꾸고, 10%은 그대로 둔다.

(Next Sentence Prediction)
또한 두 문장의 관계를 이해하기 위해 모델에 두 문장을 동시에 제공하여
50%는 실제 연결되는 문장, 50%는 말뭉치내의 무작위 문장이다.
다음 문장으로 오는지 IsNext여부를 softmax를 할당하여 학습한다.

[CLS]는 문장 시작토큰
[SEP]은 두 개의 문장 구분용 토큰
[MASK]는 말그대로 마스킹

임베딩은 토큰임베딩, 위치임베딩에 문장 구분 임베딩의 합으로 이뤄진다고한다
토큰 임베딩은 Word piece 방식을 쓴다고한다
위치 임베딩은 Transformer Sigsoid 함수를 이용한걸 변형해서 쓴다고한다???
문장 구분 임베딩은 토큰에다 속하는 문장별로 다른 마스크를 준다(0, 1같은)

손실 함수 최소화를 위해 MLM과 NSP를 같이 쓴다고한다

412p
토크나이저는 사전 학습된 Bert 토크나이저를 사용

416p
인코더 블록은 12개 사용한다 BERT에서
구조 중 pooler는 [CLS] 토큰 벡터를 한 번 더 비 선형 변환을 수행하기 위해
선형 변환과, 비선형 변환인 Tanh 함수를 사용한다 <- ???
https://shyu0522.tistory.com/81 <- 여길 참고하자

-----------------------------------------------------------------------------

417p
BART는 트랜스포머 인코더-디코더 구조가 결합된 Seq2Seq 구조
양방향 자기회귀 트랜스포머(Bidirectional Auto-Regressive Transformer)
노이즈 제거 오토인코더로 사전 학습된다.

※ 노이즈 제거 오토인코더: 입력 데이터에 잡음을 추가하고 잡음이 없는 원본 데이터로 복원하는 방식으로 학습하는 인코더

BERT의 마스킹과 GPT의 다음단어 예측 학습 방법을 일반화해서 학습한다.

418p의 그림 참조
노이즈가 추가된 텍스트를 인코더에 입력->원본 텍스트를 디코더에 입력
디코더가 원본 텍스트를 생성할 수 있게 학습한다.
이를 통해 문장 구조와 의미를 보존하면서 다양한 변형 학습 가능

순방향 정보만 인식가능한/문장 생성 분야에서 뛰어나지 않던 단점을 개선??? 했다고함

트랜스포머와 달리 인코더의 모든 계층과
디코더의 모든 계층 사이의 어텐션을 수행하지는 않음(그랬던가?)
attention.txt에 인코더 매 시점의 은닉 상태를 모두 이용한다고했다
BART의 인코더는 BERT처럼 마지막 계층으로 의미를 짜내는 모양인듯

419p
BART는 마스킹 말고도 노이즈 기법이 다양함
토큰 마스킹:MLM임
중요단어파악/문맥이해/중요정보추출 성능 향상

토큰 삭제: 무작위 삭제, 뭐가 삭제됐나 맞춤
모델의 일반화 성능 향상

문장 순열: 온점으로 문장을 끊고 순서를 섞음, 원래 문장 순서를 맞춰야함
순서 다른 문장 처리 쉬워짐

문서 회전: 임의의 토큰으로 문서가 시작, 문서의 순서는 그대로
문서의 시작점 인식가능

텍스트 채우기: 몇 개의 토큰을 하나의 구간으로 묶고
일부 구간을 마스크 토큰으로 대체
묶은 구간의 길이는 0부터 임의의 값으로 설정가능하고
일반적으로 푸아송 분포로 구간 나눔
모델은 연속된 마스크 토큰을 복구하되, 마스킹 안된 토큰도 구분해야함
모델이 누락한 단어를 예측함으로써, 더 많은 정보를 활용하여 입력 문장을 잘 이해

422p
{
BART 다운스트림 키워드로 더 자세한걸 확인해봐야겠다
책의 설명은 너무 얼렁뚱땅인것같은데?
https://chloelab.tistory.com/34
- ㅅㅂ -

BART에 각기 다른 다운스트림에는 각기 다른 미세 조정이 필요하다

문장 분류는 입력 문장을 인코더-디코더에 동일하게 입력하고
디코더의 마지막 토큰 은닉 상태를 선형 분류기의 입력값으로 사용
BERT의 CLS와 비슷하지만, 전체 입력과 어텐션 연산이 적용된 은닉 상태를 사용

토큰 분류도 인코더와 디코더에 동일하게 문장을 입력
디코더의 각 시점별 마지막 은닉 상태를 토큰 분류기의 입력값으로 사용

추상적 질의 응답과 문장 요약과 같은 작업은 걍하는듯? 디코더를 딱히 건들 필요도x
학습 방식과 유사함

기계 번역은 기계 번역용 인코더를 추가한???다
추가된 인코더는 기존의 단어 사전 사용 x
디코더는 사전학습된 가중치와 단어사전 사용
학습은 두 단계로 이루어지는데 첫 번째는
새로 추가된 트랜스포머 인코더의 가중치와 위치 임베딩, 셀프 어텐션 입력 행렬 가중치만 갱신
두 번째는 신경망의 가중치를 작은 반복으로 모두 갱신

입력 문장의 전체적인 의미를 고려하고
문장 내부의 토큰 사이 상관관계를 파악해 문장의 의미를 더욱 정확하게 이해할 수 있다
???
}

428p
BART는 인코더와 디코더가 동일한 임베딩 계층을 사용(shared 계층)
layernorm_embedding 임베딩 벡터의 마지막 차원에 대해 계층정규화를 수행한다.
인코더의 마지막 계층의 출력값은 디코더의 모든 계층과 어텐션 연산을 수행
마지막 디코더 계층의 출력값은 출력 크기가 단어 사전의 크기인 완전 연결 계층(이게 아마 선형 임베딩인듯?)을 통과해
언어모델을 형성

429p
모델 평가 방법에서 ROUGE 점수라는걸 사용한다
※ ROUGE-N(Recall-Oriented Understudy for Gisting Evaluation + N-gram): 생성된 요약문과 정답 요약문이 얼마나 유사한지
N-gram 정밀도와 재현율을 이용해 평가하는 지표

유니그램을 사용하면 ROUGE-1, 바이그램은 ROUGE-2, ... N-gram은 ROUGE-N인 식이며
재현율: (생성된 문장과 정답 문장에 등장한 토큰)/(정답 문장에 등장한 토큰)
정밀도: (생성된 문장과 정답 문장에 등장한 토큰)/(생성된 문장에 등장한 토큰)

ROUGE-L: 생성된 요약문과 정답 요약문 사이에서 최장 공통부분 수열 기반(LCS)의 통계 방식
ROUGE-LSUM: 위 ROUGE-L의 변형, 텍스트 내의 개행 문자를 문장 경계로 인식하고
각 문장 쌍에 대해 LCS를 계산하고
각 문장 쌍의 LCS를 합집합 하여 중복된 부분을 제거한 후, 길이를 계산(= union-LCS)
ROUGE-W: 연속된 LCS에 가중치를 부여하여 계산
LCS의 길이 말고도 문자열 내의 단어에 가중치를 부여해 평가

ROUGE-LSUM은 정확성과 완전성을 모두 반영할 수 있는 지표로 사용됨
ROUGE-W는 단어 간 유사도를 고려해 요약 문장이 입력 문장의 의미를 더욱 잘 전달하는지 평가하는데 유용