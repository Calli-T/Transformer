370p
그래서 yield가 뭐냐?
https://www.daleseo.com/python-yield/

Multi30k의 데이터셋은 영어-독어 번역 데이터셋이 있다

372p
토큰 임베딩 클래스에서 d_model^0.5 만큼으로 곱하는 이유가 무엇인가???

2_2의 코드에서
Seq2SeqTransformer 클래스는 TokenEmbedding 클래스로
소스와 타겟 데이터를 임력 임베딩으로 변환

이를 PositionalEncoding 클래스로 입력 임베딩에 위치 인코딩 첨가

트랜스포머 블록(= self.transformer)는 파이토치의 트랜스포머 클래스
적용
인코더와 디코더는 layers 변수의 값으로 구성

순방향 메서드의 마지막에 적용되는 것은 generator이며, 마지막
트랜스포머 디코더 블록에서 산출되는 벡터를 선형변환하여 어휘사전의
logits 제작

---
트랜스포머 클래스에서
d_model은 입/출력 차원수
nhead는 헤드 수
num_encoder_layers와 num_decoder_layers는 이름 그대로
dim_forward는 순방향 신경망의 은닉층 크기
드롭아웃은 드롭아웃
계층 정규화 입실론은 계층 정규화의 분모에 더해지는 아주 작은 값

기본값
transformer = torch.nn.Transformer(d_model=512,
                                   nhead=8,
                                   num_encoder_layers=6,
                                   num_decoder_layers=6,
                                   dim_feedforward=2048,
                                   dropout=0.1,
                                   activation=torch.nn.functional.relu,
                                   layer_norm_eps=1e-05,
                                    )
---
트랜스포머의 순방향 메서드의 매개변수는 다음과 같다
src, tgt는 인코더와 디코더의 시퀀스로, [N, S, d] 형태의 데이터를 입력받는다
[시퀀스 길이, 배치 크기, 임베딩 차원]

src_mask와 tgt_mask는 소스와 타겟 시퀀스의 마스크로
[소스(타깃) 시퀀스 길이, 시퀀스 길이]

마스크 값이 0이면 모든 입력 단어가 동일한 가중치,
1이라면 모든 단어의 가중치가 0으로 설정되어 어탠션 x
-inf라면 마스킹된 위치의 정보를 모델이 무시한다

진짜 뭔소리야
{
메모리 마스크는 인코더 출력의 마스크로
[타겟 시퀀스 길이, 소스 시퀀스 길이]의 형태이며
값이 0인 위치에서는 어텐션 x ???

소스, 타깃, 메모리 키 패딩 마스크는 소스 타깃, 메모리 시퀀스에 대한
패딩 마스크, [배치 크기, 소스(타깃) 시퀀스 길이] 형태의 데이터를
입력 받으며
메모리 키 패딩 마스크는 소스 키 패딩 마스크와 동일한 형태의 데이터를
입력받는다 ???

키 패딩 마스크: 입력 시퀀스에서 패딩 토큰이 위치한 부분을 가리는 이진
마스크, 패딩 토큰이 실제 의미를 가지지 않는 것으로 간주되어 해당
위치의 어텐션 결과에 대한 가중치를 0
}

순방향 메서드는 인스턴스 설정과 입력 시퀀스로 타깃 시퀀스의
임베딩 텐서를 반환하며 [타깃 시퀀스 길이, 배치 길이, 임베딩 차원]을 반환

현재 클래스는 어휘 사전에 대한 로짓을 생성하므로
임베딩 차원이 타깃 데이터의 어휘 사전 크기로 변경됨

-------------------------------------------------
model_architecture 코드에서
PAD_IDX는 손실함수에서 무시하도록 지정해준다
