355p
트랜스포머는 모델은 입력 시퀀스를 병렬로 처리하는 기능을 가진다
셀프 어텐션을 기반으로 하며, 이는 순차 처리나 반복 연결에 의존하지 않고
입력 토큰 간의 관계를 직접 처리하고 이해하도록한다.
모델이 재귀나 합성곱 연산 없이 입력 토큰 간의 관계를 직접 모델링한다.

대규모 데이터셋에 효율적이라고 한다
언어 모델링과 텍스트 분류에 효과적
기계 번역, 언어 모델링, 텍스트 요약과 같은
장기적인 종속성을 포함하는 작업에 주로 사용

356p
트랜스포머 기반 모델의 학습 방식은 2개가 있다.

오토인코딩방식: 랜덤하게 문장의 일부를 빈칸 토큰으로 만들고
해당 빈칸에 어떤 단어가 적절할지 예측하는 작업(Task) 수행
예측할 때 양옆의 토큰을 참조하므로 양방향구조, 이를 인코더라고 한다

자기 회귀 방식: 예측되는 토큰의 왼쪽에 있는 토큰들만 참조
단방향이고 이를 디코더라고한다

357p
※ 트랜스포머 해석
https://analytics4everything.tistory.com/150

트랜스포머 모델은 어텐션 메커니즘을 사용하며
이를 이용해서 시퀀스 임베딩을 표현한다.

인코더와 디코더 간의 상호작용으로
입력 시퀀스의 중요한 부분에 초점을 맞추어 문백을 이해하고
적절한 출력을 생성한다.

인코더는 입력 시퀀스를 임베딩하여 고차원 벡터로 변환
디코더는 인코더의 출력을 입력으로 받아 출력 시퀀스를 생성
이 때, 어텐션 메커니즘은 인코더와 디코더 단어 사이의 상관관계를 계산하여 중요한 정보에 집중
입력 시퀀스의 각 단어가 출력 시퀀스의 어떤 단어와 관련이 있는지를 파악,
번역이나 요약문 생성등의 작업 수행

358p
인코더와 디코더는 두 부분으로 되어있고
이는 각각 N개의 트랜스포머 블록으로 되어있다.
이 블록은 멀티 헤드 어텐션과, 순방향 신경망으로 이루어져있다

멀티헤드 어텐션: 입력 시퀀스에서 쿼리, 키, 값 벡터를 정의하여
입력 시퀀스들의 관계를 셀프 어텐션 하는 벡터 표현 방법이다