0220
전반적인 설명
https://heekangpark.github.io/nlp/attention#kramdown_seq2seq--%EC%96%B4%ED%85%90%EC%85%98-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EB%8F%99%EC%9E%91-%EB%B0%A9%EB%B2%95

어텐션 종류별로
https://velog.io/@guide333/Attention-%EC%A0%95%EB%A6%AC

또다른 설명
https://moondol-ai.tistory.com/316

또또다른 설명
https://hyen4110.tistory.com/31

1. Seq2Seq 모델이란
Many-to-Many(다-대-다) 모델의 일종으로 (RNN 기반이라한다)
번역, 요약과 같이 시퀀스를 받아 시퀀스를 출력한다

2-1 Seq2Seq 모델의 동작 방식
인코더와 디코더로 나누어져있다
인코더는 문맥벡터로 시퀀스를 변환
디코더는 이를 받아 출력 시퀀스를 출력

1) 인코더의 은닉 상태는 적절한 값으로 초기화된다
2) 매 시점에 원문의 단어(의 임베딩)이 입력한다
3) 인코더는 이를 사용해 은닉 상태를 업데이트
4) 2,3의 과정을 모든 입력 시퀀스에 반복
5) 최종 은닉 상태(= context vector)를 디코더로 넘김
6) context vector로 자신의 은닉상태 초기화
7) 최초 시점에는 <sos> 토큰을 입력으로 받음
8) 다음 단어예측
9) 예측한 단어를 입력으로 받고 은닉상태 갱신
10) 정해진횟수 혹은 <eos> 토큰 나올때까지 8,9 반복

※ sos는 start of sequence, eos는 end of sequence

2-2. Seq2Seq 모델의 학습 방법
교사 강요 방식으로 진행,
디코더의 입력값으로 출력 단어가 아닌
실제 정답 단어를 입력
(추측? 출력이 나올텐데, 이를 다시 넣는게 아니고 실제 단어를 입력하되
그 과정에서 출력과 실제 단어를 비교하여 '학습'하는게 아닌가???)

3. Seq2Seq 모델의 한계
입력시퀀스를 하나의 고정길이 벡터에 압축요약 -> 정보의 손실발생
RNN 기반이라 기울기 소실과 폭주

4. Attention Mechanism이란
Attention Mechanism이란 고정길이벡터에 모든 정보를 담아야하는 인코더의 부담을 덜기 위한 것이다.
디코더에서 다음 단어 예측을 예측을 위해 인코더의 마지막 은닉 상태(=context vector) 뿐아니라
인코더의 매 시점의 은닉 상태를 모두 이용하는것이다.

모든 시점을 사용하는것은 다음과 같은 가정에 기반한다.
'디코더가 단어 X를 출력하기 직전의 디코더의 은닉 상태는
인코더가 입력 시퀀스에서 X와 연관이 깊은 단어를 읽은 직후의 은닉 상태와 유사할 것이다'

따라서, 인코더 은닉 상태의 여러 시점에서
어디에 좀 더 집중(= Attention)하는지 아는것이 중요하다.
이를위해 다음과 같은 과정을 거친다
1) 현재 디코더의 은닉 상태와 은닉 상태간 유사도를 계산한다
2) 이 유사도를 확률의 형태로 변경한다
3) 그 값으로 인코더 은닉 상태의 가중합을 구해 보정된 context vector를 구한다
4) 보정된 context vector로 다음 단어를 예측한다

5. Seq2Seq + Attention 모델의 동작 방법
시점 t에서 디코더의 출력단어 x_t+1를 예측하는 방법은
1) 스코어 함수 f를 이용하여
인코더 은닉 상태 h_k와
디코더의 이전 은닉 상태 s_t의
유사도를 모은 벡터e를 계산한다
2) Attention Distribution α를 구한다.
α = softmax(e)
소프트맥스함수를 거치면 확률 형태로 변경됨
3) 어텐션 분포 α를 가중치로 하여,
모든 인코더 은닉 상태의 가중합(= Attention Value) a를 계산한다
a = (k=1, N) Σ α_k * h_k = αH (단, α_k는 어텐션 분포 α의 k번째 값)
4) 어텐션 값(= 가중합) a와 디코더의 은닉 상태 s_t를 연결(concatenate)하여 v_t를 만들어낸다
5) v_t 값으로 ~s_t값을 뽑아낸다. ~s_t = tanh(W_c * v_t + b_c)이며 W_c는 (학습가능한) 가중치 행렬 W_c이고 b_c는 편향이다
6) ~s_t를 '출력층'의 입력으로 사용하여 예측 벡터를 얻는다.


※ 닷 프로덕트 어텐션이다
※1) 이 때, f는 내적(Luong Attention의 경우)이 가장 간단한 형태이며,
학습가능한 스코어함수(Bahdanau Attention의 경우) f가 f(s,h) = V^T * tanh(W_1 * s + W_2 * h)
그 외에 여러 형태 있음

※3) a를 context vector라고 부르기도 하는데
인코더의 문맥을 포함하고 있다고 하여 붙인 이름으로
seq2seq 인코더의 마지막 은닉 상태와 불리우는 이름이 같으니 혼동 x

※4) 약간 다른 방식이 있는듯
4) a와 디코더 입력 x_t를 합쳐 v_t-1을 만들고 이를 입력으로 하여 디코더의 새로운 은닉상태 s_t를 만든다
5) s_t로부터 시점 t의 단어를 예측한다

※5) W_c의 크기는 (은닉 상태 크기, 은닉 상태 크기 x 2)이다
v_t가 연결을 통해 (은닉 상태 크기*2,1)의 크기가 되었기 때문이다

※6-1) y_t_hat = softmax(W_y * ~s_t + b_y) 의과정을 통해 예측벡터를 가지고 단어를 추론할 수 있다
※6-2) context vector를 decoder에 넣는(feed) 방법은 아키텍쳐 설계에 따라 다르다고 한다.
※6-3) 예시로 든 아키텍쳐에서는 ~s_t를 출력층의 입력으로 넣고 돌아가면서 자연스럽게 은닉층이 갱신되는듯
4번의 추가 설명과 궤를 같이하는듯하다