https://paperswithcode.com/sota/image-classification-on-imagenet
imagenet 모델 티어표
601p
ViT는 이미지를 위한 딥러닝 모델의 일종으로, 합성곱 계층 방법을 사용하지 않고
트랜스포머 모델의 셀프 어텐션을 적용하였다
지역 특징을 추출하지 않고 전체 이미지를 한 번에 셀프 어텐션으로 처리한다

단, ViT의 좌->우, 상->하 순차 입력은 2차원 구조의 이미지 특성을 온전히 반영할 수 없다고 한다?
물체의 크기나 해상도를 계층적으로 학습하는 Swin과 CvT가 나왔다고한다

Swin은 로컬윈도를 활용해 각 계층의 어텐션이 되는
패치의 크기와 개수를 다양하게 구성해 이미지의 특징을 학습시킨다 <- ???
ViT구조와 비교하면 로컬윈도 안의 어텐션/로컬윈도 간의 어텐션으로 나눠 수행해서
이미지 특징의 계층학습적 학습을 진행한다고한다.
어텐션 함수는 상대적 위치 편향을 반영해 어텐션 값 자체에 위치정보를 포함 <- ???

CvT는 Conv 연산을 ViT에 적용한 모델, 저수준과 고수준 특징을 계층적으로 반영한다고 한다.
Q K V중 KV를 기존 벡터보다 축소했다고한다???

603p
ViT는 트랜스포머 모델의 구조를 사용하며
토큰대신 이미지가 격자로 작은 단위로 나뉘어 순차적으로 입력된다
그림의 좌->우/상->하 순서로 잘라서 넣는듯
(그림에서 CLS 토큰은 존재함)

604p
CNN모델과 ViT모델 비교

둘 다 목적은 이미지 특징을 잘 표현하는 임베딩을 만드는것
※ Embedding은 ML계열에선 벡터 뭉치전반을 통칭하는듯, Embed의 뜻 자체는 '포함시키다'

CNN의 임베딩은 이미지 패치중 일부만 선택하여 학습하고, 이를통해 이미지 전체의특징을 추출한다.
ViT 임베딩은 이미지 패치 전체를 동시에 셀프 어텐션 하여 패치가 서로에게 끼치는 영향을 계산한다.

예시 사진은 고양이의 오른쪽 눈을 표현하는데 사용되지 않는 다른 픽셀을 보여주는 CNN과
모든 픽셀(패치, 격자, 토큰, 뭐든지)을 셀프 어텐션하는 ViT의 차이점을 나타내는듯

※ 좁은 수용 영역(Receptive Field, RF)
RF를 가진 CNN은 전체 이미지를 표현하는데 수많은 계층이 필요하나
트랜스포머 모델은 어텐션 거리를 계산하여 한 개의 ViT 레이어만 있으면 된다고 한다.

605p
※ 귀납적 편향: 딥러닝에서, 일반화 성능 향상을 위한 가정을 의미

이미지 데이터는 공간적 관계가 잘나타나있어
지역적 편향을 가진 합성곱 신경망이 잘 사용된다
CNN이 여지껏 전체 이미지의 특징을 잘알수있던 이유는
이미지를 작은 조각으로 나뉘어 지역적 특징을 강조했기 때문
(지역적 편향)

반면, 시계열 데이터는 시간적 관계가 잘나타나있으므로
순차적 편향을 가진 RNN이 잘 사용된다
RNN은 이전 시간의 상태를 기억하고 현재 입력과 결합하여 예측하므로
순차적인 특징을 강조하는 특성을 가지고 있어서 시계열 데이터에서 좋은 성능을 발휘한다.
(순차적 편향)

그러나 특정 관계 편향이 강할수록
다른 다양한 관계를 표현하기 어려워
귀납적 편향이 약한 모델이 필요하다

ViT가 대용량 이미지 데이터도 잘 학습하는것은
QKV의 임베딩으로 일반화된 관계를 학습하기 때문에 귀납적 편향이 없기때문이다

요약: RNN의 순차적 편향, CNN의 지역적 편향같은 귀납적 편향이 ViT에는 없기 때문에
여러 유형의 데이터나 다양한 관계를 표현할 수 있다.

모델의 구조와 매개변수가 데이터가
적합한 가정을 하고 있음을 귀납적 편향이 보여준다
이런 가정이 올바르다면
더 적은 데이터로 높은 일반화 성능을 보일 수 있다
그러나 너무 강한 귀납적 편향은
다른 유형의 데이터나 관계를 포함하는데 어려움을 초래할 수 있다.

606p
ViT 모델의 구조

ViT모델은 패치 임베딩과 인코더로 이루어진다
패치 임베딩: 이미지를 구조에 맞게 일정한 크기의 패치로 나눈다음 벡터의 형태로 변환하는 임베딩 작업수행
인코더: 각 패치와의 관계를 학습함

¿
그림에서 마지막에 계층정규화 -> 순방향신경망 -> 판별까지는 알겠으나
계층정규화앞, 인코더의 출력쪽에 시퀀스그림은 ??? 별로 안중요한거일지도 모르겠다
https://mishuni.tistory.com/137 여기 그림에는 인코더 -> FC Layer가 바로 연결되어있음

패치 임베딩
{
    입력 이미지를 작은 패치로 분할하는 과정
    전처리로 정사각형 모양으로 이미지를 바꾸고,
    CNN의 계층의 커널과 유사한 방법으로 이미지를 패치로 나눈다
    커널 한 변 크기와 스트라이드 크기를 같게 하는 예시와, 스트라이드가 1인예시 모두 있음

    patch_size = (image_size - kernel_size)/stride

    맨먼저 CLS토큰을 놓고 이후로 쭉
    좌->우/상->하 기준으로 이미지 패치를 쭉 토큰으로 늘어놓는다
    {
    ※ 분류 토큰(Special Classification Token, CLS): (분류작업등에서) 전체 시퀀스를 대표하는 벡터
    https://jimmy-ai.tistory.com/338
    ?????? CLS 토큰은 어떻게 만드는가?
    CLS 토큰과 문장내 다른 단어들 사이의 self-attention으로 생성?
    https://github.com/Gubuzeong/Getting-Started-with-Google-BERT/issues/3
    평범하게 그냥 임베딩을 무작위로(혹은 뭐 제비어 초기화 방법등등)을 해서 한 토큰(벡터)을 CLS로 지정한다음
    가장 앞에 오게 두고 이에 적합하도록 '학습' 하는 과정을 통해 생성된것인가?
    }

    패치 인코딩을 거치고 나서 위치 정보가 담긴 임베딩 벡터를 더해준다
    {
    ??? 위치 임베딩이란
    위치 임베딩은 위치 정보를 담고있는 벡터(텐서)로, 학습이 가능하다
    이미지의 패치레벨에서 동작하므로 공간정보가 덜중요해져서 2차원과 1차원은 별차이없으므로,
    1차원을 주로 사용한다고한다
    https://hongl.tistory.com/232
    https://math.fandom.com/ko/wiki/%EC%9C%84%EC%B9%98_%EC%9E%84%EB%B2%A0%EB%94%A9
    https://wikidocs.net/137253
    }

    패치인코딩->위치임베딩
    다 끝나면 계층 정규화를 적용하여 마무리한다
    패치 인코딩->위치임베딩 추가 -> 계층 정규화
}

인코더 계층
{
    멀티헤드 어텐션을 사용하는 인코더 계층과 동일함
    N개의 인코더 반복 적용

    마지막 레이어에서는 분류 토큰을 추출(CLS와 같은놈은 아닌듯?)
    이 토큰은 이미지 데이터를 잘 표현하는 특징 벡터로 간주,
    이를 이미지 분류 및 검색등에 사용함

    마지막 인코더의 출력 시퀀스(책의 표현에서는 시퀀셜 산출물)
    를 전부쓰는게 아니고, 분류 토큰 벡터만 사용한다고한다
    이를 FC에 넣고 분류하는듯
}

623p
혼동행렬을 보면 결과를 통해 오분류된 클래스에 대해서 문제점을 파악하는데 용이하다