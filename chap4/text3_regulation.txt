정칙화: 손실 함수에 규제를 가해 모델이 암기가 아니라 일반화를 하도록 하여 과대적합 문제를 방지하는 기술
from 규제, 일반화 to 과대적합 방지

암기는 데이터의 특성이나 패턴 대신 노이즈를 학습한 경우에 이루어 지며, 훈련데이터만 잘 되고 새로운 데이터는 안됨
일반화는 모델에 새로운 데이터도 정확한 예측이 가능해지는것
정칙화는 노이즈에 강건하고 일반화된 모델을 구축하기 위해 사용하는방법

정칙화를 적용하면 학습 데이터의 작은 차이에 덜 민감해짐 -> 모델의 분산 값이 낮아진다
의존하는 특징의 수를 줄임 -> 추론능력 개선

정칙화는 모델이 복잡하고 학습에 사용되는 데이터의 수가 적을 때 활용
모델이 단순하다 -> 매개변수의 수가 적어 정칙화 필요 x
데이터의 수가 많거나 잘 정제된 경우 -> 노이즈가 적어 사용 x

스케일링, 표준화, 정칙화 용어들...
https://blog.nerdfactory.ai/2021/06/15/Normalization-Theorem-1.html

정칙화(Regulation)는 구분없이 사용할 때는 정규화(Normalization)라고도 부른다
※ 정규화는 어떤 대상을 일정한 규칙이나 기준에 따르는 '정규적인' 상태로 만드는것이나 비정상적인 대상을 정상으로 되돌리는 과정

---------------------------------

L1 정칙화(Lasso): L1 노름으로 규제하는 방법

L1노름은 벡터(행렬)의 절대값 합계를 통해 계산한다.
L1 정칙화는 손실 함수에 가중치 절대값의 합을 추가해 과대 적합을 방지한다.
모델 학습은 비용이 0이 되는 방향으로 진행되고,
손실 함수에 값을 추가한다면 오차가 더 커지므로
모델은 추가된 값까지 최소화 할 수 있는 방향으로 학습한다.
학습에서 값이 작은 가중치는 0으로 수렴한다 -> 특징의 개수가 줄어든다 (= 특징 선택의 효과를 얻을 수 있다)

L1 정칙화의 식은
L1 = λ * (i=0, to n)∑abs(w_i)
람다는 규제강도, 0보다 큰 값으로 설정한다.
작을 수록 과대적합, 클 수록 과소적합에 취약

L1 정칙화 성능상의
가중치합을 계산해야하므로 계산 복잡도가 늘어난다.
미분할 수 없으므로 역전파 계산하는데 더 많은 리소스 소모된다.
배율이 적절하지 않으면 모델의 해석이 어려워진다. 최적의 _lambda 값은 반복이 필요

(chap4_4코드에서 실제로 해보니 chap3의 14번 코드보다 느리고, _lambda가 0.5보단 0.005에서 잘 동작하였다)

---------------------------------

L2 정칙화: L2 노름을 사용해 규제하는 정칙화의 방법
손실 함수에 가중치 제곱의 합을 추가, 그외에는 L1과 동일
하나의 특징이 너무 중요한 요소가 되지 않도록 규제를 가한다.
가중치가 0이 되는 일은 적고 0에 가까워진다.
L1에 비해 가중치 값들이 비교적 균일하게 분포된다.

L1 = λ * (i=0, to n)∑abs(w_i^2)

184p에 L1 vs L2 정리 있음
모델링: 희소함/희소안함
학습: 복잡한거 안됨/됨
가중치: 0가능/0에가깝게 가능
이상치: 강함/약함
특징 선택: 있음/없음

※ optim의 parameter에 weight_decay=(_lambda 값)을 넣어두면 간단하게 L2 정칙화 적용 가능