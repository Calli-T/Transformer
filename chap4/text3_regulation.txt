정칙화: 손실 함수에 규제를 가해 모델이 암기가 아니라 일반화를 하도록 하여 과대적합 문제를 방지하는 기술
from 규제, 일반화 to 과대적합 방지

암기는 데이터의 특성이나 패턴 대신 노이즈를 학습한 경우에 이루어 지며, 훈련데이터만 잘 되고 새로운 데이터는 안됨
일반화는 모델에 새로운 데이터도 정확한 예측이 가능해지는것
정칙화는 노이즈에 강건하고 일반화된 모델을 구축하기 위해 사용하는방법

정칙화를 적용하면 학습 데이터의 작은 차이에 덜 민감해짐 -> 모델의 분산 값이 낮아진다
의존하는 특징의 수를 줄임 -> 추론능력 개선

정칙화는 모델이 복잡하고 학습에 사용되는 데이터의 수가 적을 때 활용
모델이 단순하다 -> 매개변수의 수가 적어 정칙화 필요 x
데이터의 수가 많거나 잘 정제된 경우 -> 노이즈가 적어 사용 x

스케일링, 표준화, 정칙화 용어들...
https://blog.nerdfactory.ai/2021/06/15/Normalization-Theorem-1.html

정칙화(Regulation)는 구분없이 사용할 때는 정규화(Normalization)라고도 부른다
※ 정규화는 어떤 대상을 일정한 규칙이나 기준에 따르는 '정규적인' 상태로 만드는것이나 비정상적인 대상을 정상으로 되돌리는 과정

---------------------------------

L1 정칙화(Lasso): L1 노름으로 규제하는 방법

L1노름은 벡터(행렬)의 절대값 합계를 통해 계산한다.
L1 정칙화는 손실 함수에 가중치 절대값의 합을 추가해 과대 적합을 방지한다.
모델 학습은 비용이 0이 되는 방향으로 진행되고,
손실 함수에 값을 추가한다면 오차가 더 커지므로
모델은 추가된 값까지 최소화 할 수 있는 방향으로 학습한다.
학습에서 값이 작은 가중치는 0으로 수렴한다 -> 특징의 개수가 줄어든다 (= 특징 선택의 효과를 얻을 수 있다)

L1 정칙화의 식은
L1 = λ * (i=0, to n)∑abs(w_i)
람다는 규제강도, 0보다 큰 값으로 설정한다.
작을 수록 과대적합, 클 수록 과소적합에 취약

L1 정칙화 성능상의
가중치합을 계산해야하므로 계산 복잡도가 늘어난다.
미분할 수 없으므로 역전파 계산하는데 더 많은 리소스 소모된다.
배율이 적절하지 않으면 모델의 해석이 어려워진다. 최적의 _lambda 값은 반복이 필요

(chap4_4코드에서 실제로 해보니 chap3의 14번 코드보다 느리고, _lambda가 0.5보단 0.005에서 잘 동작하였다)

---------------------------------

L2 정칙화: L2 노름을 사용해 규제하는 정칙화의 방법
손실 함수에 가중치 제곱의 합을 추가, 그외에는 L1과 동일
하나의 특징이 너무 중요한 요소가 되지 않도록 규제를 가한다.
가중치가 0이 되는 일은 적고 0에 가까워진다.
L1에 비해 가중치 값들이 비교적 균일하게 분포된다.

L1 = λ * (i=0, to n)∑abs(w_i^2)

184p에 L1 vs L2 정리 있음
모델링: 희소함/희소안함
학습: 복잡한거 안됨/됨
가중치: 0가능/0에가깝게 가능
이상치: 강함/약함
특징 선택: 있음/없음

※ optim의 parameter에 weight_decay=(_lambda 값)을 넣어두면 간단하게 L2 정칙화 적용 가능
------------------
모멘텀: 경사 하강법 알고리즘 변형 중 하나로,
이전의 이동했던 방향과 기울깅의 크기를 고려하여 가중치를 갱신
이를 위해 지수 가중 이동 평균을 사용하며,
이전 기울기 값의 일부를 현재 기울기 값에 추가해 가중치를 갱신한다.

※ 지수 가중 평균(=지수 이동 평균):데이타의 이동평균을 구할 때 오래된 데이터가 미치는 영향을 지수적으로 감쇠하도록 만드는 방법
※ 이동평균(移動平均, moving average, rolling -, running -)은 전체 데이터 집합의 여러 하위 집합에 대한 일련의 평균을 만들어 데이터 요소를 분석하는 계산이다

모멘텀 공식

v_i = γ*v_i-1 + α∇f(W_i)
W_i+1=W_i-v_i

v_i는 i번째 모멘텀 값, 이동벡터
γ는 모멘텀계수, 하이퍼파라미터

감마 값이 [0, 1]이고 0이라면, 일반적인 경사하강법과 다를것이 없다. 일반적으로 0.9와같은 값을 사용한다.
파이토치의 모멘텀은 최적화 함수의 momentum 하이퍼파라미터를 설정하면 적용되는 것으로 구현되었다.

--------------------

엘라스틱 넷
L1, L2를 같이 쓰는거고 혼합비율을 [0, 1] 사이로 정하기만 하면된다.
특징의 수가 샘플의 수보다 더 많을 때 유의미한 결과를 가져온다고 한다.
그러나 더 많은 튜닝, 더 많은 리소스 사용이 있다고 한다.

--------------------

드롭아웃: 정칙화 기법중 하나로, 일부 노드를 일정 비율로 제거하거나 0으로 설정해 과대적합을 방지하는 기술

노드간 동조화 현상: 특정 노드의 가중치나 편향이 큰 값을 가지게 되면 다른 노드가 해당 노드에 의존하는 현상
해당 현상은 과대 적합을 발생시키는 이유중 하나이며
드롭아웃을 통해 간단하고 효율적으로 해결할 수 있다.
그러나 충분한 데이터셋과 비교적 깊은 모델이 갖추어 질 때 적용할만하다.(모든 노드를 이용해 학습하는게 아니므로)

파이토치에서는 nn.Dropout 클래스로 쉽게 구현된다
self.dropout = nn.Dropout(p=0.5)
여기서 p는 베르누이 분포의 모수를 의미한다...고 하는데 (해당 노드 제거에) 성공할 확률인듯하다.
----------------
그레이디언트 클리핑: 가중치 최대값을 규제해 최대 임곗값을 초과하지 않도록 하는 정칙화의 기법
특정 노드의 가중치가 너무 크지 않도록하여 과대 적합을 방지한다.

그레이디언트 클리핑 공식
w = r *(w/abs(w)), if:||w||>r
※ ||w||은 노름

기울기가 r을 초과하면 r로 줄여버리는 개념인듯
그레이디엍느 클리핑은 L2노름을 사용해 최대 기울기를 규제하는게 일반적이다

RNN이나 LSTM모델들은 기울기 폭주에 취약한데, 해당 모델들에 주로 사용한다.
가중치 값에 엄격한 제약조건을 요구하거나 모델이 큰 기울기에 민감한 상황에 유용하다.

torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0)

최대 임곗값은 하이퍼파라미터이므로 경험적으로 선택
기울기 최댓값을 규제한다면 비교적 큰 학습률이 사용가능
최대 임곗값이 높으면 모델의 표현력이 떨어진다.
최대 임곗값이 낮은 경우 학습이 불안정해질 수 있다.
