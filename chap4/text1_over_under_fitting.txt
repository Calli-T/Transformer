160p
과소적합: 훈련과 새로운 데이터 모두 성능이 좋지않음, 학습이 잘 안된것
과대적합: 훈련 데이터는 우수하게 예측하지만 새로운 데이터는 제대로 예측할 수가 없는것

161p
편향이 높으면 추정치가 일정한 값을 가질 확률 증가
분산 높으면 추정치에 대한 변동폭 증가 및 데이터의 노이즈 학습하여 과적합 문제 발생
모델이 훈련말고 새로운 데이터에 대해서도 우수한 성능을 보이려면 편향과 분산이 모두 낮아야한다.

https://opentutorials.org/module/3653/22071
예측값들과 정답이 대체로 멀리 떨어져 있으면 결과의 편향(bias)이 높다고 말하고,
예측값들이 자기들끼리 대체로 멀리 흩어져있으면 결과의 분산(variance)이 높다고 말합니다.

편향과 분산은 trade-off 관계라고한?다

모델 복잡도올라감 -> 분산 커지고 편향 작아짐
모델 복잡도내려감 -> 분산 작아지고 편향 커짐

162p
과대적합은 모델의 일반화능력을 저하
과소적합은 데이터의 특징을 제대로 학습 x

이 둘을 피하기 위한 방법으로
데이터 수집, 피처 엔지니어링, 모델 변경조기 중단, 모델 변경
배치 정규화, 가중치 초기화 정칙화
가 있다.

163p
배치 정규화: 내부 공변량 변화를 줄여 과적합을 방지하는 기술
공변량: 입력 데이터의 분포
내부 공변량 변화: 신경망 내부의 은닉층마다 입력되는 값의 분포가 달라지는 현상, ICS

배치단위로 학습하는 경우, 상위 계층의 배치가 변경될 때마다
데이터의 분포가 달라져 변동되는 것을 새로이 학습해야하며
이는 모델의 학습 속도 저하로 이어진다.

NO batch normalization -> 가중치 갱신 어려움, 초기 가중치에 민감

배치 정규화는 미니 배치의 입력을 정규화 하는 방식이다
미니배치 -> 전체 데이터를 (주로 2^N)크기의 미니배치로 자름
미니배치의 정확한 뜻이 이게 맞던가?

정규화 종류: 배치정규화/ 계층정규화/인스턴스정규화/그룹정규화가 잇다
이것들이 뭔소린지는 나중에 알아보자
https://www.tensorflow.org/addons/tutorials/layers_normalizations?hl=ko
https://m.blog.naver.com/chrhdhkd/222014776700

차원, 채널은 데이터 종류따라 달라진다.

계층 정규화:
미니배치 샘플 전체 x
각 샘플에 개별 수행
샘플간의 의존 관계가 없음
샘플이 다른 길이를 가져도 수행가능
RNN이나 트랜스포머에 사용

인스턴스 정규화:
채널과 샘플을 기준으로 수행
각 샘플에 개별적으로 수행
입력이 다른 분포를 갖는 작업에 적합
GAN이나 Style transfer 모델에서 주로 사용

그룹 정규화: 채널을 N개 그룹으로 나누고 그룹 내에서 정규화 수행
그룹을 하나로 설정하면 인스턴스와 동일
그룹의 개수를 채널개수와 동일하게 하면 계층 정규화와 동일
CNN에서 배치 크기가 작으면 배치가 데이터셋을 대표한다고 보기 어려워서 대안으로 사용

배치 정규화는 주로 순방향 신경망에 쓴다.

166p
배치 정규화 공식은
yi = xi - E[x] / sqrt(var[x] + ε) * γ + β
xi 원래값 E[x]산술평균, Var[X] 분산, ε 분모0방지 매우작은 상수, 감마 표준편차 스케일링, 베타 평균 시프트, X 모집단