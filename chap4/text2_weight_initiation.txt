170p
적절한 초기화는 기울기 폭주나 소실 문제 완화가능
모델의 수렴  속도 향상을 통한 학습 프로세스 개선 가능

상수 초기화: 같은 숫자로 모든 가중치 초기화
그렇게 하면 대칭 파괴 현상이 일어나 학습에 문제가 생긴다.
대칭 파괴: 초기 가중치가 동일한 상수일 때 모든 노드가 동일한 출력을 생성하여 네트워크가 어려워지는 현상

무작위 초기화: 초기 가중치의 값을 무작위 값이나 특정 분포 형태로 초기화
ex) 정규 분포의 형태로 가중치의 값을 초기화
무작위, 균등, 정규 분포, 잘린 정규 분포, 희소 정규 분포 초기화 등등이 있음
-> 대칭 파괴 문제는 발생 안한다
--> 그러나 계층이 많으면 활성화 값이 양 끝단에 치우쳐 기울기 소실 현상이 발생

172p
제이비어&글로럿 초기화
해당 초기화는 균등 분포나 정규 분포를 사용해 가중치를 초기화 하는 방법이다.
각 노드의 출력 분산이 입력 분산과 동일하도록 가중치를 초기화하는 방법

해당 페이지의 U(-a, a)는 uniform 즉 균등분포를 의미한다. (알파벳 U가 맞는지, 맞다면 필기체가 뭔지는 모르겠다)
균등분포는 이산확률변수와 연속확률변수모두 가능하며 연속의 경우 U(a, b)인 균등분포가 [a, b] 구간에서 균등한 확률을 가진다.

균등 분포의 경우
W = U(-a, a)에서 무작위로 선택,
a = gain * (6/(fan_in+fan_out))^0.5
fan_in: 이전 계층의 노드 수
fan_out: 다음 계층의 노드 수
gain: scale을 의미

정규 분포의 경우
W = N(0, std^2)에서 무작위로 선택,
std = gain * (2/(fan_in+fan_out))^0.5