※이 내용들은 한 번 다시 봐야겠다 뭔소리야

219p
사전 학습된 모델이란 말 그대로 이미 학습이 완료된 모델
(대규모 데이트세트로)

사전 학습된 모델 자체를 시스템에 적용하거나,
사전학습된 임베딩 벡터를 활용해 모델을 구성할 수 있다.

Embedding Vector: 입력 데이터를 연속적이고 조밀한 벡터로 매핑한것

이미 학습된 모델 일부를 활용하거나,
추가학습함으로 모델의 성능을 끌어낼 수 있다.

학습에 필요한 시간이 대폭 감소하거나
모델 개발 프로세스의 가속화하여
모델의 성능 향상 가능

전이 학습이나 백본 네트워로 사용할 수 있다.

220p
Backbone: 입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델이나 모델의 일부를 의미
노이즈와 불필요한 특징을 제거하고 가장 중요한 특징을 추출할 수 있다.
추출한 특징들은 다시 새로운 모델에 입력가능

ex)
객체 검출 모델을 포즈 추정 모델이나 이미지 분할 모델로 확장할 때
모델을 처음부터 구성하는게 아니고
객체를 검출하는 합성곱 신경망을 가져와 최종 계층을 바꿔 구성가능

백본으로 사용할 딥러닝 모델은 데이터에 따른 과대적합을 방지하기위해 정규화/정칙화와 같은 기술 적용하는것을 권장.
사전학습된 은 미세 조정이나 전이 학습을 적용해 과대 적합을 피해야한다.

NLP나 Computer Vision 작업에서 백본이 되는 모델은
BERT/GPT,VGG-16,ResNet등의 초대규모 딥러닝 모델을 사용한다.

221p
전이 학습:
어떤 작업을 수행하기 위해
이미 사전 학습된 모델을 재사용해
새로운 작업이나 관련 도메인의
성능을 향상시킬 수 있는 기술을 의미한다.
(여기서 도메인이란 모델이 작동하도록 설계된 데이터의 특정영역)

특정 영역의 대규모 데이터세트에서 사전 학습된 모델을
다른 영역의 작은 데이터세트로 미세 조정해 활용한다.
소스 도메인 to 타깃 도메인

222p
전이 학습은 사전 학습된 모델과 미세 조정된 모델의 관계를 설명하기 위해 2개 영역으로 구분된다고한다
업스트림 모델: 전이 학습을 수행하기 위해 사전 학습된 모델, 전이 학습 파이프 라인 시작
다운스트림 모델: 미세 조정된 모델, 전이 학습 파이프라인 마지막

https://blog.naver.com/PostView.naver?blogId=dbwjd516&logNo=222998988514&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView

전이 학습에는 귀납적/변환적/비지도 전이 학습이 있다

223p
귀납적 전이 학습: 기존에 학습한 모델의 지식을 활용하여 새로운 작업을 수행하기 위한 방법(들)
https://bibimnews.com/entry/%ED%95%99%EC%8A%B5%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%80%EC%A1%B1%ED%98%84%EC%83%81-%ED%95%B4%EA%B2%B0%EB%B2%95-%EC%A0%84%EC%9D%B4%ED%95%99%EC%8A%B5Transfer-Learning
https://goatlab.tistory.com/entry/Deep-Learning-%EC%A0%84%EC%9D%B4-%ED%95%99%EC%8A%B5-Transfer-Learning-1
대상 도메인이 소스 도메인과 비슷할 때 자주 사용된다고 한다.


자기주도적 학습: 비지도 전이 학습의 유형 중 하나, 노레이블 데이터셋에서 사용
대규모 데이터세트에서 특징을 추출하는 오토 인코더와 같은 모델을 학습시킨다음
저차원 공간에서 레이블된 데이터로 미세조정하는 방법

다중 작업 학습:
레이블이 지정된 소스 도메인과 타깃 도메인 데이터를 기반으로
모델에 여러 작업을 동시에 가르치는 방법을 의미
모델 구조는 공유 계층과 작업별 계층으로 나뉘어져 있다.

공유 계층에서는 소스 도메인과 타겟 도메인의 데이터 셋에서 모델을 사전 학습한 다음
단일 작업을 위해 작업별 계층마다 타겟 도메인 셋으로 미세 조정하는 방법으로 모델을 구성
작업마다 서로 다른 학습 데이터셋으로 사용하여 미세 조정
하나의 작업에 과대적합되지않아 일반화된 모델을 얻을 수 있다
서로 다른 작업이 동일한 도메인을 사용하므로 성능향상에 기여한다고 함

이게 뭔소리여 대체

224p
변환적 전이 학습: 소스 도메인과 타겟 도메인이 유사하지만 완전히 동일하지 않은 경우를 의미한다.
(소스 및 대상 작업의 도메인이 정확히 동일하지 않지만 상호 관련된 시나리오는 변환적 전이 학습 전략을 사용한다.
출처: https://goatlab.tistory.com/entry/Deep-Learning-전이-학습-Transfer-Learning-1 [GOATLAB:티스토리])
일반적으로 소스 도메인에는 레이블이 있고 타겟에는 없는경우에 사용된다.

레이블이 지정된소스 도메인으로 사전 학습된 모델 구축
지정안된 타겟 도메인으로 모델을 미세 조정해
특정 작업에 대한 성능 향상

변환적 전이 학습은 또다시 도메인 적응과 표본 선택 편향/공변량 이동으로 나뉜다.

도메인 적응: 소스 도메인과 타깃 도메인의 특징 분포를 전이 시키는 방법 (전이 학습-변환적 전이 학습)
소스/타깃 도메인은 유사하지만 다르므로 두 도메인간의 특징 공간과 분포는 서로 다르다.
타깃 도메인에서 모델의 성능을 향상시키기 위해 소스 도메인이 조정될 수 있다.

표본 선택 편향/공변량 이동: 소스 도메인과 타깃 도메인의 분산과 편향이 크게 다를 때
표본을 선택해 편향이나 공변량을 이동시키는 방법
소스 도메인과 타겟 도메인은 완전히 동일하지 않음->모델이 학습데이터에서 좋은 성능을 보였더라도 테스트에서 성능이 좋지 않을 수 있음->
무작위/비무작위 샘플링 방법이나 도메인 적응을 통해 해당 학습치만 전이시키는 방법

224p
비지도 전이 학습: 소스 도메인/타깃 도메인 모두 레이블이 없는 경우의 전이 학습 방법
이러한 방법은 소스 도메인에서 타깃 도메인의성능을 개선하는 데 사용할 수 있는 특징 표현을 학습한다.

레이블이 없는 (소스)전체 데이터로 학습해
데이터가 가진 특징/특성을 구분할 수 있게
사전 학습된 모델을 구축하고
소규모의 레이블이 지정된 (타겟)데이터를 통해 미세 조정한다.

레이블의 영향이 없이 데이터의 특징을 학습했으므로
미세 조정을 통해 효과적으로 타깃 도메인을 예측할 수 있다.

GAN/군집화 등이 여기 포함된다.

225p
전이 학습은 사전 학습된 모델의 지식을 활용->
새로운 도메인에 대한 예측 진행 가능->
작은 데이터셋으로 우수한 결과

시간과 리소스 소모 최소화, 더 높은 학습률 가능

사전 학습된 모델 예시
자연어 처리: Word2Vec/fastText/BERT
컴퓨터 비전: ResNet-50/VGG-16

225p
https://m.blog.naver.com/dh0985/222321560055
책의 내용 설명/이해 포기

https://velog.io/@nomaday/n-shot-learning
의 내용으로 대체
원-샷 학습: 하나의 샘플만을 사용해 모델을 학습하는방법
퓨-샷 학습: 한 클래스당 일부 샘플 이미지만 사용해여 새로운 클래스를 인식
제로샷 학습: 라벨링 되지 않은 새로운 클래스에 대한 분류 작업을 수행할 때 이전에 학습된 모델을 사용하여 분류하는 기술, 학습과정에서 본 적이 없는 새로운 클래스를 인식

쿼리 셋: 새로운 클래스를 분류하기 위한 입력데이터
서포트 셋: 학습에 사용될 클래스의 대표 샘플

레이블링과 도메인에 따른 전이학습
귀납적 전이 - 자기주도적 학습: 소스 레이블x 타깃o
귀납적 전이 - 다중 작업 학습: 소스 레이블o 타깃o
변환적 전이 학습: 소스 레이블 o 타깃 x
비지도 전이 학습: 소스 레이블 x 타깃 x
제로샷/퓨샷/원샷 학습: 소스 레이블 o 타깃 x

특징 추출 및 미세 조정은 둘 다 전이 학습에 사용되는 일반적인 기술

특징추출: 사전 훈련된 모델을 수정하지 않고 분류 새로운 모델을 초기화
미세 조정: 사전 학습된 모델의 일부를 재사용

데이터 적고 유사도 높으면 합성곱 계층 전부 동결
데이터 많고 유사도 높으면 하위계층 제외하고 전부 동결
데이터 적고 유사도 낮으면 상위계층 일부만 동결
데이터 많고 유사도 낮으면 전체 모델 전체 재학습

적고 낮으면 상위계층에서 저차원의 특징을 잡아내는듯, 최대한의 성능을 끌오이기 위한 작업이다
많고 낮으면 걍 다시 전부 훈련
적고 높으면 훈련된걸 전부 요긴하게 씀
많고 높으면 훈련된걸 대부분 쓰고 하위계층에서 살짝만 바뀌게 하는식인듯, 비슷해도 완전 같지는 않으니 최대한의 성능을 하위계층에서 끌어내본다