264p
텍스트 벡터화: 텍스트를 벡터로 바꿈

딱히 안중요함
{
간단한 방법으로는 원-핫 인코딩, 빈도 벡터화 등이 있다.
원핫인코딩은 단어를 색인으로 매핑, 해당하는 단어만 1 나머지는 0
빈도 벡터화는 문서에서 단어의 등장횟수를 세어 작성하는 방식
둘 다 간단하지만 벡터의 희소성이 크다

단점은
토큰개수만큼의 차원이 필요하지만
입력문장내에 토큰 수는 현저히 적다 -> 차원의 저주
또 텍스트의 벡터가 입력 텍스트의 의미를 내포하고 있지 않아서
의미적으로 유사해도 벡터가 유사하지 않을 수 있다
i scream for ice cream과 ice cream for i scream은 모두 [1, 1, 1, 1, 1]
}

이런 문제를 해결하기 위해 워드 임베딩(Word2Vec이나 fastText) 기법 사용
워드 임베딩: 단어를 고정된 길이의 실수 벡터로 표현하는 방법

단어의 의미를
벡터 공간에서다른 단어와의 상대적 위치로 표현해
단어간의 관계를 추론할 수 있다.

워드 임베딩으로 고정된 임베딩을 학습하면
다의어나 문맥 정보를 다루기 힘들다는 단점이 있기때문에
인공신경망을 활용해 동적 임베딩 기법을 사용한다.

정리: 단어를 벡터로 변환해 벡터공간에서 상대적위치로 단어간의 관계 추론
고정된 임베딩을 쓰면 여러문제가 생겨 딥러닝으로 동적 임베딩을 만들어냄

266p
언어모델: 입력된 문장으로 각 문장을 생성할 수 있는 확률을 계산하는 모델

주어진 문장 뒤에 나올 수 있는 문장은 다양하기 때문에
완성된 문장 단위로 확률을 계산하는 것은 어려운 어렵다
->
하나의 토큰 단위로 예측한다

자기회귀 언어 모델: 입력된 문장의 조건부 확률을 이용해 다음에 올 단어를 예측한다.
이름 자체는 모델의 출력값이 다시 입력값으로 들어가기 때문에 명명되었다

언어모델에서 조건부 확률은
이전 단어들의 시퀀스가 주어질 때
다음 단어의 확률을 계산하는 것

이전에 등장한 모든 토큰의 정보를 고려하며,
문장의 문맥정보를 파악하고
다음 단어를 생성
다음다음 단어는 다음 단어까지의 단어들로 이를 반복...
P(w_i|w_1, w_2, ... , w_i-1) = P(w1, w2, ... , w_i) / P(w_1, w_2, ... , w_i-1)

조건부 확률의 연쇄법칙을 위의 식에 적용하면 아래와 같이 된다
P(w_t|w_1, w_2, ..., w_t-1) = P(w_1)P(w_2|w_1) ⋯ P(w_t|w_1, w_2, ⋯ ,w_t-1)
w_1이 일어날 확률과 w_1이 일어날 경우에 w_2가 일어날 확률의곱을 한무 반복

268p
통계적 언어 모델: 언어의 통계적 구조를 이용해
문장의 다음의 시퀀스를 생성하거나 분석하고
시퀀스에 대한 확률 분포를 추정하여
문장의 문맥을 파악해
다음에 등장할 단어의 확률을 예측

기본적으로 마르코프 체인을 이용하여 구현됨

----------------------------------

빈도 기반의 조건부 확률 모델에서는 각 변수가 발생한 빈도수를 기반으로 확률을 계산
corpus에 안녕하세요 1000번 안녕하세요 만나서 700번, 안녕하세요 반갑습니다 100번의 경우
P(만나서|안녕하세요) = P(안녕하세요 만나서) / P(안녕하세요) = 700/1000
P(반갑습니다|안녕하세요) = P(안녕하세요 반갑습니다) / P(안녕하세요) = 100/1000

그러나, 단어의 순서와 빈도에만 기초하여 문장의 확률을 예측하면
문맥을 제대로 파악하지 못하는 경우가 생기고 이는
부정확하거나 불완전한 결과를 초래할 수 있고
한 번도 등장한 적이 없는 단어나 문장에 대해
정확한 확률을 예측하기가 어렵다

※ 데이터 희소성 문제: 관측한 적이 없는 데이터를 예측하지 못하는 문제

하지만 통계적 언어 모델은
기존에 통계적 언어 모델은 기존에 학습 텍스트 데이터에서
패턴을 찾아 확률 분포를 생성하며
이를 이용해 새로운 문장을 생성가능,
다양한 종류의 텍스트 데이터를 학습할 수 있다.

GPT나 BERT모델같이 신경망을 통해 확률 P를 계산할 수도 있다

270p
N-gram은 가장 기초적인 통계적 언어 모델이다.
연속된 N개를 하나의 단위로 취급하여 추론한다.
N-1개의 토큰을 고려해 확률을 계산한다

P(w_t|w_t-1, w_t-2, ... , w_t-N+1)
w_t는 예측하려는 단어, w_t-1부터 w_t-N+1까지는
예측에 사용되는 이전 단어들을 의미한다.

271p
N-gram은 구(phrase)를 처리할 수 있다.
입이 무겁다를 예시로 들며 관용적 표현을 파악할 수 있다는 것을 설명하였다.

272p
TF-IDF(Term Frequency-Inverse Document Frequency): 텍스트 문서에 특정 단어의 중요도를 계산하는 방법
통계적인 가중치를 의미한다. BoW(Bag-of-Words)에 가중치를 부여하는 방법

BoW: 문서나 문장을 단어의 집합으로 표현하는 방법
문서나 문장에 등장하는 단어의 중복을 허용하여 빈도를 기록

['That movie is famous movie']
I   like    that    movie   don't   famous  is
0   0       1       2       0       1       1

이를 이용해 벡터화 하는 경우 모든 단어는 동일한 가중치를 얻는다.
따라서 don't는 자주 등장하지 않는 corpus 예를들어
영화 리뷰 긍/부정 분류 모델을 만든다고 가정하면 높은 성능을 가지기 어렵다.

------------------------------------

단어빈도(TF): 문서 내에서 특정 단어의 빈도수를 나타내는 값.
3개의 문서에서 movie라는 단어가 4번 나오면 TF가 4
TF(t, d) = count(t, d)
TF가 높으면 특정 문서에서 중요한 역할을 할 수도 있으나 그냥 관용어나 전문용어일 수 있다.
TF는 단순 빈도수이므로 문서의 길이에 따라 늘어날 수도 있다.

문서 빈도(DF): 한 단어가 얼마나 많은 문서에 나타나는지를 의미하는값
DF(t, D) = count(t ∈ d:d ∈ D)
위 사례에서 DF는 3
DF가 높으면 일반적으로 널리 사용되며, 중요도가 낮을 수 있다.
DF가 낮으면 특정 단어가 적은 수의 문서에만 등장한다.
그러므로 특정한 문맥에서만 사용되는 단어일 가능성이있고 중요도가 높을 수도 있다.

역문서 빈도(Inverse Document Frequency, IDF): 전체 문서 수를 문서 빈도로 나눈것에 로그를 취한 값,
문서 내에서 특정 단어가 얼마나 중요한지를 나타낸다.

문서 빈도가 높으면 일반적인 단어, 상대적으로 덜중요함->문서빈도의 역수를 취하면 단어의 빈도수가 적을수록 IDF 값이 커지게 보정
IDF(t, D) = log(count(D)/(1+DF(t, D))) ※ +1은 작은 수를 더해 분모 0이되는걸 방지, 로그는 너무 큰 값을 방지

TF-IDF
TF-IDF(t,d,D) = TF(t,d)*IDF(t,d)
문서내에 단어가 자주 등장하지만 전체 문서 내에 해당 단어가 적게 등장하면 TF-IDF 값은 커진다.
따라서, 전체 문서에 자주 등장할 확률이 높은 관사나 관용어 등의 가중치가 낮아진다.

274p
라이브러리는 사이킷런에 준비되어있다. TF-IDF
클래스 설명은 2_TF-IDF.py 코드에

277p
vectorizer는 TF-IDF클래스로 불러오고 이걸로 문자열 리스트를 (즉 corpus를) 학습 가능
학습한 vectorizer는 거기다 문자열을 transform한다
그걸 받아와서 toArray()로 Numpy 배열화
vocabulary_는 사용된 단어사전이고
TF-IDF는 문서수*단어수의 배열, 각 행은 하나의 문서, 값은 해당 단어에 대한 색인 값
해당 배열에서 행당 최대 점수 N개만 추리면 문서에서 가장 중요한 단어 추출 가능

그러나 아직 문장의 순서나 문맥의 고려가 없고
문장 생성에는 순서가 중요해서 부적합하다.
또 벡터가 문서내 중요도를 의미하지, 단어의 의미를 담고있는게 아님

-------------------------------------
Word2Vec: 단어간의 유사성을 분포 가설 기반으로 측정하는 모델
※분포가설: 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정
동시 발생 확률 분포를 이용해 단어 간의 유사성을 측정
'내일 자동차를 타고 부산에 간다'
'내일 비행기를 타고 부산에 간다'
두 문장에서 '자동차'와 '비행기'는 주변에 분포한 단어들이 동일하거나 유사하므로
비슷한 의미를 가질것이라고 추정
※분산 표현: 단어를 고차원 벡터 공간에 매핑하여 단어의 의미를 담는 것

단어 벡터화의 방법은
희소표현과 밀집 표현으로 구분됨
희소표현: 벡터 대부분의 요소가 0
밀집표현: 벡터의 대부분이 0이 아닌 실수로 이루어짐

밀집표현을 쓰면 단어 사전의 크기가 커지더라도 벡터의 크기는 고정으로 사용가능하며
벡터 공간상에서 단어 간의 거리를 효과적으로 계산할 수 있다
단어 임베딩 벡터라고 하며 Word2Vec은 대표적인 단어 임베딩 기법이다.

Word2Vec은 CBoW와 Skip-gram이라는 두 가지 방법을 사용한다
https://www.goldenplanet.co.kr/our_contents/blog?number=859&pn=1
https://heytech.tistory.com/352
-

CBoW(Continuous Bag of Words): 주변의 단어를 가지고 중심의 단어를 예측하는 방법, 모델이다
Center Word/Context Word
Sliding Window: 주변 고려할 단어 개수를 Window, 그리고 이게 학습을 위해 움직이므로 Sliding

'세상의 재미있는 일들은 모두 밤에 일어난다' 라는 문장에서
(재미있는 일들은 | 세상의)
(세상의 일들은 모두 |재미있는)
(세상의 재미있는 모두 밤에 | 일들은)
이런 식으로 좌우 2개 단어까지 학습데이터로 넣고 (주변단어 | 중심단어)로 구성

CBoW 모델은 각 입력 단어의 원-핫 벡터를 입력으로 받는다.
투사층에 입력된 원-핫 벡터는 인덱스에 해당하는 임베딩 벡터를 반환하는 순람표 구조가 된다. (※LUT)
투사층을 통과한 각 단어는 E 크기의 임베딩 벡터로 변환된다.
각 단어들의 임베딩 벡터들의 평균 값을 계산한 다음
평균 벡터를 가중치 행렬 W'(아래첨자)E*V와 (※E행 V열벡터를 의미) 곱하면 V 크기의 벡터를 얻는다.
이 벡터를 소프트맥스 함수를 이용해 중심 단어를 예측한다.
(즉, 예측값 y_hat을 실제 중심 단어의 원-핫 벡터 y와의 차이를 줄이도록 하는것이 CBoW의 목표이다)

{
고정벡터의 크기가 E이고 단어 집합의 크기가 V일 때
입력층->투사층 가중치W 벡터는 V*E행렬
원핫임베딩 벡터는 1*V 행렬
곱하면 1*E행렬, 즉 E는 투사층의 크기
(※행벡터표기<->열벡터표기의 경우 계산전에 전치해둘것)
1*E행렬의 평균을 구하고
가중치행렬 W'(아래첨자)E*V에 곱하면 1*V 크기의 벡터를 얻는다.
이걸 똑같이 1*V크기인 중심단어의 원-핫벡터와
오차함수 소프트맥스에 집어넣고 오차역전파, SGD등을 사용해 학습하면된다.
}

※ CBoW에 하나 있는 은닉층은 활성화 함수가 없고 연산만을 담당하므로 투사층이라고 부른다
??? LUT 테이블은 어디에서 가져오는가? 어디에도 그 답이 없다

-
281p
Skip-gram: 중심 단어를 입력으로 받아서 주변 단어를 예측하는 모델이다.

중심단어를 기준으로 양쪽으로 윈도 크기 만큼의 단어를 주변 단어로 삼아 훈련 데이터 세트를 만든다
(세상의 | 재미있는), (세상의 |일들은)...
하나의 윈도우에서 중심 단어와 주변 단어를 하나의 쌍으로 하여 여러 학습 데이터를 만든다

더 많은 학습 데이터세트가 추출되고 성능이 CBoW보다 뛰어난 편이라고한다.
드물게 등장하는 단어도 잘 학습한다고 하고
단어 벡터도 벡터 공간에서 더 유의미한거리 관계를 형성한다고 한다

Skip-gram 모델도 원-핫 벡터를 투사층에 입력하고 해당 단어의 임베딩 벡터를 가져온 다음
임베딩과 W'(아래첨자)E*V 가중치와의 곱셈을 통해 V 크기(단어수 크기)의 벡터를 얻고
이 벡터 각각을 소프트맥스를 때려 주변 단어를 예측한다.

소프트맥스 연산은 모든 단어를 대상으로 내적 연산을 수행한다.
말뭉치가 커짐 -> 단어 사전의 크기가 커짐 -> 연산이 느려짐 문제가 있어서

이를 해결하기위해 계층적 소프트맥스와 네거티브 샘플링 기법을 사용함
-
283p
https://hcnoh.github.io/2023-02-20-nlp-02
https://uponthesky.tistory.com/15

계층적 소프트맥스는 기존 softmax의 계산량을 현격하게 줄인, softmax에 근사시키는 방법론
출력층을 호프만 트리 구조로 표현해 연산을 수행
(책에서는 단순히 이진트리라고 되어있다)
단어는 항상 리프 노드로 되어있다.
자주 등장할 수록 노드의 상위에 위치한다.
루트노드에서 해당하는 단어까지의 경로의 노드에 해당하는 벡터만 최적화하면되는듯
상위노드 - 하위노드간 엣지는 확률을 나타낸다
각 단어의 확률은 해당 단어까지의 경로의 모든 엣지에 해당하는 확률을곱한것
계층적 소프트맥스의 시간 복잡도는 O(log 2 V)

Word2Vec의 Huffman Tree는 모든 '단어'를 Leaf Node로 가진다고 한다
단어의 등장 빈도를 가지고 Huffman Tree를 구성한다

리프노드에는 단어가, 그 외에 노드에는 벡터가 있다.
단어(=리프노드)에 이르는 경로의 벡터만 갱신하면 된다고 한다
이 노드의 벡터 각각에는 어떤 값이 존재하는가 ???
(아마 하위 노드 각각으로 갈 확률 즉 합이 1인 가중치가 아니겠는가?)
(2개짜리 소프트맥스를 여러번 하는 구조로 추정한다???)

-
284p
https://heytech.tistory.com/354
https://techblog-history-younghunjo1.tistory.com/434 <- 얘 설명이 가장 근본있는듯

네거티브 샘플링: 전체 단어 집합에서 일부 단어를 샘플링하여 오답 단어로 사용하는 기법
Word2Vec 모델에서 사용된다.

학습 윈도 내에 등장하지 않는 단어를 n개 추출하여
정답 단어와 함께 소프트 맥스 연산을 수행
-> 전체 단어의 확률을 계산할 필요 없이 모델을 효과적으로 학습

추출할 단어 n개는 5~20개 정도 쓰고
P(w_i) = f(w_i)^0.75 / Σ(j=0 to V) f(w_j)^0.75 ※_(언더바)는 아래첨자 대신사용
P는 단어 w_i가 네거티브 샘플로 추출될 확률, f는 빈도이다.
0.75제곱은 실험으로 나온 최적값이라고 한다

285p
네거티브 샘플링 예시이다
(word2vec은 임베딩 기법이며, skip-gram은 word2vec에서 중심단어로 주변단어를 임베딩하는 기법이고, 네거티브 샘플링은 skip-gram의 최적화 기법이다)
'세상의 재미있는 일들은 모두 밤에 일어난다' 라는 문장에서
'재미있는'이라는 중심 단어에서 ('재미있는', '세상의'), ('재미있는', '일들은'), ('재미있는', '모두') 쌍은 1로 레이블링
('재미있는', '걸어가다'), ('재미있는', '주십시오'), ('재미있는', '미리'), ('재미있는', '안녕하세요') 쌍은 0으로 레이블링된 학습데이터를 만드는 예시
0과 1로 이루어진 벡터가 하나 나올거고, 그걸 내적하면 벡터가 하나 나올거고
그 벡터를 시그모이드에 집어넣어 단어 각각에 대한 확률로 바꾼다
1이면 해당 확률 값이 높아지도록, 0이면 낮아지도록 가중치가 조정될 것이다
(아마 시그모이드 출력값을 오차함수 즉 로지스틱 함수에 넣어 오차를 만들고 가중치를 조정하지 않겠는가???)

네거티브 샘플링 순서
1. 오답단어 N 숫자 정함
2. N개의 단어를 샘플링함, 각 단어의 샘플링 확률은 (해당단어 빈도수^0.75) / (Σ 모든 빈도수^0.75의 합), 빈도는 각 단어의 등장횟수/모든단어의 등장횟수 합
3. 주변 단어와 중심단어의 입력을 1로, 중심단어와 오답단어의 입력을 0으로 레이블링한 훈련데이터 만듬
4. 중심단어를 모델에 넣고 각 단어들의 예측값을 가져온다
5. 실제 레이블(0, 1)과 비교하여 각 단어들와의 오차 값을 가져온다
6. 해당 값으로부터 오차역전파를 통해 중심 단어와 주변 단어의 임베딩 테이블의 가중치벡터를 업데이트한다.

임베딩 테이블이 2개인데 훈련 데이터 단어 집합의 크기를 가지므로 크기가 같다고 한다.
하나는 중심 단어의 테이블룩업, 다른 하나는 주변 단어의 테이블 룩업을 위한 임베딩 테이블
이후 선택적으로 중심 단어의 임베딩 테이블을 최종 임베딩 테이블로 사용할 수 있다고 한다.