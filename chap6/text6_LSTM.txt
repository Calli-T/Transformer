https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr

시간적으로 먼 과거의 정보를 기억할 필요가 있는 연속형 데이터가 존재한다.
먼 시점에서의 데이터에서 힌트를 얻어야할 것

장기의존성 문제나 기울기 소실/폭주 문제 발생 가능성 존재함

LSTM(장단기 메모리)는 순환 신경망과 비슷한 구조에
메모리 셀과 게이트 추가하여 위 두 문제 해결

314p
LSTM(Long Short Term Memory)은 셀 상태/망각 게이트/기억 게이트/출력 게이트로 정보의 흐름을 제어
각 하는일
셀 상태: 정보를 저장하고 유지하는 메모리, 출력/망각 게이트에 의해 제어
망각 게이트: 이전 셀 상태에서 어떤 정보를 삭제할지 결정, 현재 입력과 이전 셀 상태를 입력으로 받아 결정
입력 게이트: 새로운 정보를 어떤 부분에 추가할 지 결정, 현재 입력과 이전 셀 상태를 입력으로 받아 결정
출력 게이트: 셀 상태의 정보 중 어떤 부분을 출력할지 결정, 현재 입력과 이전 셀 상태와 새로 추가된 정보를 입력으로 받아 결정

315p
LSTM의 메모리 셀은 은닉 상태와 유사하지만 출력값 계산에 직접 사용되지 않는다.

버리기/기억/출력에 관해 각 게이트로 추가적인 연산 수행
세 가지 게이트는 모두 활성화 함수로 시그모이드 사용
[0, 1] 사이의 값으로 변환하므로 게이트의 출력값은 각각 0에서 1사이의 값으로 결정
이 값은 해당 게이트가 입력값에 대해 얼마나 많은 정보를 통과시킬지 결정
ex) 망각 게이트의 출력값이 1이면
이전 시점의 기억 상태가 현재 시점의 기억 상태에 완전히 유지
0이면 이전 시점의 기억이 현재 시점의 기억 상태에 반영 안됨

---
망각 게이트는 현재 시점의 입력과 이전 시점의 은닉 상태를 입력으로 받아
각각의 가중치와 곱한다음 편향을 추가하여 처리하고
그 값을 시그모이드에 넣는다
해당 값 [0, 1]에서 0에 가까울수록 더 많은 정보를 삭제한다.

망각 게이트 f_t의 계산 수식은 다음과 같다
f_t = σ(W_x^(f) * x_t + W_h^(f) * h_t-1 + b^(f)) ※ 아래첨자는 _로, 윗첨자는 ^로 표시되었다

W_x^(f): 입력 값을 위한 가중치
W_h^(f): 은닉 상태를 위한 가중치
b^(f): 망각 게이트의 편향
x_t: 현재 시점의 입력값
h_t-1: t-1시점의 은닉 상태

---
기억 게이트는 g_i와 i_t로 구성되어 있다.
i_t는 활성화 함수로 시그모이드를 사용,
g_i는 활성화 함수로 하이퍼볼릭 탄젠트사용

i_t = sigmoid(W_x^(i) * x_t + W_x^(i) * h_t-1 + b^(i)) ※ 왜 시그모이드 표기가 한 쪽만 영어로 되어있지?, 블로그쪽 정보는 같은 σ다
g_i = tanh(W_x^(g) * x_t + W_x^(g) * h_t-1 + b^(g))
위 식에서 W는 다 가중치, b는 전부 편향/입력도 현재 입력과 이전 은닉 상태로 동일하다.

i_t는 [0, 1]의 값을 가지므로 현재 시점의 정보를
얼마나 기억할(더할) 것인지 결정하는 가중치 역할을 한다.
1에 가까울 수록 많이 기억한다.

---
메모리 셀은 망각 게이트와 기억 게이트의 정보로 현재 시점의 메모리 셀 값을 계산한다.
c_t = f_t ⊙ c_t-1 + g_i ⊙ i_t
망각게이트 출력 값과 f_t와 이전 시점의 메모리 셀 값 c_t-1를 ⊙ 연산 즉 아다마르 곱을 하며(즉, f_t가 0에 가까울 수록 기억하는 정보가 적고)
기억 게이트의 출력 값인 g_i와 i_t도 아다마르 곱하여 앞선 값에 더한다.(i_t가 0에 가까울 수록 추가하는 정보가 적다)
※ 아마다르 곱: 각 행렬의 원소끼리만 곱하는 행렬 곱, 동일한 크기의 행렬을 곱한다

---
출력 게이트는 현재 시점의 은닉 상태를 제어한다.
o_t = σ(W_x^(o) * x_t + W_h^(o) * h_t-1 + b^(o))
o_t는 시그모이드로, [0, 1] 사이의 값이며
여전히 현재 시점의 은닉 상태를 '얼마나' 사용할지 결정하는 값이다
은닉 상태 갱신은
h_t = o_t ⊙ tanh(c_t)
h_t: 현재 시점의 은닉상태
o_t: 출력 게이트의 값, 은닉상태를 제어
c_t: 메모리 셀 값

망각 게이트와 기억 게이트의 값을 통해 메모리 셀 값을 도출해내고,
메모리 셀 값에 하이퍼볼릭 탄젠트를 곱한것과 출력 게이트의 값을 아마다르 곱하여 (현재 시점의 은닉 상태가 이전 시점의 은닉 상태에서 얼마나 영향을 받는지 계산)
현재 시점의 은닉상태를 만들어준다

320p
장단기 메로리 모델은 파이토치의 torch.nn.LSTM 클래스에서 지원한다
활성화 함수를 정할 필요는 없는 것이, 장단기 메모리는 활성화 함수가 명확히 정의되어있다.

lstm = torch.nn.LSTM(
    input_size,
    hidden_size, # 은닉 상태 크기
    num_layers=1,
    bias=False,
    batch_first=True,
    dropout=0,
    bidirectgional=False,
    proj_size=0 # 투사 크기, 장단기 메모리 계층의 출력에 대한 선형 투사 크기를 결정한다
)

※ 선형투사: 벡터를 선형 변환하여 다른 차원의 벡터로 매핑하는 과정 (projection, 사영)
※※ 투사 크기 매개변수로 메돌의 크기와 복잡도를 변경가능, 은닉 상태 크기보다 작은 값으로 설정

??? 순환 신경망 클래스의 매개변수를 잘 읽어보자, 이건 여러번 읽어봐야 뭔 소리인지 알듯
https://wikidocs.net/22886
를 다시 찬찬히 읽어보고 매개변수 개수 부터 이해를하자...