286p
Lookup 연산을 수행하는데는 룩업 테이블이 필요하고 이는 임베딩 클래스로 구현가능
V는 학습단어수, 임베딩 차원은 E
W(아래첨자)V*E 행렬과 W'(아래첨자)E*V 행렬을 최적화 하면서 학습하는것이 Word2Vec 모델
E*V행렬은 어떤 단어의 임베딩을 다른 단어의 예측으로 나타내는 행렬???

임베딩 클래스는 단어나 범주형 변수와 같은 이산 변수를
연속적인 벡터 형태로 변환해 사용가능

연속적인 벡터 표현은 모델이 학습하는동안
단어의 의미나 관련된 정보를 포착, 이를 기반으로 단어간의 유사도 계산

torch.nn.Embedding클래스 활용법
num_embeddings는 단어 사전 크기
embedding_dim은 임베딩 벡터의 차원
max_norm은 임베딩 벡터의 최대 크기(노름의 최대 크기) 상한선, 이보다 크면 최대 크기로 자른다
norm_type은 기본값 2.0이고, 규제 방식 즉 노름이 L1이냐 L2냐인듯

padding_idx는 패딩 토큰의 인덱스, 해당 인덱스의 임베딩 벡터를 0으로 설정한다.
병렬 처리를 위해 입력 배치의 문장 길이가 동일해야한다고 하니
입력 문장을 일정한 길이로 맞추는 역할을 한다고 한다
패딩 인덱스는 임베딩 수보다 적어야하고
해당 인덱스의 벡터값은 모델 학습에 최적화 되지 않는다.

288p